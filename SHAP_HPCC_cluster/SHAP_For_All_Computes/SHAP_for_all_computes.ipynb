{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import colors as plt_colors\n",
    "\n",
    "## Measure time\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from json import JSONDecoder, JSONDecodeError  # for reading the JSON data files\n",
    "import re  # for regular expressions\n",
    "import os  # for os related operations\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from collections import Counter\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "data = json.load(open('/Users/chaupham/Downloads/17Feb2020.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compute-1-1', 'compute-1-10', 'compute-1-11', 'compute-1-12', 'compute-1-13', 'compute-1-14', 'compute-1-15', 'compute-1-16', 'compute-1-17', 'compute-1-18', 'compute-1-19', 'compute-1-2', 'compute-1-20', 'compute-1-21', 'compute-1-22', 'compute-1-23', 'compute-1-24', 'compute-1-25', 'compute-1-26', 'compute-1-27', 'compute-1-28', 'compute-1-29', 'compute-1-3', 'compute-1-30', 'compute-1-31', 'compute-1-32', 'compute-1-33', 'compute-1-34', 'compute-1-35', 'compute-1-36', 'compute-1-37', 'compute-1-38', 'compute-1-39', 'compute-1-4', 'compute-1-40', 'compute-1-41', 'compute-1-42', 'compute-1-43', 'compute-1-44', 'compute-1-45', 'compute-1-46', 'compute-1-47', 'compute-1-48', 'compute-1-49', 'compute-1-5', 'compute-1-50', 'compute-1-51', 'compute-1-52', 'compute-1-53', 'compute-1-54', 'compute-1-55', 'compute-1-56', 'compute-1-57', 'compute-1-58', 'compute-1-59', 'compute-1-6', 'compute-1-60', 'compute-1-7', 'compute-1-8', 'compute-1-9', 'compute-10-25', 'compute-10-26', 'compute-10-27', 'compute-10-28', 'compute-10-29', 'compute-10-30', 'compute-10-31', 'compute-10-32', 'compute-10-33', 'compute-10-34', 'compute-10-35', 'compute-10-36', 'compute-10-37', 'compute-10-38', 'compute-10-39', 'compute-10-40', 'compute-10-41', 'compute-10-42', 'compute-10-43', 'compute-10-44', 'compute-2-1', 'compute-2-10', 'compute-2-11', 'compute-2-12', 'compute-2-13', 'compute-2-14', 'compute-2-15', 'compute-2-16', 'compute-2-17', 'compute-2-18', 'compute-2-19', 'compute-2-2', 'compute-2-20', 'compute-2-21', 'compute-2-22', 'compute-2-23', 'compute-2-24', 'compute-2-25', 'compute-2-26', 'compute-2-27', 'compute-2-28', 'compute-2-29', 'compute-2-3', 'compute-2-30', 'compute-2-31', 'compute-2-32', 'compute-2-33', 'compute-2-34', 'compute-2-35', 'compute-2-36', 'compute-2-37', 'compute-2-38', 'compute-2-39', 'compute-2-4', 'compute-2-40', 'compute-2-41', 'compute-2-42', 'compute-2-43', 'compute-2-44', 'compute-2-45', 'compute-2-46', 'compute-2-47', 'compute-2-48', 'compute-2-49', 'compute-2-5', 'compute-2-50', 'compute-2-51', 'compute-2-52', 'compute-2-53', 'compute-2-54', 'compute-2-55', 'compute-2-56', 'compute-2-57', 'compute-2-58', 'compute-2-59', 'compute-2-6', 'compute-2-60', 'compute-2-7', 'compute-2-8', 'compute-2-9', 'compute-3-1', 'compute-3-10', 'compute-3-11', 'compute-3-12', 'compute-3-13', 'compute-3-14', 'compute-3-15', 'compute-3-16', 'compute-3-17', 'compute-3-18', 'compute-3-19', 'compute-3-2', 'compute-3-20', 'compute-3-21', 'compute-3-22', 'compute-3-23', 'compute-3-24', 'compute-3-25', 'compute-3-26', 'compute-3-27', 'compute-3-28', 'compute-3-29', 'compute-3-3', 'compute-3-30', 'compute-3-31', 'compute-3-32', 'compute-3-33', 'compute-3-34', 'compute-3-35', 'compute-3-36', 'compute-3-37', 'compute-3-38', 'compute-3-39', 'compute-3-4', 'compute-3-40', 'compute-3-41', 'compute-3-42', 'compute-3-43', 'compute-3-44', 'compute-3-45', 'compute-3-46', 'compute-3-47', 'compute-3-48', 'compute-3-49', 'compute-3-5', 'compute-3-50', 'compute-3-51', 'compute-3-52', 'compute-3-53', 'compute-3-54', 'compute-3-55', 'compute-3-56', 'compute-3-6', 'compute-3-7', 'compute-3-8', 'compute-3-9', 'compute-4-1', 'compute-4-10', 'compute-4-11', 'compute-4-12', 'compute-4-13', 'compute-4-14', 'compute-4-15', 'compute-4-16', 'compute-4-17', 'compute-4-18', 'compute-4-19', 'compute-4-2', 'compute-4-20', 'compute-4-21', 'compute-4-22', 'compute-4-23', 'compute-4-24', 'compute-4-25', 'compute-4-26', 'compute-4-27', 'compute-4-28', 'compute-4-29', 'compute-4-3', 'compute-4-30', 'compute-4-31', 'compute-4-32', 'compute-4-33', 'compute-4-34', 'compute-4-35', 'compute-4-36', 'compute-4-37', 'compute-4-38', 'compute-4-39', 'compute-4-4', 'compute-4-40', 'compute-4-41', 'compute-4-42', 'compute-4-43', 'compute-4-44', 'compute-4-45', 'compute-4-46', 'compute-4-47', 'compute-4-48', 'compute-4-5', 'compute-4-6', 'compute-4-7', 'compute-4-8', 'compute-4-9', 'compute-5-1', 'compute-5-10', 'compute-5-11', 'compute-5-12', 'compute-5-13', 'compute-5-14', 'compute-5-15', 'compute-5-16', 'compute-5-17', 'compute-5-18', 'compute-5-19', 'compute-5-2', 'compute-5-20', 'compute-5-21', 'compute-5-22', 'compute-5-23', 'compute-5-24', 'compute-5-3', 'compute-5-4', 'compute-5-5', 'compute-5-6', 'compute-5-7', 'compute-5-8', 'compute-5-9', 'compute-6-1', 'compute-6-10', 'compute-6-11', 'compute-6-12', 'compute-6-13', 'compute-6-14', 'compute-6-15', 'compute-6-16', 'compute-6-17', 'compute-6-18', 'compute-6-19', 'compute-6-2', 'compute-6-20', 'compute-6-3', 'compute-6-4', 'compute-6-5', 'compute-6-6', 'compute-6-7', 'compute-6-8', 'compute-6-9', 'compute-7-1', 'compute-7-10', 'compute-7-11', 'compute-7-12', 'compute-7-13', 'compute-7-14', 'compute-7-15', 'compute-7-16', 'compute-7-17', 'compute-7-18', 'compute-7-19', 'compute-7-2', 'compute-7-20', 'compute-7-21', 'compute-7-22', 'compute-7-23', 'compute-7-24', 'compute-7-25', 'compute-7-26', 'compute-7-27', 'compute-7-28', 'compute-7-29', 'compute-7-3', 'compute-7-30', 'compute-7-31', 'compute-7-32', 'compute-7-33', 'compute-7-34', 'compute-7-35', 'compute-7-36', 'compute-7-37', 'compute-7-38', 'compute-7-39', 'compute-7-40', 'compute-7-41', 'compute-7-42', 'compute-7-43', 'compute-7-44', 'compute-7-45', 'compute-7-46', 'compute-7-47', 'compute-7-48', 'compute-7-49', 'compute-7-5', 'compute-7-50', 'compute-7-51', 'compute-7-52', 'compute-7-53', 'compute-7-54', 'compute-7-55', 'compute-7-56', 'compute-7-57', 'compute-7-58', 'compute-7-59', 'compute-7-6', 'compute-7-60', 'compute-7-7', 'compute-7-8', 'compute-7-9', 'compute-8-1', 'compute-8-10', 'compute-8-11', 'compute-8-12', 'compute-8-13', 'compute-8-14', 'compute-8-15', 'compute-8-16', 'compute-8-17', 'compute-8-18', 'compute-8-19', 'compute-8-2', 'compute-8-20', 'compute-8-21', 'compute-8-22', 'compute-8-23', 'compute-8-24', 'compute-8-25', 'compute-8-26', 'compute-8-27', 'compute-8-28', 'compute-8-29', 'compute-8-3', 'compute-8-30', 'compute-8-31', 'compute-8-32', 'compute-8-33', 'compute-8-34', 'compute-8-35', 'compute-8-36', 'compute-8-37', 'compute-8-38', 'compute-8-39', 'compute-8-4', 'compute-8-40', 'compute-8-41', 'compute-8-42', 'compute-8-43', 'compute-8-44', 'compute-8-45', 'compute-8-46', 'compute-8-47', 'compute-8-48', 'compute-8-49', 'compute-8-5', 'compute-8-50', 'compute-8-51', 'compute-8-52', 'compute-8-53', 'compute-8-54', 'compute-8-55', 'compute-8-56', 'compute-8-57', 'compute-8-58', 'compute-8-59', 'compute-8-6', 'compute-8-60', 'compute-8-7', 'compute-8-8', 'compute-8-9', 'compute-9-1', 'compute-9-10', 'compute-9-11', 'compute-9-12', 'compute-9-13', 'compute-9-14', 'compute-9-15', 'compute-9-16', 'compute-9-17', 'compute-9-18', 'compute-9-19', 'compute-9-2', 'compute-9-20', 'compute-9-21', 'compute-9-22', 'compute-9-23', 'compute-9-24', 'compute-9-25', 'compute-9-26', 'compute-9-27', 'compute-9-28', 'compute-9-29', 'compute-9-3', 'compute-9-30', 'compute-9-31', 'compute-9-32', 'compute-9-33', 'compute-9-34', 'compute-9-35', 'compute-9-36', 'compute-9-37', 'compute-9-38', 'compute-9-39', 'compute-9-4', 'compute-9-40', 'compute-9-41', 'compute-9-42', 'compute-9-43', 'compute-9-44', 'compute-9-45', 'compute-9-46', 'compute-9-47', 'compute-9-48', 'compute-9-49', 'compute-9-5', 'compute-9-50', 'compute-9-51', 'compute-9-52', 'compute-9-53', 'compute-9-54', 'compute-9-55', 'compute-9-56', 'compute-9-57', 'compute-9-58', 'compute-9-59', 'compute-9-6', 'compute-9-60', 'compute-9-7', 'compute-9-8', 'compute-9-9']\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "print(list(data.keys()))\n",
    "print(len(list(data.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[[0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631313131313132, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0664040404040404, 0.5375, 0.5375, 0.5375, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06629292929292929, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631313131313132, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7375], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0663080808080808, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5458333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631313131313132, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0663080808080808, 0.5375, 0.5375, 0.5375, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632828282828282, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5458333333333333, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5375, 0.5375, 0.7475], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5375, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.1368421052631579, 0.06635858585858585, 0.5375, 0.5375, 0.5375, 0.5375, 0.74], [0.6105263157894737, 0.47368421052631576, 0.14736842105263157, 0.06635858585858585, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5333333333333333, 0.5458333333333333, 0.5375, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0664949494949495, 0.5375, 0.5458333333333333, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631313131313132, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632323232323233, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06653030303030304, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06635353535353536, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634343434343434, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06644949494949495, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.6105263157894737, 0.47368421052631576, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632828282828282, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06641919191919192, 0.5375, 0.5375, 0.525, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631818181818182, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632323232323233, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06642424242424243, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06642424242424243, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.47368421052631576, 0.1368421052631579, 0.06632828282828282, 0.5333333333333333, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631818181818182, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632828282828282, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06652020202020202, 0.5375, 0.5375, 0.5375, 0.5333333333333333, 0.7375], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.745], [0.6105263157894737, 0.47368421052631576, 0.14736842105263157, 0.06634343434343434, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.745], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6105263157894737, 0.47368421052631576, 0.1368421052631579, 0.06633838383838384, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5333333333333333, 0.7475], [0.6105263157894737, 0.4842105263157895, 0.1368421052631579, 0.06632828282828282, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.7475], [0.6105263157894737, 0.47368421052631576, 0.14736842105263157, 0.06634343434343434, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.1368421052631579, 0.06633838383838384, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.47368421052631576, 0.1368421052631579, 0.06634848484848485, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.1368421052631579, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7375], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06634343434343434, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06635353535353536, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.1368421052631579, 0.06635353535353536, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06635858585858585, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06635353535353536, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06636363636363636, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634343434343434, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06635353535353536, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.1368421052631579, 0.06634848484848485, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06665656565656566, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06636363636363636, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06638383838383838, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0663939393939394, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06638888888888889, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06666666666666667, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0663939393939394, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06638888888888889, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7375], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06655555555555556, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.0663989898989899, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.0663989898989899, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.45263157894736844, 0.37894736842105264, 0.14736842105263157, 0.06365656565656565, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.755], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.11465151515151516, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.765], [0.5473684210526316, 0.4421052631578947, 0.14736842105263157, 0.12462626262626261, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7325], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12471212121212122, 0.5375, 0.5375, 0.5375, 0.5375, 0.7575], [0.5789473684210527, 0.45263157894736844, 0.1368421052631579, 0.12478282828282829, 0.5375, 0.5375, 0.5375, 0.5375, 0.7675], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12504040404040404, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7625], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12484848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.765], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.12491919191919193, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7625], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12503030303030302, 0.5375, 0.5375, 0.5375, 0.5375, 0.765], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12497979797979797, 0.5375, 0.5375, 0.5375, 0.5375, 0.735], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12496969696969697, 0.5375, 0.5375, 0.5375, 0.5375, 0.765], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.124989898989899, 0.5375, 0.5375, 0.5375, 0.5375, 0.6675], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.1250050505050505, 0.5375, 0.5375, 0.5375, 0.5375, 0.765], [0.5578947368421052, 0.45263157894736844, 0.14736842105263157, 0.12512121212121213, 0.5333333333333333, 0.5458333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12508080808080807, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.76], [0.5684210526315789, 0.4631578947368421, 0.1368421052631579, 0.12505555555555556, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12506060606060607, 0.5375, 0.5375, 0.5375, 0.5375, 0.7625], [0.5789473684210527, 0.47368421052631576, 0.14736842105263157, 0.12509090909090909, 0.5375, 0.5375, 0.5375, 0.5375, 0.765], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.12509090909090909, 0.5375, 0.5375, 0.5375, 0.5375, 0.72], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.12508585858585858, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.7625], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.1251060606060606, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.5789473684210527, 0.47368421052631576, 0.1368421052631579, 0.12513131313131312, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.5789473684210527, 0.4421052631578947, 0.14736842105263157, 0.12515656565656566, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7225], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12516161616161617, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.765], [0.5578947368421052, 0.4631578947368421, 0.14736842105263157, 0.12515151515151515, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12515151515151515, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7625], [0.5684210526315789, 0.4631578947368421, 0.14736842105263157, 0.12517171717171716, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.76], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12516666666666668, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.76], [0.5263157894736842, 0.4, 0.14736842105263157, 0.1251818181818182, 0.5375, 0.5375, 0.5375, 0.5333333333333333, 0.585], [0.3473684210526316, 0.28421052631578947, 0.14736842105263157, 0.05588888888888889, 0.5375, 0.5375, 0.5375, 0.5333333333333333, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.1368421052631579, 0.055868686868686866, 0.5375, 0.5375, 0.5375, 0.5375, 0.33], [0.3368421052631579, 0.28421052631578947, 0.1368421052631579, 0.0557929292929293, 0.5375, 0.5375, 0.5375, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.0557929292929293, 0.5375, 0.5375, 0.525, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.3325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.055767676767676765, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.1368421052631579, 0.055767676767676765, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.33], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578282828282828, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578282828282828, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578282828282828, 0.5375, 0.5333333333333333, 0.5375, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5375, 0.5375, 0.5375, 0.335], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578787878787878, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.3375], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578787878787878, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.1368421052631579, 0.05578787878787878, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577272727272727, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.33], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577272727272727, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.28421052631578947, 0.14736842105263157, 0.05577272727272727, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.325], [0.3894736842105263, 0.3368421052631579, 0.14736842105263157, 0.09537373737373737, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.5075], [0.3894736842105263, 0.3368421052631579, 0.14736842105263157, 0.1283939393939394, 0.5333333333333333, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.78], [0.3473684210526316, 0.28421052631578947, 0.1368421052631579, 0.09613131313131312, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.33], [0.5473684210526316, 0.4421052631578947, 0.14736842105263157, 0.06465656565656565, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06501515151515151, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06501515151515151, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7025], [0.5789473684210527, 0.45263157894736844, 0.14736842105263157, 0.06510606060606061, 0.5375, 0.5375, 0.5375, 0.5375, 0.7], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06523232323232324, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.6975], [0.42105263157894735, 0.3473684210526316, 0.14736842105263157, 0.06523232323232324, 0.5375, 0.5375, 0.5375, 0.5458333333333333, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06097979797979798, 0.5375, 0.5458333333333333, 0.5333333333333333, 0.5375, 0.7125], [0.5368421052631579, 0.4421052631578947, 0.14736842105263157, 0.06445959595959595, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7075], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06476767676767677, 0.5375, 0.5458333333333333, 0.5333333333333333, 0.5375, 0.7125], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06474242424242424, 0.5375, 0.5375, 0.5333333333333333, 0.5458333333333333, 0.71], [0.5052631578947369, 0.3894736842105263, 0.14736842105263157, 0.06502020202020202, 0.5375, 0.5375, 0.5375, 0.5375, 0.38], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06502020202020202, 0.5375, 0.5375, 0.5375, 0.5375, 0.7125], [0.5368421052631579, 0.4421052631578947, 0.14736842105263157, 0.06487878787878788, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7175], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06442929292929293, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.45263157894736844, 0.14736842105263157, 0.0647020202020202, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.715], [0.47368421052631576, 0.35789473684210527, 0.14736842105263157, 0.06492929292929293, 0.5375, 0.5375, 0.5375, 0.5375, 0.6475], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.06507070707070707, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7175], [0.5368421052631579, 0.43157894736842106, 0.14736842105263157, 0.06452020202020202, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5684210526315789, 0.4631578947368421, 0.14736842105263157, 0.06481818181818182, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7275], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06460606060606061, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7175], [0.4421052631578947, 0.3368421052631579, 0.14736842105263157, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.67], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06466666666666666, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.725], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.0642979797979798, 0.5375, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0642979797979798, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06425252525252526, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.72], [0.5473684210526316, 0.45263157894736844, 0.15789473684210525, 0.06423232323232324, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06487373737373738, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.72], [0.5263157894736842, 0.43157894736842106, 0.15789473684210525, 0.06337878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5789473684210527, 0.47368421052631576, 0.15789473684210525, 0.06496969696969697, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.715], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.4842105263157895, 0.4, 0.15789473684210525, 0.06340404040404041, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7125], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06495959595959595, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06472727272727273, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7275], [0.4421052631578947, 0.3263157894736842, 0.15789473684210525, 0.06472727272727273, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.6825], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06481313131313131, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.72], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06433333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5052631578947369, 0.3894736842105263, 0.15789473684210525, 0.06471212121212122, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.335], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.061030303030303025, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5473684210526316, 0.4421052631578947, 0.14736842105263157, 0.06426262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.72], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.0646969696969697, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7175], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.065010101010101, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5368421052631579, 0.43157894736842106, 0.15789473684210525, 0.065010101010101, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.065010101010101, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06434343434343434, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5368421052631579, 0.43157894736842106, 0.15789473684210525, 0.06434343434343434, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7125], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06484848484848485, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5052631578947369, 0.42105263157894735, 0.15789473684210525, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06114646464646464, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06109090909090909, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.49473684210526314, 0.4105263157894737, 0.15789473684210525, 0.06109090909090909, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5473684210526316, 0.45263157894736844, 0.15789473684210525, 0.06104545454545454, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7125], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06485858585858587, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.49473684210526314, 0.4105263157894737, 0.15789473684210525, 0.06485858585858587, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7], [0.5578947368421052, 0.45263157894736844, 0.14736842105263157, 0.06476262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06462121212121212, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.71], [0.5052631578947369, 0.4105263157894737, 0.14736842105263157, 0.06462121212121212, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5473684210526316, 0.45263157894736844, 0.14736842105263157, 0.06338888888888888, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.6975], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.06098989898989899, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7025], [0.5157894736842106, 0.43157894736842106, 0.14736842105263157, 0.06098989898989899, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.705], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06423232323232324, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7025], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06110606060606061, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5473684210526316, 0.4421052631578947, 0.15789473684210525, 0.06110606060606061, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7125], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.064510101010101, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.335], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.061126262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06424242424242424, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.47368421052631576, 0.3684210526315789, 0.15789473684210525, 0.06424242424242424, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.33], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06105555555555556, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7175], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06106565656565656, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.4421052631578947, 0.3368421052631579, 0.15789473684210525, 0.06334848484848485, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.33], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06334848484848485, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7125], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.06332323232323232, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.71], [0.43157894736842106, 0.3368421052631579, 0.15789473684210525, 0.06332323232323232, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.695], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.0632979797979798, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7175], [0.5473684210526316, 0.4421052631578947, 0.15789473684210525, 0.0632979797979798, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.4842105263157895, 0.4105263157894737, 0.15789473684210525, 0.0647979797979798, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.06426262626262626, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06482323232323232, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5157894736842106, 0.42105263157894735, 0.15789473684210525, 0.06482323232323232, 0.5375, 0.5375, 0.5375, 0.5458333333333333, 0.7075], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06417676767676768, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5473684210526316, 0.4421052631578947, 0.15789473684210525, 0.06417676767676768, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5473684210526316, 0.4421052631578947, 0.15789473684210525, 0.06325757575757576, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06325757575757576, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.335], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06104545454545454, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06104545454545454, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0645959595959596, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7275], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.0645959595959596, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06455555555555556, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06455555555555556, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06463131313131314, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06457070707070707, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7125], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06434848484848485, 0.5375, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7125], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06414141414141414, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7225], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06414141414141414, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.725], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.06455555555555556, 0.5375, 0.5375, 0.5375, 0.5375, 0.6975], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06466666666666666, 0.5375, 0.5375, 0.5375, 0.5375, 0.725], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.06466666666666666, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06462626262626263, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.05524747474747475, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.45263157894736844, 0.14736842105263157, 0.06442929292929293, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.06455050505050505, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.72], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06455050505050505, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06424747474747475, 0.5458333333333333, 0.5375, 0.5375, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06440909090909092, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.72], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06464646464646465, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06468181818181819, 0.5375, 0.5375, 0.5375, 0.5375, 0.705], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.0642020202020202, 0.5375, 0.5375, 0.5375, 0.5375, 0.72], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.0644949494949495, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.725], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0644949494949495, 0.5375, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.705], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06412121212121212, 0.5375, 0.5458333333333333, 0.5375, 0.55, 0.7075], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06446464646464646, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7075], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06426262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06445454545454546, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06433333333333333, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0643030303030303, 0.5375, 0.5375, 0.5375, 0.5375, 0.705], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0643030303030303, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.7025], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06453030303030302, 0.5375, 0.5375, 0.5375, 0.5375, 0.71], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06410606060606061, 0.5375, 0.5375, 0.5375, 0.5375, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0645, 0.5375, 0.5375, 0.5375, 0.5375, 0.695], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06324242424242424, 0.5375, 0.5375, 0.5375, 0.5375, 0.7], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06448989898989899, 0.5375, 0.5375, 0.5375, 0.5375, 0.705], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06448989898989899, 0.5375, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.695], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06451515151515151, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06435858585858587, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06432323232323232, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.55, 0.7075], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06476262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06427777777777778, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.6975], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.07856060606060605, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.7275], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07878787878787878, 0.5375, 0.5375, 0.5375, 0.5375, 0.7325], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07878787878787878, 0.5375, 0.5375, 0.5375, 0.5375, 0.7175], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07881313131313132, 0.5375, 0.5375, 0.5375, 0.5375, 0.7275], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07907575757575758, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7325], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07901010101010102, 0.5375, 0.5375, 0.5375, 0.5375, 0.72], [0.6, 0.47368421052631576, 0.14736842105263157, 0.0791111111111111, 0.5375, 0.5375, 0.5375, 0.5375, 0.715], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07913636363636364, 0.5375, 0.5375, 0.5375, 0.5375, 0.7175], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07923232323232324, 0.5375, 0.5375, 0.5375, 0.5375, 0.7175], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07872727272727273, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.6, 0.47368421052631576, 0.15789473684210525, 0.0788030303030303, 0.5375, 0.5375, 0.5375, 0.5375, 0.74], [0.6, 0.47368421052631576, 0.14736842105263157, 0.07890909090909091, 0.5375, 0.5375, 0.5375, 0.5375, 0.72], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07896969696969697, 0.5375, 0.5375, 0.5375, 0.5375, 0.73], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07906565656565656, 0.5375, 0.5375, 0.5375, 0.5375, 0.7225], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07911616161616161, 0.5375, 0.5458333333333333, 0.5375, 0.5375, 0.7225], [0.6, 0.47368421052631576, 0.14736842105263157, 0.07922727272727273, 0.5375, 0.5458333333333333, 0.5375, 0.5375, 0.725]]"
      ],
      "text/plain": [
       "[[0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0664040404040404,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06629292929292929,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663080808080808,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663080808080808,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0664949494949495,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632323232323233,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06653030303030304,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06644949494949495,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06641919191919192,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.525,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632323232323233,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06642424242424243,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06642424242424243,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06632828282828282,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06652020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.7375],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06636363636363636,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06634848484848485,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06665656565656566,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06636363636363636,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638383838383838,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663939393939394,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06666666666666667,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663939393939394,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06655555555555556,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663989898989899,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.45263157894736844,\n",
       "  0.37894736842105264,\n",
       "  0.14736842105263157,\n",
       "  0.06365656565656565,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.755],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.11465151515151516,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.12462626262626261,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12471212121212122,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7575],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.1368421052631579,\n",
       "  0.12478282828282829,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7675],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12504040404040404,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12484848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12491919191919193,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12503030303030302,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12497979797979797,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.735],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12496969696969697,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.124989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6675],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.1250050505050505,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.12512121212121213,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12508080808080807,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.76],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.1368421052631579,\n",
       "  0.12505555555555556,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12506060606060607,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12509090909090909,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12509090909090909,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12508585858585858,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.1251060606060606,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12513131313131312,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.5789473684210527,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.12515656565656566,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12516161616161617,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5578947368421052,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12515151515151515,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12515151515151515,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12517171717171716,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.76],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12516666666666668,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.76],\n",
       " [0.5263157894736842,\n",
       "  0.4,\n",
       "  0.14736842105263157,\n",
       "  0.1251818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.585],\n",
       " [0.3473684210526316,\n",
       "  0.28421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.05588888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.055868686868686866,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.28421052631578947,\n",
       "  0.1368421052631579,\n",
       "  0.0557929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.0557929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.525,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.055767676767676765,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.055767676767676765,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.335],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3375],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.28421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3894736842105263,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.09537373737373737,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5075],\n",
       " [0.3894736842105263,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.1283939393939394,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.78],\n",
       " [0.3473684210526316,\n",
       "  0.28421052631578947,\n",
       "  0.1368421052631579,\n",
       "  0.09613131313131312,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06465656565656565,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06501515151515151,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06501515151515151,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06510606060606061,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06523232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.42105263157894735,\n",
       "  0.3473684210526316,\n",
       "  0.14736842105263157,\n",
       "  0.06523232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06097979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5368421052631579,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06445959595959595,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7075],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06476767676767677,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06474242424242424,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5052631578947369,\n",
       "  0.3894736842105263,\n",
       "  0.14736842105263157,\n",
       "  0.06502020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.38],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06502020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5368421052631579,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06487878787878788,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06442929292929293,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.0647020202020202,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.715],\n",
       " [0.47368421052631576,\n",
       "  0.35789473684210527,\n",
       "  0.14736842105263157,\n",
       "  0.06492929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6475],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06507070707070707,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7175],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.14736842105263157,\n",
       "  0.06452020202020202,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06481818181818182,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06460606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.4421052631578947,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.67],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06466666666666666,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0642979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0642979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06425252525252526,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.72],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06423232323232324,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06487373737373738,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5263157894736842,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.06337878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.06496969696969697,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.715],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.4842105263157895,\n",
       "  0.4,\n",
       "  0.15789473684210525,\n",
       "  0.06340404040404041,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06495959595959595,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06472727272727273,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.4421052631578947,\n",
       "  0.3263157894736842,\n",
       "  0.15789473684210525,\n",
       "  0.06472727272727273,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6825],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06481313131313131,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06433333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5052631578947369,\n",
       "  0.3894736842105263,\n",
       "  0.15789473684210525,\n",
       "  0.06471212121212122,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.061030303030303025,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06426262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0646969696969697,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7175],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06434343434343434,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.06434343434343434,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06484848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5052631578947369,\n",
       "  0.42105263157894735,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06114646464646464,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06109090909090909,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.49473684210526314,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.06109090909090909,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06485858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.49473684210526314,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.06485858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06476262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06462121212121212,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.71],\n",
       " [0.5052631578947369,\n",
       "  0.4105263157894737,\n",
       "  0.14736842105263157,\n",
       "  0.06462121212121212,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06338888888888888,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06098989898989899,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7025],\n",
       " [0.5157894736842106,\n",
       "  0.43157894736842106,\n",
       "  0.14736842105263157,\n",
       "  0.06098989898989899,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.705],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06423232323232324,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7025],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06110606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06110606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.064510101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.061126262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06424242424242424,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.47368421052631576,\n",
       "  0.3684210526315789,\n",
       "  0.15789473684210525,\n",
       "  0.06424242424242424,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.33],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06105555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06106565656565656,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.4421052631578947,\n",
       "  0.3368421052631579,\n",
       "  0.15789473684210525,\n",
       "  0.06334848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.33],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06334848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06332323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.43157894736842106,\n",
       "  0.3368421052631579,\n",
       "  0.15789473684210525,\n",
       "  0.06332323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.695],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0632979797979798,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.0632979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.4842105263157895,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.0647979797979798,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06426262626262626,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06482323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5157894736842106,\n",
       "  0.42105263157894735,\n",
       "  0.15789473684210525,\n",
       "  0.06482323232323232,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06417676767676768,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06417676767676768,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06325757575757576,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06325757575757576,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645959595959596,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645959595959596,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06463131313131314,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06457070707070707,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06434848484848485,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06414141414141414,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7225],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06414141414141414,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06455555555555556,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06466666666666666,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.725],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06466666666666666,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06462626262626263,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.05524747474747475,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06442929292929293,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06455050505050505,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455050505050505,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06424747474747475,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06440909090909092,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06464646464646465,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06468181818181819,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0642020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0644949494949495,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0644949494949495,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06412121212121212,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06446464646464646,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06426262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06445454545454546,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06433333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0643030303030303,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0643030303030303,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06453030303030302,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06410606060606061,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.695],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06324242424242424,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06448989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06448989898989899,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.695],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06451515151515151,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06435858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06432323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06476262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06427777777777778,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.07856060606060605,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07878787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07878787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07881313131313132,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7275],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07907575757575758,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07901010101010102,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.0791111111111111,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.715],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07913636363636364,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07923232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07872727272727273,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.0788030303030303,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.07890909090909091,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07896969696969697,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.73],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07906565656565656,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07911616161616161,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.07922727272727273,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.725]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"compute-1-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "print(len(data[\"compute-1-1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['labels', 'CPU1 Temp', 'CPU1 Temp_min', 'CPU1 Temp_max', 'CPU2 Temp', 'CPU2 Temp_min', 'CPU2 Temp_max', 'Inlet Temp', 'Inlet Temp_min', 'Inlet Temp_max', 'Memory usage', 'Memory usage_min', 'Memory usage_max', 'Fan1 speed', 'Fan1 speed_min', 'Fan1 speed_max', 'Fan2 speed', 'Fan2 speed_min', 'Fan2 speed_max', 'Fan3 speed', 'Fan3 speed_min', 'Fan3 speed_max', 'Fan4 speed', 'Fan4 speed_min', 'Fan4 speed_max', 'Power consumption', 'Power consumption_min', 'Power consumption_max', 'mse', 'radius', 'description', '__metrics', 'index', 'axis', 'name', 'text', 'arr', 'total', 'leadername', 'orderG', 'x', 'y', 'vy', 'vx', 'order', 'x2', 'color']"
      ],
      "text/plain": [
       "['labels',\n",
       " 'CPU1 Temp',\n",
       " 'CPU1 Temp_min',\n",
       " 'CPU1 Temp_max',\n",
       " 'CPU2 Temp',\n",
       " 'CPU2 Temp_min',\n",
       " 'CPU2 Temp_max',\n",
       " 'Inlet Temp',\n",
       " 'Inlet Temp_min',\n",
       " 'Inlet Temp_max',\n",
       " 'Memory usage',\n",
       " 'Memory usage_min',\n",
       " 'Memory usage_max',\n",
       " 'Fan1 speed',\n",
       " 'Fan1 speed_min',\n",
       " 'Fan1 speed_max',\n",
       " 'Fan2 speed',\n",
       " 'Fan2 speed_min',\n",
       " 'Fan2 speed_max',\n",
       " 'Fan3 speed',\n",
       " 'Fan3 speed_min',\n",
       " 'Fan3 speed_max',\n",
       " 'Fan4 speed',\n",
       " 'Fan4 speed_min',\n",
       " 'Fan4 speed_max',\n",
       " 'Power consumption',\n",
       " 'Power consumption_min',\n",
       " 'Power consumption_max',\n",
       " 'mse',\n",
       " 'radius',\n",
       " 'description',\n",
       " '__metrics',\n",
       " 'index',\n",
       " 'axis',\n",
       " 'name',\n",
       " 'text',\n",
       " 'arr',\n",
       " 'total',\n",
       " 'leadername',\n",
       " 'orderG',\n",
       " 'x',\n",
       " 'y',\n",
       " 'vy',\n",
       " 'vx',\n",
       " 'order',\n",
       " 'x2',\n",
       " 'color']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = json.load(open('/Users/chaupham/Downloads/17Feb2020_cluster_info.json'))\n",
    "list(data2[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"#1f77b4\",\n",
      "\"#ff7f0e\",\n",
      "\"#2ca02c\",\n",
      "\"#d62728\",\n",
      "\"#9467bd\",\n",
      "\"#8c564b\",\n",
      "\"#e377c2\",\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data2)):\n",
    "    print(f'\"{data2[i][\"color\"]}\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['compute-1-1', 'compute-1-10', 'compute-1-11', 'compute-1-12', 'compute-1-13', 'compute-1-14', 'compute-1-15', 'compute-1-16', 'compute-1-17', 'compute-1-18', 'compute-1-19', 'compute-1-2', 'compute-1-20', 'compute-1-21', 'compute-1-22', 'compute-1-23', 'compute-1-24', 'compute-1-25', 'compute-1-26', 'compute-1-27', 'compute-1-28', 'compute-1-29', 'compute-1-3', 'compute-1-30', 'compute-1-31', 'compute-1-32', 'compute-1-33', 'compute-1-34', 'compute-1-35', 'compute-1-36', 'compute-1-37', 'compute-1-38', 'compute-1-39', 'compute-1-4', 'compute-1-40', 'compute-1-41', 'compute-1-42', 'compute-1-43', 'compute-1-44', 'compute-1-45', 'compute-1-46', 'compute-1-47', 'compute-1-48', 'compute-1-49', 'compute-1-5', 'compute-1-50', 'compute-1-51', 'compute-1-52', 'compute-1-53', 'compute-1-54', 'compute-1-55', 'compute-1-56', 'compute-1-57', 'compute-1-58', 'compute-1-59', 'compute-1-6', 'compute-1-60', 'compute-1-7', 'compute-1-8', 'compute-1-9', 'compute-10-25', 'compute-10-26', 'compute-10-27', 'compute-10-28', 'compute-10-29', 'compute-10-30', 'compute-10-31', 'compute-10-32', 'compute-10-33', 'compute-10-34', 'compute-10-35', 'compute-10-36', 'compute-10-37', 'compute-10-38', 'compute-10-39', 'compute-10-40', 'compute-10-41', 'compute-10-42', 'compute-10-43', 'compute-10-44', 'compute-2-1', 'compute-2-10', 'compute-2-11', 'compute-2-12', 'compute-2-13', 'compute-2-14', 'compute-2-15', 'compute-2-16', 'compute-2-17', 'compute-2-18', 'compute-2-19', 'compute-2-2', 'compute-2-20', 'compute-2-21', 'compute-2-22', 'compute-2-23', 'compute-2-24', 'compute-2-25', 'compute-2-26', 'compute-2-27', 'compute-2-28', 'compute-2-29', 'compute-2-3', 'compute-2-30', 'compute-2-31', 'compute-2-32', 'compute-2-33', 'compute-2-34', 'compute-2-35', 'compute-2-36', 'compute-2-37', 'compute-2-38', 'compute-2-39', 'compute-2-4', 'compute-2-40', 'compute-2-41', 'compute-2-42', 'compute-2-43', 'compute-2-44', 'compute-2-45', 'compute-2-46', 'compute-2-47', 'compute-2-48', 'compute-2-49', 'compute-2-5', 'compute-2-50', 'compute-2-51', 'compute-2-52', 'compute-2-53', 'compute-2-54', 'compute-2-55', 'compute-2-56', 'compute-2-57', 'compute-2-58', 'compute-2-59', 'compute-2-6', 'compute-2-60', 'compute-2-7', 'compute-2-8', 'compute-2-9', 'compute-3-1', 'compute-3-10', 'compute-3-11', 'compute-3-12', 'compute-3-13', 'compute-3-14', 'compute-3-15', 'compute-3-16', 'compute-3-17', 'compute-3-18', 'compute-3-19', 'compute-3-2', 'compute-3-20', 'compute-3-21', 'compute-3-22', 'compute-3-23', 'compute-3-24', 'compute-3-25', 'compute-3-26', 'compute-3-27', 'compute-3-28', 'compute-3-29', 'compute-3-3', 'compute-3-30', 'compute-3-31', 'compute-3-32', 'compute-3-33', 'compute-3-34', 'compute-3-35', 'compute-3-36', 'compute-3-37', 'compute-3-38', 'compute-3-39', 'compute-3-4', 'compute-3-40', 'compute-3-41', 'compute-3-42', 'compute-3-43', 'compute-3-44', 'compute-3-45', 'compute-3-46', 'compute-3-47', 'compute-3-48', 'compute-3-49', 'compute-3-5', 'compute-3-50', 'compute-3-51', 'compute-3-52', 'compute-3-53', 'compute-3-54', 'compute-3-55', 'compute-3-56', 'compute-3-6', 'compute-3-7', 'compute-3-8', 'compute-3-9', 'compute-4-1', 'compute-4-10', 'compute-4-11', 'compute-4-12', 'compute-4-13', 'compute-4-14', 'compute-4-15', 'compute-4-16', 'compute-4-17', 'compute-4-18', 'compute-4-19', 'compute-4-2', 'compute-4-20', 'compute-4-21', 'compute-4-22', 'compute-4-23', 'compute-4-24', 'compute-4-25', 'compute-4-26', 'compute-4-27', 'compute-4-28', 'compute-4-29', 'compute-4-3', 'compute-4-30', 'compute-4-31', 'compute-4-32', 'compute-4-33', 'compute-4-34', 'compute-4-35', 'compute-4-36', 'compute-4-37', 'compute-4-38', 'compute-4-39', 'compute-4-4', 'compute-4-40', 'compute-4-41', 'compute-4-42', 'compute-4-43', 'compute-4-44', 'compute-4-45', 'compute-4-46', 'compute-4-47', 'compute-4-48', 'compute-4-5', 'compute-4-6', 'compute-4-7', 'compute-4-8', 'compute-4-9', 'compute-5-1', 'compute-5-10', 'compute-5-11', 'compute-5-12', 'compute-5-13', 'compute-5-14', 'compute-5-15', 'compute-5-16', 'compute-5-17', 'compute-5-18', 'compute-5-19', 'compute-5-2', 'compute-5-20', 'compute-5-21', 'compute-5-22', 'compute-5-23', 'compute-5-24', 'compute-5-3', 'compute-5-4', 'compute-5-5', 'compute-5-6', 'compute-5-7', 'compute-5-8', 'compute-5-9', 'compute-6-1', 'compute-6-10', 'compute-6-11', 'compute-6-12', 'compute-6-13', 'compute-6-14', 'compute-6-15', 'compute-6-16', 'compute-6-17', 'compute-6-18', 'compute-6-19', 'compute-6-2', 'compute-6-20', 'compute-6-3', 'compute-6-4', 'compute-6-5', 'compute-6-6', 'compute-6-7', 'compute-6-8', 'compute-6-9', 'compute-7-1', 'compute-7-10', 'compute-7-11', 'compute-7-12', 'compute-7-13', 'compute-7-14', 'compute-7-15', 'compute-7-16', 'compute-7-17', 'compute-7-18', 'compute-7-19', 'compute-7-2', 'compute-7-20', 'compute-7-21', 'compute-7-22', 'compute-7-23', 'compute-7-24', 'compute-7-25', 'compute-7-26', 'compute-7-27', 'compute-7-28', 'compute-7-29', 'compute-7-3', 'compute-7-30', 'compute-7-31', 'compute-7-32', 'compute-7-33', 'compute-7-34', 'compute-7-35', 'compute-7-36', 'compute-7-37', 'compute-7-38', 'compute-7-39', 'compute-7-40', 'compute-7-41', 'compute-7-42', 'compute-7-43', 'compute-7-44', 'compute-7-45', 'compute-7-46', 'compute-7-47', 'compute-7-48', 'compute-7-49', 'compute-7-5', 'compute-7-50', 'compute-7-51', 'compute-7-52', 'compute-7-53', 'compute-7-54', 'compute-7-55', 'compute-7-56', 'compute-7-57', 'compute-7-58', 'compute-7-59', 'compute-7-6', 'compute-7-60', 'compute-7-7', 'compute-7-8', 'compute-7-9', 'compute-8-1', 'compute-8-10', 'compute-8-11', 'compute-8-12', 'compute-8-13', 'compute-8-14', 'compute-8-15', 'compute-8-16', 'compute-8-17', 'compute-8-18', 'compute-8-19', 'compute-8-2', 'compute-8-20', 'compute-8-21', 'compute-8-22', 'compute-8-23', 'compute-8-24', 'compute-8-25', 'compute-8-26', 'compute-8-27', 'compute-8-28', 'compute-8-29', 'compute-8-3', 'compute-8-30', 'compute-8-31', 'compute-8-32', 'compute-8-33', 'compute-8-34', 'compute-8-35', 'compute-8-36', 'compute-8-37', 'compute-8-38', 'compute-8-39', 'compute-8-4', 'compute-8-40', 'compute-8-41', 'compute-8-42', 'compute-8-43', 'compute-8-44', 'compute-8-45', 'compute-8-46', 'compute-8-47', 'compute-8-48', 'compute-8-49', 'compute-8-5', 'compute-8-50', 'compute-8-51', 'compute-8-52', 'compute-8-53', 'compute-8-54', 'compute-8-55', 'compute-8-56', 'compute-8-57', 'compute-8-58', 'compute-8-59', 'compute-8-6', 'compute-8-60', 'compute-8-7', 'compute-8-8', 'compute-8-9', 'compute-9-1', 'compute-9-10', 'compute-9-11', 'compute-9-12', 'compute-9-13', 'compute-9-14', 'compute-9-15', 'compute-9-16', 'compute-9-17', 'compute-9-18', 'compute-9-19', 'compute-9-2', 'compute-9-20', 'compute-9-21', 'compute-9-22', 'compute-9-23', 'compute-9-24', 'compute-9-25', 'compute-9-26', 'compute-9-27', 'compute-9-28', 'compute-9-29', 'compute-9-3', 'compute-9-30', 'compute-9-31', 'compute-9-32', 'compute-9-33', 'compute-9-34', 'compute-9-35', 'compute-9-36', 'compute-9-37', 'compute-9-38', 'compute-9-39', 'compute-9-4', 'compute-9-40', 'compute-9-41', 'compute-9-42', 'compute-9-43', 'compute-9-44', 'compute-9-45', 'compute-9-46', 'compute-9-47', 'compute-9-48', 'compute-9-49', 'compute-9-5', 'compute-9-50', 'compute-9-51', 'compute-9-52', 'compute-9-53', 'compute-9-54', 'compute-9-55', 'compute-9-56', 'compute-9-57', 'compute-9-58', 'compute-9-59', 'compute-9-6', 'compute-9-60', 'compute-9-7', 'compute-9-8', 'compute-9-9']"
      ],
      "text/plain": [
       "['compute-1-1',\n",
       " 'compute-1-10',\n",
       " 'compute-1-11',\n",
       " 'compute-1-12',\n",
       " 'compute-1-13',\n",
       " 'compute-1-14',\n",
       " 'compute-1-15',\n",
       " 'compute-1-16',\n",
       " 'compute-1-17',\n",
       " 'compute-1-18',\n",
       " 'compute-1-19',\n",
       " 'compute-1-2',\n",
       " 'compute-1-20',\n",
       " 'compute-1-21',\n",
       " 'compute-1-22',\n",
       " 'compute-1-23',\n",
       " 'compute-1-24',\n",
       " 'compute-1-25',\n",
       " 'compute-1-26',\n",
       " 'compute-1-27',\n",
       " 'compute-1-28',\n",
       " 'compute-1-29',\n",
       " 'compute-1-3',\n",
       " 'compute-1-30',\n",
       " 'compute-1-31',\n",
       " 'compute-1-32',\n",
       " 'compute-1-33',\n",
       " 'compute-1-34',\n",
       " 'compute-1-35',\n",
       " 'compute-1-36',\n",
       " 'compute-1-37',\n",
       " 'compute-1-38',\n",
       " 'compute-1-39',\n",
       " 'compute-1-4',\n",
       " 'compute-1-40',\n",
       " 'compute-1-41',\n",
       " 'compute-1-42',\n",
       " 'compute-1-43',\n",
       " 'compute-1-44',\n",
       " 'compute-1-45',\n",
       " 'compute-1-46',\n",
       " 'compute-1-47',\n",
       " 'compute-1-48',\n",
       " 'compute-1-49',\n",
       " 'compute-1-5',\n",
       " 'compute-1-50',\n",
       " 'compute-1-51',\n",
       " 'compute-1-52',\n",
       " 'compute-1-53',\n",
       " 'compute-1-54',\n",
       " 'compute-1-55',\n",
       " 'compute-1-56',\n",
       " 'compute-1-57',\n",
       " 'compute-1-58',\n",
       " 'compute-1-59',\n",
       " 'compute-1-6',\n",
       " 'compute-1-60',\n",
       " 'compute-1-7',\n",
       " 'compute-1-8',\n",
       " 'compute-1-9',\n",
       " 'compute-10-25',\n",
       " 'compute-10-26',\n",
       " 'compute-10-27',\n",
       " 'compute-10-28',\n",
       " 'compute-10-29',\n",
       " 'compute-10-30',\n",
       " 'compute-10-31',\n",
       " 'compute-10-32',\n",
       " 'compute-10-33',\n",
       " 'compute-10-34',\n",
       " 'compute-10-35',\n",
       " 'compute-10-36',\n",
       " 'compute-10-37',\n",
       " 'compute-10-38',\n",
       " 'compute-10-39',\n",
       " 'compute-10-40',\n",
       " 'compute-10-41',\n",
       " 'compute-10-42',\n",
       " 'compute-10-43',\n",
       " 'compute-10-44',\n",
       " 'compute-2-1',\n",
       " 'compute-2-10',\n",
       " 'compute-2-11',\n",
       " 'compute-2-12',\n",
       " 'compute-2-13',\n",
       " 'compute-2-14',\n",
       " 'compute-2-15',\n",
       " 'compute-2-16',\n",
       " 'compute-2-17',\n",
       " 'compute-2-18',\n",
       " 'compute-2-19',\n",
       " 'compute-2-2',\n",
       " 'compute-2-20',\n",
       " 'compute-2-21',\n",
       " 'compute-2-22',\n",
       " 'compute-2-23',\n",
       " 'compute-2-24',\n",
       " 'compute-2-25',\n",
       " 'compute-2-26',\n",
       " 'compute-2-27',\n",
       " 'compute-2-28',\n",
       " 'compute-2-29',\n",
       " 'compute-2-3',\n",
       " 'compute-2-30',\n",
       " 'compute-2-31',\n",
       " 'compute-2-32',\n",
       " 'compute-2-33',\n",
       " 'compute-2-34',\n",
       " 'compute-2-35',\n",
       " 'compute-2-36',\n",
       " 'compute-2-37',\n",
       " 'compute-2-38',\n",
       " 'compute-2-39',\n",
       " 'compute-2-4',\n",
       " 'compute-2-40',\n",
       " 'compute-2-41',\n",
       " 'compute-2-42',\n",
       " 'compute-2-43',\n",
       " 'compute-2-44',\n",
       " 'compute-2-45',\n",
       " 'compute-2-46',\n",
       " 'compute-2-47',\n",
       " 'compute-2-48',\n",
       " 'compute-2-49',\n",
       " 'compute-2-5',\n",
       " 'compute-2-50',\n",
       " 'compute-2-51',\n",
       " 'compute-2-52',\n",
       " 'compute-2-53',\n",
       " 'compute-2-54',\n",
       " 'compute-2-55',\n",
       " 'compute-2-56',\n",
       " 'compute-2-57',\n",
       " 'compute-2-58',\n",
       " 'compute-2-59',\n",
       " 'compute-2-6',\n",
       " 'compute-2-60',\n",
       " 'compute-2-7',\n",
       " 'compute-2-8',\n",
       " 'compute-2-9',\n",
       " 'compute-3-1',\n",
       " 'compute-3-10',\n",
       " 'compute-3-11',\n",
       " 'compute-3-12',\n",
       " 'compute-3-13',\n",
       " 'compute-3-14',\n",
       " 'compute-3-15',\n",
       " 'compute-3-16',\n",
       " 'compute-3-17',\n",
       " 'compute-3-18',\n",
       " 'compute-3-19',\n",
       " 'compute-3-2',\n",
       " 'compute-3-20',\n",
       " 'compute-3-21',\n",
       " 'compute-3-22',\n",
       " 'compute-3-23',\n",
       " 'compute-3-24',\n",
       " 'compute-3-25',\n",
       " 'compute-3-26',\n",
       " 'compute-3-27',\n",
       " 'compute-3-28',\n",
       " 'compute-3-29',\n",
       " 'compute-3-3',\n",
       " 'compute-3-30',\n",
       " 'compute-3-31',\n",
       " 'compute-3-32',\n",
       " 'compute-3-33',\n",
       " 'compute-3-34',\n",
       " 'compute-3-35',\n",
       " 'compute-3-36',\n",
       " 'compute-3-37',\n",
       " 'compute-3-38',\n",
       " 'compute-3-39',\n",
       " 'compute-3-4',\n",
       " 'compute-3-40',\n",
       " 'compute-3-41',\n",
       " 'compute-3-42',\n",
       " 'compute-3-43',\n",
       " 'compute-3-44',\n",
       " 'compute-3-45',\n",
       " 'compute-3-46',\n",
       " 'compute-3-47',\n",
       " 'compute-3-48',\n",
       " 'compute-3-49',\n",
       " 'compute-3-5',\n",
       " 'compute-3-50',\n",
       " 'compute-3-51',\n",
       " 'compute-3-52',\n",
       " 'compute-3-53',\n",
       " 'compute-3-54',\n",
       " 'compute-3-55',\n",
       " 'compute-3-56',\n",
       " 'compute-3-6',\n",
       " 'compute-3-7',\n",
       " 'compute-3-8',\n",
       " 'compute-3-9',\n",
       " 'compute-4-1',\n",
       " 'compute-4-10',\n",
       " 'compute-4-11',\n",
       " 'compute-4-12',\n",
       " 'compute-4-13',\n",
       " 'compute-4-14',\n",
       " 'compute-4-15',\n",
       " 'compute-4-16',\n",
       " 'compute-4-17',\n",
       " 'compute-4-18',\n",
       " 'compute-4-19',\n",
       " 'compute-4-2',\n",
       " 'compute-4-20',\n",
       " 'compute-4-21',\n",
       " 'compute-4-22',\n",
       " 'compute-4-23',\n",
       " 'compute-4-24',\n",
       " 'compute-4-25',\n",
       " 'compute-4-26',\n",
       " 'compute-4-27',\n",
       " 'compute-4-28',\n",
       " 'compute-4-29',\n",
       " 'compute-4-3',\n",
       " 'compute-4-30',\n",
       " 'compute-4-31',\n",
       " 'compute-4-32',\n",
       " 'compute-4-33',\n",
       " 'compute-4-34',\n",
       " 'compute-4-35',\n",
       " 'compute-4-36',\n",
       " 'compute-4-37',\n",
       " 'compute-4-38',\n",
       " 'compute-4-39',\n",
       " 'compute-4-4',\n",
       " 'compute-4-40',\n",
       " 'compute-4-41',\n",
       " 'compute-4-42',\n",
       " 'compute-4-43',\n",
       " 'compute-4-44',\n",
       " 'compute-4-45',\n",
       " 'compute-4-46',\n",
       " 'compute-4-47',\n",
       " 'compute-4-48',\n",
       " 'compute-4-5',\n",
       " 'compute-4-6',\n",
       " 'compute-4-7',\n",
       " 'compute-4-8',\n",
       " 'compute-4-9',\n",
       " 'compute-5-1',\n",
       " 'compute-5-10',\n",
       " 'compute-5-11',\n",
       " 'compute-5-12',\n",
       " 'compute-5-13',\n",
       " 'compute-5-14',\n",
       " 'compute-5-15',\n",
       " 'compute-5-16',\n",
       " 'compute-5-17',\n",
       " 'compute-5-18',\n",
       " 'compute-5-19',\n",
       " 'compute-5-2',\n",
       " 'compute-5-20',\n",
       " 'compute-5-21',\n",
       " 'compute-5-22',\n",
       " 'compute-5-23',\n",
       " 'compute-5-24',\n",
       " 'compute-5-3',\n",
       " 'compute-5-4',\n",
       " 'compute-5-5',\n",
       " 'compute-5-6',\n",
       " 'compute-5-7',\n",
       " 'compute-5-8',\n",
       " 'compute-5-9',\n",
       " 'compute-6-1',\n",
       " 'compute-6-10',\n",
       " 'compute-6-11',\n",
       " 'compute-6-12',\n",
       " 'compute-6-13',\n",
       " 'compute-6-14',\n",
       " 'compute-6-15',\n",
       " 'compute-6-16',\n",
       " 'compute-6-17',\n",
       " 'compute-6-18',\n",
       " 'compute-6-19',\n",
       " 'compute-6-2',\n",
       " 'compute-6-20',\n",
       " 'compute-6-3',\n",
       " 'compute-6-4',\n",
       " 'compute-6-5',\n",
       " 'compute-6-6',\n",
       " 'compute-6-7',\n",
       " 'compute-6-8',\n",
       " 'compute-6-9',\n",
       " 'compute-7-1',\n",
       " 'compute-7-10',\n",
       " 'compute-7-11',\n",
       " 'compute-7-12',\n",
       " 'compute-7-13',\n",
       " 'compute-7-14',\n",
       " 'compute-7-15',\n",
       " 'compute-7-16',\n",
       " 'compute-7-17',\n",
       " 'compute-7-18',\n",
       " 'compute-7-19',\n",
       " 'compute-7-2',\n",
       " 'compute-7-20',\n",
       " 'compute-7-21',\n",
       " 'compute-7-22',\n",
       " 'compute-7-23',\n",
       " 'compute-7-24',\n",
       " 'compute-7-25',\n",
       " 'compute-7-26',\n",
       " 'compute-7-27',\n",
       " 'compute-7-28',\n",
       " 'compute-7-29',\n",
       " 'compute-7-3',\n",
       " 'compute-7-30',\n",
       " 'compute-7-31',\n",
       " 'compute-7-32',\n",
       " 'compute-7-33',\n",
       " 'compute-7-34',\n",
       " 'compute-7-35',\n",
       " 'compute-7-36',\n",
       " 'compute-7-37',\n",
       " 'compute-7-38',\n",
       " 'compute-7-39',\n",
       " 'compute-7-40',\n",
       " 'compute-7-41',\n",
       " 'compute-7-42',\n",
       " 'compute-7-43',\n",
       " 'compute-7-44',\n",
       " 'compute-7-45',\n",
       " 'compute-7-46',\n",
       " 'compute-7-47',\n",
       " 'compute-7-48',\n",
       " 'compute-7-49',\n",
       " 'compute-7-5',\n",
       " 'compute-7-50',\n",
       " 'compute-7-51',\n",
       " 'compute-7-52',\n",
       " 'compute-7-53',\n",
       " 'compute-7-54',\n",
       " 'compute-7-55',\n",
       " 'compute-7-56',\n",
       " 'compute-7-57',\n",
       " 'compute-7-58',\n",
       " 'compute-7-59',\n",
       " 'compute-7-6',\n",
       " 'compute-7-60',\n",
       " 'compute-7-7',\n",
       " 'compute-7-8',\n",
       " 'compute-7-9',\n",
       " 'compute-8-1',\n",
       " 'compute-8-10',\n",
       " 'compute-8-11',\n",
       " 'compute-8-12',\n",
       " 'compute-8-13',\n",
       " 'compute-8-14',\n",
       " 'compute-8-15',\n",
       " 'compute-8-16',\n",
       " 'compute-8-17',\n",
       " 'compute-8-18',\n",
       " 'compute-8-19',\n",
       " 'compute-8-2',\n",
       " 'compute-8-20',\n",
       " 'compute-8-21',\n",
       " 'compute-8-22',\n",
       " 'compute-8-23',\n",
       " 'compute-8-24',\n",
       " 'compute-8-25',\n",
       " 'compute-8-26',\n",
       " 'compute-8-27',\n",
       " 'compute-8-28',\n",
       " 'compute-8-29',\n",
       " 'compute-8-3',\n",
       " 'compute-8-30',\n",
       " 'compute-8-31',\n",
       " 'compute-8-32',\n",
       " 'compute-8-33',\n",
       " 'compute-8-34',\n",
       " 'compute-8-35',\n",
       " 'compute-8-36',\n",
       " 'compute-8-37',\n",
       " 'compute-8-38',\n",
       " 'compute-8-39',\n",
       " 'compute-8-4',\n",
       " 'compute-8-40',\n",
       " 'compute-8-41',\n",
       " 'compute-8-42',\n",
       " 'compute-8-43',\n",
       " 'compute-8-44',\n",
       " 'compute-8-45',\n",
       " 'compute-8-46',\n",
       " 'compute-8-47',\n",
       " 'compute-8-48',\n",
       " 'compute-8-49',\n",
       " 'compute-8-5',\n",
       " 'compute-8-50',\n",
       " 'compute-8-51',\n",
       " 'compute-8-52',\n",
       " 'compute-8-53',\n",
       " 'compute-8-54',\n",
       " 'compute-8-55',\n",
       " 'compute-8-56',\n",
       " 'compute-8-57',\n",
       " 'compute-8-58',\n",
       " 'compute-8-59',\n",
       " 'compute-8-6',\n",
       " 'compute-8-60',\n",
       " 'compute-8-7',\n",
       " 'compute-8-8',\n",
       " 'compute-8-9',\n",
       " 'compute-9-1',\n",
       " 'compute-9-10',\n",
       " 'compute-9-11',\n",
       " 'compute-9-12',\n",
       " 'compute-9-13',\n",
       " 'compute-9-14',\n",
       " 'compute-9-15',\n",
       " 'compute-9-16',\n",
       " 'compute-9-17',\n",
       " 'compute-9-18',\n",
       " 'compute-9-19',\n",
       " 'compute-9-2',\n",
       " 'compute-9-20',\n",
       " 'compute-9-21',\n",
       " 'compute-9-22',\n",
       " 'compute-9-23',\n",
       " 'compute-9-24',\n",
       " 'compute-9-25',\n",
       " 'compute-9-26',\n",
       " 'compute-9-27',\n",
       " 'compute-9-28',\n",
       " 'compute-9-29',\n",
       " 'compute-9-3',\n",
       " 'compute-9-30',\n",
       " 'compute-9-31',\n",
       " 'compute-9-32',\n",
       " 'compute-9-33',\n",
       " 'compute-9-34',\n",
       " 'compute-9-35',\n",
       " 'compute-9-36',\n",
       " 'compute-9-37',\n",
       " 'compute-9-38',\n",
       " 'compute-9-39',\n",
       " 'compute-9-4',\n",
       " 'compute-9-40',\n",
       " 'compute-9-41',\n",
       " 'compute-9-42',\n",
       " 'compute-9-43',\n",
       " 'compute-9-44',\n",
       " 'compute-9-45',\n",
       " 'compute-9-46',\n",
       " 'compute-9-47',\n",
       " 'compute-9-48',\n",
       " 'compute-9-49',\n",
       " 'compute-9-5',\n",
       " 'compute-9-50',\n",
       " 'compute-9-51',\n",
       " 'compute-9-52',\n",
       " 'compute-9-53',\n",
       " 'compute-9-54',\n",
       " 'compute-9-55',\n",
       " 'compute-9-56',\n",
       " 'compute-9-57',\n",
       " 'compute-9-58',\n",
       " 'compute-9-59',\n",
       " 'compute-9-6',\n",
       " 'compute-9-60',\n",
       " 'compute-9-7',\n",
       " 'compute-9-8',\n",
       " 'compute-9-9']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = json.load(open('/Users/chaupham/Downloads/17Feb2020_clusterarr.json'))\n",
    "list(label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ],
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['compute-1-1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['CPU1 Temp', 'CPU2 Temp', 'Inlet Temp', 'Memory usage', 'Fan1 speed', 'Fan2 speed', 'Fan3 speed', 'Fan4 speed', 'Power consumption']"
      ],
      "text/plain": [
       "['CPU1 Temp',\n",
       " 'CPU2 Temp',\n",
       " 'Inlet Temp',\n",
       " 'Memory usage',\n",
       " 'Fan1 speed',\n",
       " 'Fan2 speed',\n",
       " 'Fan3 speed',\n",
       " 'Fan4 speed',\n",
       " 'Power consumption']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = list(map(lambda x: x.split(\"_\")[0] ,['CPU1 Temp',\n",
    " 'CPU1 Temp_min',\n",
    " 'CPU1 Temp_max',\n",
    " 'CPU2 Temp',\n",
    " 'CPU2 Temp_min',\n",
    " 'CPU2 Temp_max',\n",
    " 'Inlet Temp',\n",
    " 'Inlet Temp_min',\n",
    " 'Inlet Temp_max',\n",
    " 'Memory usage',\n",
    " 'Memory usage_min',\n",
    " 'Memory usage_max',\n",
    " 'Fan1 speed',\n",
    " 'Fan1 speed_min',\n",
    " 'Fan1 speed_max',\n",
    " 'Fan2 speed',\n",
    " 'Fan2 speed_min',\n",
    " 'Fan2 speed_max',\n",
    " 'Fan3 speed',\n",
    " 'Fan3 speed_min',\n",
    " 'Fan3 speed_max',\n",
    " 'Fan4 speed',\n",
    " 'Fan4 speed_min',\n",
    " 'Fan4 speed_max',\n",
    " 'Power consumption',\n",
    " 'Power consumption_min',\n",
    " 'Power consumption_max']))\n",
    "\n",
    "from  more_itertools import unique_everseen\n",
    "feature_names = list(unique_everseen(feature_names))\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num computes = 467\n"
     ]
    }
   ],
   "source": [
    "data_list = list()\n",
    "for key in data.keys():\n",
    "    tmp_data = data[key]\n",
    "    tmp_label = label[key]\n",
    "    tmp  =[data_ + [label_] for data_, label_ in zip(tmp_data, tmp_label)]\n",
    "    data_list.append(pd.DataFrame(data=tmp, columns=feature_names + [\"cluster\"]))\n",
    "\n",
    "print(\"Num computes =\",len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    260\n",
       "2     28\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0][\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[1][\"cluster\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict ={0:\"#1f77b4\",\n",
    "1:\"#ff7f0e\",\n",
    "2:\"#2ca02c\",\n",
    "3:\"#d62728\",\n",
    "4:\"#9467bd\",\n",
    "5:\"#8c564b\",\n",
    "6:\"#e377c2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb_and_shap(name, i, df, feature_names, train_error_list, valid_error_list, skip_list, colors_dict, lgb_params = None,):\n",
    "    while df[\"cluster\"].value_counts().min() == 1:\n",
    "        clus = df[\"cluster\"].value_counts().reset_index().tail(1)[\"index\"].values[0]\n",
    "        df = df[df.cluster != clus]\n",
    "        \n",
    "    if  df[\"cluster\"].nunique() <= 1:\n",
    "        print(\"Just 1 class. Skipped!\")\n",
    "        skip_list.append(name + \"_\" + str(i))\n",
    "        print(\"\\n\")\n",
    "        return\n",
    "        \n",
    "    elif df[\"cluster\"].nunique() == 2:\n",
    "        metric = \"binary_logloss\"\n",
    "        objective = \"binary\"\n",
    "    else:\n",
    "        metric = \"multi_logloss\"\n",
    "        objective = \"multiclass\"\n",
    "        \n",
    "    print('df[\"cluster\"].nunique()',  df[\"cluster\"].nunique())\n",
    "    class_order = list(set(df.cluster))\n",
    "    \n",
    "    # def run_lgb(df)\n",
    "    feature_cols = feature_names\n",
    "    early_stopping = 200\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df[feature_cols], df[\"cluster\"], test_size=0.33, random_state=2020, stratify = df[\"cluster\"])\n",
    "    random_seed = 2020\n",
    "\n",
    "    ## prepare the model\n",
    "    if lgb_params == None:\n",
    "        lgb_params = {\n",
    "            'boosting_type':'gbdt', 'colsample_bytree':0.8, #'class_weight': {0 : 1. , 1: weight},\n",
    "            'importance_type':'gain', 'learning_rate':0.01, 'max_depth':2,\n",
    "            'min_child_samples':15,# 'min_child_weight':0.001, 'min_split_gain':0.0,\n",
    "            'n_estimators':3000, 'n_jobs': 16, 'num_leaves':31, 'subsample_freq':16,\n",
    "            'seed': random_seed, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "            'subsample':.9, 'subsample_for_bin':200000,  \"metric\":metric ,'objective':objective\n",
    "        }\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    record_store = dict()\n",
    "    lgb_model.fit( X=X_train, y=y_train, feature_name = feature_cols, #categorical_feature = cate_cols, \n",
    "                  early_stopping_rounds= early_stopping, eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "                  eval_names=[\"train\", \"valid\"],\n",
    "                  eval_metric= \"multi_logloss\",\n",
    "                  verbose = -1, callbacks = [lgb.record_evaluation(record_store)])\n",
    "\n",
    "    pred_train = lgb_model.predict(X_train)\n",
    "    pred_valid = lgb_model.predict(X_valid)\n",
    "\n",
    "    train_error_list.append(accuracy_score(y_train, pred_train))\n",
    "    valid_error_list.append(accuracy_score(y_valid, pred_valid))\n",
    "    \n",
    "    len_cluster = df[\"cluster\"].nunique()\n",
    "    explainer = shap.TreeExplainer(lgb_model)\n",
    "    shap_values = explainer.shap_values(X=X_valid.head(1), y=y_valid.head(1))\n",
    "\n",
    "    shap_values_np = np.array(shap_values)\n",
    "\n",
    "    shap_importance = pd.DataFrame({\"feature_name\": feature_cols})\n",
    "    for i, cluster in enumerate(class_order):\n",
    "        shap_class_i = pd.DataFrame({\"feature_name\": feature_cols, cluster: np.abs(shap_values[i]).sum(axis=0)})\n",
    "        shap_importance = pd.merge(shap_importance, shap_class_i, on=\"feature_name\", how=\"inner\")\n",
    "\n",
    "    tmp = shap_importance.drop(\"feature_name\", 1).sum(axis=1).sort_values(ascending=True).index\n",
    "    shap_importance = shap_importance.reindex(tmp)\n",
    "    shap_importance.to_csv(f\"shap_dataframe/shap_importance_{name}.csv\", index=False)\n",
    "    \n",
    "    colors = list()\n",
    "    for c in class_order:\n",
    "        colors.append(colors_dict[c])\n",
    "\n",
    "    # get class ordering from shap values\n",
    "    class_inds = np.argsort([-np.abs(shap_values[i]).mean() for i in range(len(shap_values))])\n",
    "\n",
    "    # create listed colormap\n",
    "    cmap = plt_colors.ListedColormap(np.array(colors)[class_inds])\n",
    "\n",
    "    shap.summary_plot(shap_values, X_valid.head(1), max_display =  len(feature_cols), show=False, \n",
    "                      plot_type = \"bar\", color=cmap,\n",
    "                      class_names=[\"cluster \" + str(i) for i in class_order])\n",
    "    \n",
    "    plt.savefig(f\"shap_pictures/dot_plot_{name}\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttrain's binary_logloss: 0.0006435\tvalid's binary_logloss: 0.0042481\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttrain's binary_logloss: 0.0418978\tvalid's binary_logloss: 0.0400076\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttrain's multi_logloss: 0.0184096\tvalid's multi_logloss: 0.124481\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttrain's binary_logloss: 0.00352904\tvalid's binary_logloss: 0.0192643\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttrain's binary_logloss: 0.0108341\tvalid's binary_logloss: 0.0536556\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's binary_logloss: 0.0322903\tvalid's binary_logloss: 0.0600412\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1072]\ttrain's multi_logloss: 0.000946029\tvalid's multi_logloss: 0.00166458\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[542]\ttrain's multi_logloss: 0.038044\tvalid's multi_logloss: 0.0558807\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[468]\ttrain's binary_logloss: 0.00437022\tvalid's binary_logloss: 0.038548\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttrain's binary_logloss: 0.0583895\tvalid's binary_logloss: 0.0758699\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[544]\ttrain's multi_logloss: 0.0408563\tvalid's multi_logloss: 0.0819128\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's binary_logloss: 0.0322152\tvalid's binary_logloss: 0.0600532\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[661]\ttrain's multi_logloss: 0.0197739\tvalid's multi_logloss: 0.0789935\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttrain's binary_logloss: 0.0171922\tvalid's binary_logloss: 0.0508588\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[215]\ttrain's multi_logloss: 0.0972085\tvalid's multi_logloss: 0.143137\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[991]\ttrain's binary_logloss: 1.17271e-05\tvalid's binary_logloss: 1.04766e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttrain's binary_logloss: 0.0153215\tvalid's binary_logloss: 0.0587957\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's binary_logloss: 1.15161e-05\tvalid's binary_logloss: 9.85563e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2325]\ttrain's binary_logloss: 2.15455e-05\tvalid's binary_logloss: 8.42678e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0194145\tvalid's binary_logloss: 0.0298663\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[544]\ttrain's multi_logloss: 0.0223173\tvalid's multi_logloss: 0.0487411\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1891]\ttrain's binary_logloss: 3.48974e-05\tvalid's binary_logloss: 0.00108648\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttrain's binary_logloss: 0.0600349\tvalid's binary_logloss: 0.0643009\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1522]\ttrain's binary_logloss: 0.000145775\tvalid's binary_logloss: 0.00027969\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[768]\ttrain's multi_logloss: 0.0126208\tvalid's multi_logloss: 0.0345973\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[848]\ttrain's binary_logloss: 0.0128909\tvalid's binary_logloss: 0.0137042\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[983]\ttrain's binary_logloss: 1.13771e-05\tvalid's binary_logloss: 1.17115e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[945]\ttrain's multi_logloss: 0.00245201\tvalid's multi_logloss: 0.0329904\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttrain's binary_logloss: 0.0168548\tvalid's binary_logloss: 0.048899\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[578]\ttrain's binary_logloss: 0.00520671\tvalid's binary_logloss: 0.0110625\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[385]\ttrain's multi_logloss: 0.0384247\tvalid's multi_logloss: 0.0661381\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttrain's multi_logloss: 0.0226938\tvalid's multi_logloss: 0.116554\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[736]\ttrain's binary_logloss: 0.0179672\tvalid's binary_logloss: 0.1176\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttrain's binary_logloss: 0.0252943\tvalid's binary_logloss: 0.0258662\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[642]\ttrain's binary_logloss: 0.00116128\tvalid's binary_logloss: 0.0222591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1502]\ttrain's binary_logloss: 1.73606e-05\tvalid's binary_logloss: 1.85651e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttrain's binary_logloss: 0.00474392\tvalid's binary_logloss: 0.03724\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1193]\ttrain's multi_logloss: 8.68821e-06\tvalid's multi_logloss: 8.87121e-06\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[672]\ttrain's binary_logloss: 0.00144391\tvalid's binary_logloss: 0.027171\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[561]\ttrain's binary_logloss: 0.0192825\tvalid's binary_logloss: 0.0631404\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 9.13003e-06\tvalid's multi_logloss: 2.22004e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[608]\ttrain's multi_logloss: 0.0333314\tvalid's multi_logloss: 0.061615\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1312]\ttrain's binary_logloss: 0.000277061\tvalid's binary_logloss: 0.00566797\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2545]\ttrain's binary_logloss: 1.92311e-05\tvalid's binary_logloss: 0.00346643\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1520]\ttrain's binary_logloss: 2.02671e-05\tvalid's binary_logloss: 0.000207613\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2064]\ttrain's multi_logloss: 0.0205454\tvalid's multi_logloss: 0.0321171\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[881]\ttrain's binary_logloss: 0.000422398\tvalid's binary_logloss: 0.00255245\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttrain's binary_logloss: 0.0207002\tvalid's binary_logloss: 0.0270064\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[496]\ttrain's binary_logloss: 0.0228585\tvalid's binary_logloss: 0.0292074\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[848]\ttrain's multi_logloss: 0.00246918\tvalid's multi_logloss: 0.0305797\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[672]\ttrain's binary_logloss: 0.00214625\tvalid's binary_logloss: 0.023677\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[592]\ttrain's multi_logloss: 0.00372049\tvalid's multi_logloss: 0.0273035\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[960]\ttrain's multi_logloss: 0.00917246\tvalid's multi_logloss: 0.0226684\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1040]\ttrain's binary_logloss: 1.11836e-05\tvalid's binary_logloss: 1.17047e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0188961\tvalid's binary_logloss: 0.0313876\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1843]\ttrain's binary_logloss: 2.12832e-05\tvalid's binary_logloss: 4.34909e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttrain's binary_logloss: 0.0172194\tvalid's binary_logloss: 0.0589416\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1840]\ttrain's binary_logloss: 0.00642548\tvalid's binary_logloss: 0.0682498\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[391]\ttrain's multi_logloss: 0.0365747\tvalid's multi_logloss: 0.0914069\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[208]\ttrain's binary_logloss: 0.0450903\tvalid's binary_logloss: 0.045458\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1334]\ttrain's multi_logloss: 9.16231e-06\tvalid's multi_logloss: 1.29618e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1168]\ttrain's multi_logloss: 0.014107\tvalid's multi_logloss: 0.0345079\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[848]\ttrain's multi_logloss: 0.00822505\tvalid's multi_logloss: 0.0209363\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1857]\ttrain's binary_logloss: 3.29365e-05\tvalid's binary_logloss: 0.000189117\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[967]\ttrain's binary_logloss: 1.16293e-05\tvalid's binary_logloss: 1.11898e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1425]\ttrain's binary_logloss: 1.38677e-05\tvalid's binary_logloss: 7.92847e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1248]\ttrain's binary_logloss: 0.0201482\tvalid's binary_logloss: 0.0238816\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[357]\ttrain's multi_logloss: 0.0166951\tvalid's multi_logloss: 0.0454001\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[769]\ttrain's multi_logloss: 0.0529502\tvalid's multi_logloss: 0.102586\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[449]\ttrain's multi_logloss: 0.0231632\tvalid's multi_logloss: 0.0637419\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[832]\ttrain's multi_logloss: 0.00754691\tvalid's multi_logloss: 0.085582\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[566]\ttrain's binary_logloss: 0.00201649\tvalid's binary_logloss: 0.0437178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1056]\ttrain's binary_logloss: 1.16757e-05\tvalid's binary_logloss: 1.01045e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1569]\ttrain's multi_logloss: 0.0159658\tvalid's multi_logloss: 0.0103695\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttrain's binary_logloss: 0.0119292\tvalid's binary_logloss: 0.0206729\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttrain's binary_logloss: 0.0346829\tvalid's binary_logloss: 0.0318692\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[283]\ttrain's binary_logloss: 0.0272458\tvalid's binary_logloss: 0.0295327\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[416]\ttrain's binary_logloss: 0.0180185\tvalid's binary_logloss: 0.067933\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0169774\tvalid's binary_logloss: 0.0304048\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[352]\ttrain's binary_logloss: 0.0108969\tvalid's binary_logloss: 0.015073\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[304]\ttrain's binary_logloss: 0.01526\tvalid's binary_logloss: 0.0551472\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1472]\ttrain's binary_logloss: 0.0220198\tvalid's binary_logloss: 0.00961314\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.81235e-05\tvalid's binary_logloss: 2.11265e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttrain's multi_logloss: 0.0269288\tvalid's multi_logloss: 0.0364909\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[491]\ttrain's binary_logloss: 0.00486927\tvalid's binary_logloss: 0.0416858\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[534]\ttrain's binary_logloss: 0.00291822\tvalid's binary_logloss: 0.0307252\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[703]\ttrain's binary_logloss: 0.00118706\tvalid's binary_logloss: 0.0057854\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2279]\ttrain's binary_logloss: 1.79174e-05\tvalid's binary_logloss: 0.0003038\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttrain's multi_logloss: 0.0160123\tvalid's multi_logloss: 0.0739319\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttrain's multi_logloss: 0.0179807\tvalid's multi_logloss: 0.0572154\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[784]\ttrain's multi_logloss: 0.00596687\tvalid's multi_logloss: 0.0475542\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\ttrain's multi_logloss: 0.0363036\tvalid's multi_logloss: 0.0565128\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttrain's binary_logloss: 0.00296939\tvalid's binary_logloss: 0.0386417\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1077]\ttrain's binary_logloss: 1.16017e-05\tvalid's binary_logloss: 1.09594e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1232]\ttrain's binary_logloss: 1.20725e-05\tvalid's binary_logloss: 6.98545e-06\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1494]\ttrain's binary_logloss: 1.12627e-05\tvalid's binary_logloss: 1.45744e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[511]\ttrain's binary_logloss: 0.00723088\tvalid's binary_logloss: 0.0426267\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's binary_logloss: 1.1574e-05\tvalid's binary_logloss: 1.25903e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[552]\ttrain's binary_logloss: 0.0108571\tvalid's binary_logloss: 0.0736752\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[433]\ttrain's binary_logloss: 0.0314112\tvalid's binary_logloss: 0.0377844\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[526]\ttrain's binary_logloss: 0.00773046\tvalid's binary_logloss: 0.0286393\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1081]\ttrain's binary_logloss: 1.19404e-05\tvalid's binary_logloss: 1.64167e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1888]\ttrain's multi_logloss: 1.54482e-05\tvalid's multi_logloss: 0.000359449\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2452]\ttrain's binary_logloss: 1.70225e-05\tvalid's binary_logloss: 5.36933e-06\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[592]\ttrain's multi_logloss: 0.0411204\tvalid's multi_logloss: 0.0714316\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 0.00106383\tvalid's binary_logloss: 0.00766664\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1600]\ttrain's binary_logloss: 0.0160764\tvalid's binary_logloss: 0.00349804\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[556]\ttrain's binary_logloss: 0.00771472\tvalid's binary_logloss: 0.0221322\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1001]\ttrain's binary_logloss: 1.17977e-05\tvalid's binary_logloss: 1.21855e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[888]\ttrain's binary_logloss: 0.000550652\tvalid's binary_logloss: 0.00621148\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2080]\ttrain's binary_logloss: 3.01916e-05\tvalid's binary_logloss: 8.01532e-06\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[990]\ttrain's binary_logloss: 1.16901e-05\tvalid's binary_logloss: 1.1688e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1018]\ttrain's binary_logloss: 1.31733e-05\tvalid's binary_logloss: 1.12178e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[574]\ttrain's binary_logloss: 0.0155782\tvalid's binary_logloss: 0.0354538\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1088]\ttrain's multi_logloss: 0.0142663\tvalid's multi_logloss: 0.0923621\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[381]\ttrain's binary_logloss: 0.0222722\tvalid's binary_logloss: 0.0909542\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[602]\ttrain's multi_logloss: 0.0106953\tvalid's multi_logloss: 0.0691801\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[560]\ttrain's multi_logloss: 0.00763195\tvalid's multi_logloss: 0.0359515\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttrain's multi_logloss: 0.0303679\tvalid's multi_logloss: 0.0856867\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[253]\ttrain's binary_logloss: 0.00960242\tvalid's binary_logloss: 0.0442477\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1536]\ttrain's multi_logloss: 2.62377e-05\tvalid's multi_logloss: 7.99401e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[465]\ttrain's binary_logloss: 0.0115487\tvalid's binary_logloss: 0.0393298\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1450]\ttrain's binary_logloss: 1.53366e-05\tvalid's binary_logloss: 7.00085e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[673]\ttrain's multi_logloss: 0.00296906\tvalid's multi_logloss: 0.0339962\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\ttrain's binary_logloss: 0.0143143\tvalid's binary_logloss: 0.0265831\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1776]\ttrain's binary_logloss: 2.56448e-05\tvalid's binary_logloss: 0.00206362\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1231]\ttrain's binary_logloss: 1.51604e-05\tvalid's binary_logloss: 1.29421e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[597]\ttrain's multi_logloss: 0.0287912\tvalid's multi_logloss: 0.0289324\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1040]\ttrain's binary_logloss: 0.00792275\tvalid's binary_logloss: 0.0062488\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1760]\ttrain's binary_logloss: 3.17348e-05\tvalid's binary_logloss: 0.000118416\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1028]\ttrain's binary_logloss: 1.15011e-05\tvalid's binary_logloss: 1.16223e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[462]\ttrain's binary_logloss: 0.00520579\tvalid's binary_logloss: 0.0393495\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[528]\ttrain's binary_logloss: 0.00907714\tvalid's binary_logloss: 0.0192989\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2124]\ttrain's binary_logloss: 3.50434e-05\tvalid's binary_logloss: 3.68003e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1056]\ttrain's multi_logloss: 0.0119722\tvalid's multi_logloss: 0.00280994\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttrain's binary_logloss: 0.0105681\tvalid's binary_logloss: 0.0424089\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[974]\ttrain's binary_logloss: 1.16787e-05\tvalid's binary_logloss: 1.51492e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1424]\ttrain's binary_logloss: 0.022709\tvalid's binary_logloss: 0.00785516\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[672]\ttrain's binary_logloss: 0.00147952\tvalid's binary_logloss: 0.0349807\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[479]\ttrain's binary_logloss: 0.00810914\tvalid's binary_logloss: 0.0282772\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1626]\ttrain's binary_logloss: 0.00139776\tvalid's binary_logloss: 0.00267675\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttrain's binary_logloss: 0.023108\tvalid's binary_logloss: 0.0291798\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1920]\ttrain's multi_logloss: 7.57176e-06\tvalid's multi_logloss: 1.1889e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttrain's binary_logloss: 0.0171308\tvalid's binary_logloss: 0.0520398\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttrain's binary_logloss: 0.0242418\tvalid's binary_logloss: 0.022911\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[896]\ttrain's multi_logloss: 0.0189913\tvalid's multi_logloss: 0.0330642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2237]\ttrain's binary_logloss: 3.16256e-05\tvalid's binary_logloss: 0.000179658\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[416]\ttrain's binary_logloss: 0.00881546\tvalid's binary_logloss: 0.0410806\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[484]\ttrain's multi_logloss: 0.0299981\tvalid's multi_logloss: 0.0605508\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[336]\ttrain's binary_logloss: 0.0154119\tvalid's binary_logloss: 0.018657\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[752]\ttrain's multi_logloss: 0.00788572\tvalid's multi_logloss: 0.0347475\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1088]\ttrain's binary_logloss: 1.15783e-05\tvalid's binary_logloss: 1.14064e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttrain's binary_logloss: 0.0123383\tvalid's binary_logloss: 0.0230248\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[590]\ttrain's multi_logloss: 0.00961404\tvalid's multi_logloss: 0.0731811\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1184]\ttrain's binary_logloss: 0.0252728\tvalid's binary_logloss: 0.0271802\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttrain's binary_logloss: 0.0129216\tvalid's binary_logloss: 0.0295538\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1106]\ttrain's binary_logloss: 1.28063e-05\tvalid's binary_logloss: 7.07326e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[448]\ttrain's binary_logloss: 0.00897004\tvalid's binary_logloss: 0.0884396\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1862]\ttrain's binary_logloss: 1.76003e-05\tvalid's binary_logloss: 5.39817e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2336]\ttrain's binary_logloss: 1.68247e-05\tvalid's binary_logloss: 1.64282e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 1.1925e-05\tvalid's multi_logloss: 0.000143663\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[401]\ttrain's binary_logloss: 0.025816\tvalid's binary_logloss: 0.0241926\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[403]\ttrain's multi_logloss: 0.0449063\tvalid's multi_logloss: 0.12344\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1088]\ttrain's multi_logloss: 0.00676015\tvalid's multi_logloss: 0.034121\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttrain's binary_logloss: 0.00212049\tvalid's binary_logloss: 0.0355803\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttrain's binary_logloss: 0.0381415\tvalid's binary_logloss: 0.0310768\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[448]\ttrain's multi_logloss: 0.0210351\tvalid's multi_logloss: 0.0474018\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[752]\ttrain's binary_logloss: 0.0372479\tvalid's binary_logloss: 0.0116034\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1874]\ttrain's binary_logloss: 1.63117e-05\tvalid's binary_logloss: 2.27125e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[880]\ttrain's multi_logloss: 0.00683532\tvalid's multi_logloss: 0.044352\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1728]\ttrain's binary_logloss: 1.6327e-05\tvalid's binary_logloss: 1.60641e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[562]\ttrain's multi_logloss: 0.00387012\tvalid's multi_logloss: 0.049798\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1152]\ttrain's multi_logloss: 0.0156124\tvalid's multi_logloss: 0.00566625\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[250]\ttrain's binary_logloss: 0.0206131\tvalid's binary_logloss: 0.0857512\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[788]\ttrain's multi_logloss: 0.00262318\tvalid's multi_logloss: 0.038788\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttrain's multi_logloss: 0.0092416\tvalid's multi_logloss: 0.0445353\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[608]\ttrain's multi_logloss: 0.0311525\tvalid's multi_logloss: 0.0589123\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[352]\ttrain's binary_logloss: 0.00333563\tvalid's binary_logloss: 0.0271248\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 1.03278e-05\tvalid's multi_logloss: 1.10821e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[208]\ttrain's binary_logloss: 0.041415\tvalid's binary_logloss: 0.0506624\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's multi_logloss: 0.0490525\tvalid's multi_logloss: 0.21473\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttrain's binary_logloss: 0.00753678\tvalid's binary_logloss: 0.0608162\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[528]\ttrain's multi_logloss: 0.0165462\tvalid's multi_logloss: 0.0352185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[688]\ttrain's multi_logloss: 0.00912379\tvalid's multi_logloss: 0.0590447\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\ttrain's binary_logloss: 0.00498292\tvalid's binary_logloss: 0.0311958\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\ttrain's binary_logloss: 0.00482755\tvalid's binary_logloss: 0.0335159\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1082]\ttrain's binary_logloss: 1.13507e-05\tvalid's binary_logloss: 1.03517e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttrain's binary_logloss: 0.0678803\tvalid's binary_logloss: 0.0830578\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1024]\ttrain's multi_logloss: 0.015221\tvalid's multi_logloss: 0.0528089\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1699]\ttrain's binary_logloss: 0.0085162\tvalid's binary_logloss: 0.00648823\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[640]\ttrain's multi_logloss: 0.0155192\tvalid's multi_logloss: 0.0273146\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[301]\ttrain's binary_logloss: 0.0129588\tvalid's binary_logloss: 0.0244458\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0183667\tvalid's binary_logloss: 0.0058465\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1030]\ttrain's binary_logloss: 1.19528e-05\tvalid's binary_logloss: 1.49171e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1066]\ttrain's binary_logloss: 1.23685e-05\tvalid's binary_logloss: 0.000128883\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2669]\ttrain's binary_logloss: 2.69316e-05\tvalid's binary_logloss: 5.727e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1049]\ttrain's binary_logloss: 1.1566e-05\tvalid's binary_logloss: 1.16243e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\ttrain's binary_logloss: 0.00895286\tvalid's binary_logloss: 0.0382493\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0138396\tvalid's binary_logloss: 0.00657553\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[912]\ttrain's multi_logloss: 0.0193016\tvalid's multi_logloss: 0.0131944\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[484]\ttrain's multi_logloss: 0.0215924\tvalid's multi_logloss: 0.0652498\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 7.8994e-05\tvalid's multi_logloss: 0.000145722\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[881]\ttrain's binary_logloss: 0.0578116\tvalid's binary_logloss: 0.120648\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1061]\ttrain's binary_logloss: 1.20896e-05\tvalid's binary_logloss: 1.28311e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2225]\ttrain's binary_logloss: 3.10893e-05\tvalid's binary_logloss: 0.000184434\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's multi_logloss: 0.0347599\tvalid's multi_logloss: 0.0477693\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttrain's binary_logloss: 0.00940328\tvalid's binary_logloss: 0.0204082\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1246]\ttrain's binary_logloss: 2.30331e-05\tvalid's binary_logloss: 0.000566791\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1888]\ttrain's binary_logloss: 0.0700815\tvalid's binary_logloss: 0.199043\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[544]\ttrain's binary_logloss: 0.00188628\tvalid's binary_logloss: 0.0071034\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1408]\ttrain's binary_logloss: 0.0179827\tvalid's binary_logloss: 0.0190431\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[720]\ttrain's binary_logloss: 0.00930785\tvalid's binary_logloss: 0.0588468\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1603]\ttrain's multi_logloss: 0.0105209\tvalid's multi_logloss: 0.0272961\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[392]\ttrain's binary_logloss: 0.00548444\tvalid's binary_logloss: 0.0310231\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1104]\ttrain's multi_logloss: 0.00384974\tvalid's multi_logloss: 0.0337019\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[926]\ttrain's binary_logloss: 0.00133422\tvalid's binary_logloss: 0.0088842\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1312]\ttrain's binary_logloss: 2.23018e-05\tvalid's binary_logloss: 0.000156937\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 2.31271e-05\tvalid's multi_logloss: 2.02819e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1696]\ttrain's binary_logloss: 0.0176968\tvalid's binary_logloss: 0.0184447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1600]\ttrain's binary_logloss: 3.41332e-05\tvalid's binary_logloss: 0.00443354\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's binary_logloss: 1.19514e-05\tvalid's binary_logloss: 1.19803e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[717]\ttrain's multi_logloss: 0.0065747\tvalid's multi_logloss: 0.0324986\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[441]\ttrain's binary_logloss: 0.00832995\tvalid's binary_logloss: 0.015967\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[768]\ttrain's binary_logloss: 0.00177614\tvalid's binary_logloss: 0.0080652\n",
      "\n",
      "\n",
      "256.15 seconds\n"
     ]
    }
   ],
   "source": [
    "train_error_list = list()\n",
    "valid_error_list = list()\n",
    "skip_list = list()\n",
    "\n",
    "\n",
    "\n",
    "tick = time.time()\n",
    "\n",
    "for i, name in enumerate(list(data.keys())):\n",
    "    run_lgb_and_shap(name, i, data_list[i], feature_names = feature_names, train_error_list = train_error_list, valid_error_list=valid_error_list, skip_list=skip_list, colors_dict=colors_dict)\n",
    "\n",
    "tock = time.time()\n",
    "print(f\"{np.round(tock - tick, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['compute-1-10_1', 'compute-1-13_4', 'compute-1-14_5', 'compute-1-18_9', 'compute-1-19_10', 'compute-1-22_14', 'compute-1-23_15', 'compute-1-24_16', 'compute-1-26_18', 'compute-1-29_21', 'compute-1-3_22', 'compute-1-30_23', 'compute-1-31_24', 'compute-1-34_27', 'compute-1-36_29', 'compute-1-37_30', 'compute-1-4_33', 'compute-1-40_34', 'compute-1-42_36', 'compute-1-43_37', 'compute-1-45_39', 'compute-1-46_40', 'compute-1-47_41', 'compute-1-49_43', 'compute-1-53_48', 'compute-1-55_50', 'compute-1-58_53', 'compute-1-59_54', 'compute-1-6_55', 'compute-1-60_56', 'compute-1-7_57', 'compute-1-8_58', 'compute-10-26_61', 'compute-10-27_62', 'compute-10-28_63', 'compute-10-30_65', 'compute-10-31_66', 'compute-10-33_68', 'compute-10-34_69', 'compute-10-35_70', 'compute-10-36_71', 'compute-10-37_72', 'compute-10-38_73', 'compute-10-39_74', 'compute-10-40_75', 'compute-10-41_76', 'compute-10-44_79', 'compute-2-1_80', 'compute-2-10_81', 'compute-2-12_83', 'compute-2-19_90', 'compute-2-2_91', 'compute-2-20_92', 'compute-2-22_94', 'compute-2-26_98', 'compute-2-29_101', 'compute-2-30_103', 'compute-2-32_105', 'compute-2-33_106', 'compute-2-37_110', 'compute-2-4_113', 'compute-2-40_114', 'compute-2-42_116', 'compute-2-43_117', 'compute-2-46_120', 'compute-2-49_123', 'compute-2-5_124', 'compute-2-50_125', 'compute-2-53_128', 'compute-2-54_129', 'compute-2-56_131', 'compute-2-57_132', 'compute-2-60_136', 'compute-2-9_139', 'compute-3-10_141', 'compute-3-11_142', 'compute-3-13_144', 'compute-3-14_145', 'compute-3-15_146', 'compute-3-16_147', 'compute-3-17_148', 'compute-3-19_150', 'compute-3-2_151', 'compute-3-20_152', 'compute-3-22_154', 'compute-3-23_155', 'compute-3-25_157', 'compute-3-26_158', 'compute-3-29_161', 'compute-3-31_164', 'compute-3-33_166', 'compute-3-35_168', 'compute-3-36_169', 'compute-3-40_174', 'compute-3-44_178', 'compute-3-45_179', 'compute-3-46_180', 'compute-3-5_184', 'compute-3-51_186', 'compute-3-53_188', 'compute-3-54_189', 'compute-3-55_190', 'compute-3-7_193', 'compute-3-8_194', 'compute-3-9_195', 'compute-4-1_196', 'compute-4-10_197', 'compute-4-12_199', 'compute-4-14_201', 'compute-4-18_205', 'compute-4-19_206', 'compute-4-2_207', 'compute-4-22_210', 'compute-4-23_211', 'compute-4-24_212', 'compute-4-27_215', 'compute-4-3_218', 'compute-4-31_220', 'compute-4-34_223', 'compute-4-36_225', 'compute-4-37_226', 'compute-4-38_227', 'compute-4-39_228', 'compute-4-40_230', 'compute-4-42_232', 'compute-4-43_233', 'compute-4-44_234', 'compute-4-47_237', 'compute-4-48_238', 'compute-4-5_239', 'compute-4-6_240', 'compute-4-7_241', 'compute-4-8_242', 'compute-4-9_243', 'compute-5-11_246', 'compute-5-14_249', 'compute-5-15_250', 'compute-5-16_251', 'compute-5-17_252', 'compute-5-4_262', 'compute-5-6_264', 'compute-5-8_266', 'compute-6-10_269', 'compute-6-12_271', 'compute-6-15_274', 'compute-6-17_276', 'compute-6-18_277', 'compute-6-19_278', 'compute-6-5_283', 'compute-6-6_284', 'compute-6-8_286', 'compute-7-1_288', 'compute-7-11_290', 'compute-7-14_293', 'compute-7-15_294', 'compute-7-18_297', 'compute-7-19_298', 'compute-7-21_301', 'compute-7-23_303', 'compute-7-26_306', 'compute-7-27_307', 'compute-7-28_308', 'compute-7-32_313', 'compute-7-33_314', 'compute-7-34_315', 'compute-7-35_316', 'compute-7-36_317', 'compute-7-40_321', 'compute-7-42_323', 'compute-7-43_324', 'compute-7-45_326', 'compute-7-46_327', 'compute-7-47_328', 'compute-7-49_330', 'compute-7-51_333', 'compute-7-6_342', 'compute-7-8_345', 'compute-7-9_346', 'compute-8-10_348', 'compute-8-11_349', 'compute-8-12_350', 'compute-8-13_351', 'compute-8-15_353', 'compute-8-16_354', 'compute-8-17_355', 'compute-8-18_356', 'compute-8-19_357', 'compute-8-24_363', 'compute-8-27_366', 'compute-8-28_367', 'compute-8-29_368', 'compute-8-3_369', 'compute-8-31_371', 'compute-8-32_372', 'compute-8-34_374', 'compute-8-35_375', 'compute-8-38_378', 'compute-8-39_379', 'compute-8-42_383', 'compute-8-43_384', 'compute-8-46_387', 'compute-8-47_388', 'compute-8-48_389', 'compute-8-5_391', 'compute-8-50_392', 'compute-8-53_395', 'compute-8-54_396', 'compute-8-57_399', 'compute-8-59_401', 'compute-8-6_402', 'compute-8-8_405', 'compute-8-9_406', 'compute-9-1_407', 'compute-9-11_409', 'compute-9-13_411', 'compute-9-14_412', 'compute-9-16_414', 'compute-9-17_415', 'compute-9-19_417', 'compute-9-2_418', 'compute-9-20_419', 'compute-9-23_422', 'compute-9-24_423', 'compute-9-25_424', 'compute-9-26_425', 'compute-9-27_426', 'compute-9-28_427', 'compute-9-3_429', 'compute-9-32_432', 'compute-9-33_433', 'compute-9-41_442', 'compute-9-45_446', 'compute-9-47_448', 'compute-9-48_449', 'compute-9-49_450', 'compute-9-5_451', 'compute-9-50_452', 'compute-9-52_454', 'compute-9-55_457', 'compute-9-57_459', 'compute-9-59_461', 'compute-9-7_464', 'compute-9-8_465']"
      ],
      "text/plain": [
       "['compute-1-10_1',\n",
       " 'compute-1-13_4',\n",
       " 'compute-1-14_5',\n",
       " 'compute-1-18_9',\n",
       " 'compute-1-19_10',\n",
       " 'compute-1-22_14',\n",
       " 'compute-1-23_15',\n",
       " 'compute-1-24_16',\n",
       " 'compute-1-26_18',\n",
       " 'compute-1-29_21',\n",
       " 'compute-1-3_22',\n",
       " 'compute-1-30_23',\n",
       " 'compute-1-31_24',\n",
       " 'compute-1-34_27',\n",
       " 'compute-1-36_29',\n",
       " 'compute-1-37_30',\n",
       " 'compute-1-4_33',\n",
       " 'compute-1-40_34',\n",
       " 'compute-1-42_36',\n",
       " 'compute-1-43_37',\n",
       " 'compute-1-45_39',\n",
       " 'compute-1-46_40',\n",
       " 'compute-1-47_41',\n",
       " 'compute-1-49_43',\n",
       " 'compute-1-53_48',\n",
       " 'compute-1-55_50',\n",
       " 'compute-1-58_53',\n",
       " 'compute-1-59_54',\n",
       " 'compute-1-6_55',\n",
       " 'compute-1-60_56',\n",
       " 'compute-1-7_57',\n",
       " 'compute-1-8_58',\n",
       " 'compute-10-26_61',\n",
       " 'compute-10-27_62',\n",
       " 'compute-10-28_63',\n",
       " 'compute-10-30_65',\n",
       " 'compute-10-31_66',\n",
       " 'compute-10-33_68',\n",
       " 'compute-10-34_69',\n",
       " 'compute-10-35_70',\n",
       " 'compute-10-36_71',\n",
       " 'compute-10-37_72',\n",
       " 'compute-10-38_73',\n",
       " 'compute-10-39_74',\n",
       " 'compute-10-40_75',\n",
       " 'compute-10-41_76',\n",
       " 'compute-10-44_79',\n",
       " 'compute-2-1_80',\n",
       " 'compute-2-10_81',\n",
       " 'compute-2-12_83',\n",
       " 'compute-2-19_90',\n",
       " 'compute-2-2_91',\n",
       " 'compute-2-20_92',\n",
       " 'compute-2-22_94',\n",
       " 'compute-2-26_98',\n",
       " 'compute-2-29_101',\n",
       " 'compute-2-30_103',\n",
       " 'compute-2-32_105',\n",
       " 'compute-2-33_106',\n",
       " 'compute-2-37_110',\n",
       " 'compute-2-4_113',\n",
       " 'compute-2-40_114',\n",
       " 'compute-2-42_116',\n",
       " 'compute-2-43_117',\n",
       " 'compute-2-46_120',\n",
       " 'compute-2-49_123',\n",
       " 'compute-2-5_124',\n",
       " 'compute-2-50_125',\n",
       " 'compute-2-53_128',\n",
       " 'compute-2-54_129',\n",
       " 'compute-2-56_131',\n",
       " 'compute-2-57_132',\n",
       " 'compute-2-60_136',\n",
       " 'compute-2-9_139',\n",
       " 'compute-3-10_141',\n",
       " 'compute-3-11_142',\n",
       " 'compute-3-13_144',\n",
       " 'compute-3-14_145',\n",
       " 'compute-3-15_146',\n",
       " 'compute-3-16_147',\n",
       " 'compute-3-17_148',\n",
       " 'compute-3-19_150',\n",
       " 'compute-3-2_151',\n",
       " 'compute-3-20_152',\n",
       " 'compute-3-22_154',\n",
       " 'compute-3-23_155',\n",
       " 'compute-3-25_157',\n",
       " 'compute-3-26_158',\n",
       " 'compute-3-29_161',\n",
       " 'compute-3-31_164',\n",
       " 'compute-3-33_166',\n",
       " 'compute-3-35_168',\n",
       " 'compute-3-36_169',\n",
       " 'compute-3-40_174',\n",
       " 'compute-3-44_178',\n",
       " 'compute-3-45_179',\n",
       " 'compute-3-46_180',\n",
       " 'compute-3-5_184',\n",
       " 'compute-3-51_186',\n",
       " 'compute-3-53_188',\n",
       " 'compute-3-54_189',\n",
       " 'compute-3-55_190',\n",
       " 'compute-3-7_193',\n",
       " 'compute-3-8_194',\n",
       " 'compute-3-9_195',\n",
       " 'compute-4-1_196',\n",
       " 'compute-4-10_197',\n",
       " 'compute-4-12_199',\n",
       " 'compute-4-14_201',\n",
       " 'compute-4-18_205',\n",
       " 'compute-4-19_206',\n",
       " 'compute-4-2_207',\n",
       " 'compute-4-22_210',\n",
       " 'compute-4-23_211',\n",
       " 'compute-4-24_212',\n",
       " 'compute-4-27_215',\n",
       " 'compute-4-3_218',\n",
       " 'compute-4-31_220',\n",
       " 'compute-4-34_223',\n",
       " 'compute-4-36_225',\n",
       " 'compute-4-37_226',\n",
       " 'compute-4-38_227',\n",
       " 'compute-4-39_228',\n",
       " 'compute-4-40_230',\n",
       " 'compute-4-42_232',\n",
       " 'compute-4-43_233',\n",
       " 'compute-4-44_234',\n",
       " 'compute-4-47_237',\n",
       " 'compute-4-48_238',\n",
       " 'compute-4-5_239',\n",
       " 'compute-4-6_240',\n",
       " 'compute-4-7_241',\n",
       " 'compute-4-8_242',\n",
       " 'compute-4-9_243',\n",
       " 'compute-5-11_246',\n",
       " 'compute-5-14_249',\n",
       " 'compute-5-15_250',\n",
       " 'compute-5-16_251',\n",
       " 'compute-5-17_252',\n",
       " 'compute-5-4_262',\n",
       " 'compute-5-6_264',\n",
       " 'compute-5-8_266',\n",
       " 'compute-6-10_269',\n",
       " 'compute-6-12_271',\n",
       " 'compute-6-15_274',\n",
       " 'compute-6-17_276',\n",
       " 'compute-6-18_277',\n",
       " 'compute-6-19_278',\n",
       " 'compute-6-5_283',\n",
       " 'compute-6-6_284',\n",
       " 'compute-6-8_286',\n",
       " 'compute-7-1_288',\n",
       " 'compute-7-11_290',\n",
       " 'compute-7-14_293',\n",
       " 'compute-7-15_294',\n",
       " 'compute-7-18_297',\n",
       " 'compute-7-19_298',\n",
       " 'compute-7-21_301',\n",
       " 'compute-7-23_303',\n",
       " 'compute-7-26_306',\n",
       " 'compute-7-27_307',\n",
       " 'compute-7-28_308',\n",
       " 'compute-7-32_313',\n",
       " 'compute-7-33_314',\n",
       " 'compute-7-34_315',\n",
       " 'compute-7-35_316',\n",
       " 'compute-7-36_317',\n",
       " 'compute-7-40_321',\n",
       " 'compute-7-42_323',\n",
       " 'compute-7-43_324',\n",
       " 'compute-7-45_326',\n",
       " 'compute-7-46_327',\n",
       " 'compute-7-47_328',\n",
       " 'compute-7-49_330',\n",
       " 'compute-7-51_333',\n",
       " 'compute-7-6_342',\n",
       " 'compute-7-8_345',\n",
       " 'compute-7-9_346',\n",
       " 'compute-8-10_348',\n",
       " 'compute-8-11_349',\n",
       " 'compute-8-12_350',\n",
       " 'compute-8-13_351',\n",
       " 'compute-8-15_353',\n",
       " 'compute-8-16_354',\n",
       " 'compute-8-17_355',\n",
       " 'compute-8-18_356',\n",
       " 'compute-8-19_357',\n",
       " 'compute-8-24_363',\n",
       " 'compute-8-27_366',\n",
       " 'compute-8-28_367',\n",
       " 'compute-8-29_368',\n",
       " 'compute-8-3_369',\n",
       " 'compute-8-31_371',\n",
       " 'compute-8-32_372',\n",
       " 'compute-8-34_374',\n",
       " 'compute-8-35_375',\n",
       " 'compute-8-38_378',\n",
       " 'compute-8-39_379',\n",
       " 'compute-8-42_383',\n",
       " 'compute-8-43_384',\n",
       " 'compute-8-46_387',\n",
       " 'compute-8-47_388',\n",
       " 'compute-8-48_389',\n",
       " 'compute-8-5_391',\n",
       " 'compute-8-50_392',\n",
       " 'compute-8-53_395',\n",
       " 'compute-8-54_396',\n",
       " 'compute-8-57_399',\n",
       " 'compute-8-59_401',\n",
       " 'compute-8-6_402',\n",
       " 'compute-8-8_405',\n",
       " 'compute-8-9_406',\n",
       " 'compute-9-1_407',\n",
       " 'compute-9-11_409',\n",
       " 'compute-9-13_411',\n",
       " 'compute-9-14_412',\n",
       " 'compute-9-16_414',\n",
       " 'compute-9-17_415',\n",
       " 'compute-9-19_417',\n",
       " 'compute-9-2_418',\n",
       " 'compute-9-20_419',\n",
       " 'compute-9-23_422',\n",
       " 'compute-9-24_423',\n",
       " 'compute-9-25_424',\n",
       " 'compute-9-26_425',\n",
       " 'compute-9-27_426',\n",
       " 'compute-9-28_427',\n",
       " 'compute-9-3_429',\n",
       " 'compute-9-32_432',\n",
       " 'compute-9-33_433',\n",
       " 'compute-9-41_442',\n",
       " 'compute-9-45_446',\n",
       " 'compute-9-47_448',\n",
       " 'compute-9-48_449',\n",
       " 'compute-9-49_450',\n",
       " 'compute-9-5_451',\n",
       " 'compute-9-50_452',\n",
       " 'compute-9-52_454',\n",
       " 'compute-9-55_457',\n",
       " 'compute-9-57_459',\n",
       " 'compute-9-59_461',\n",
       " 'compute-9-7_464',\n",
       " 'compute-9-8_465']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## check some computes having been skipped\n",
    "print(data_list[262].cluster.value_counts())\n",
    "\n",
    "print(data_list[415].cluster.value_counts())\n",
    "\n",
    "print(data_list[464].cluster.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df[\"cluster\"].nunique() 7\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4000]\ttrain's multi_logloss: 0.00470968\tvalid's multi_logloss: 0.00769448\n",
      "\n",
      "\n",
      "84.33 seconds\n"
     ]
    }
   ],
   "source": [
    "## run with all data (combine all the computes)\n",
    "tick = time.time()\n",
    "metric = \"multi_logloss\"\n",
    "objective = \"multiclass\"\n",
    "lgb_params = {\n",
    "        'boosting_type':'gbdt', 'colsample_bytree':0.8, #'class_weight': {0 : 1. , 1: weight},\n",
    "        'importance_type':'gain', 'learning_rate':0.01, 'max_depth':2,\n",
    "        'min_child_samples':15,# 'min_child_weight':0.001, 'min_split_gain':0.0,\n",
    "        'n_estimators':4000, 'n_jobs': 16, 'num_leaves':31, 'subsample_freq':16,\n",
    "        'seed': 2020, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "        'subsample':.9, 'subsample_for_bin':200000,  \"metric\":metric ,'objective':objective\n",
    "    }\n",
    "\n",
    "run_lgb_and_shap(name=\"TotalData\", i=\"None\", df=pd.concat(data_list,0), feature_names=feature_names, train_error_list = train_error_list, valid_error_list=valid_error_list, \n",
    "                 skip_list=skip_list, colors_dict=colors_dict,lgb_params=lgb_params)\n",
    "tock = time.time()\n",
    "print(f\"{np.round(tock - tick, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[1.0, 0.9895833333333334, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9895833333333334, 0.96875, 0.9583333333333334, 0.9895833333333334, 0.9791666666666666, 0.9894736842105263, 0.9583333333333334, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 0.9791666666666666, 1.0, 0.9791666666666666, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9789473684210527, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9583333333333334, 0.9479166666666666, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.96875, 1.0, 0.9791666666666666, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9791666666666666, 0.9789473684210527, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.96875, 0.9791666666666666, 0.9791666666666666, 1.0, 0.96875, 0.9791666666666666, 1.0, 1.0, 1.0, 0.9789473684210527, 0.9895833333333334, 0.9789473684210527, 0.9791666666666666, 0.96875, 0.9791666666666666, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 0.9894736842105263, 0.9895833333333334, 0.9894736842105263, 0.9791666666666666, 1.0, 1.0, 0.9894736842105263, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9791666666666666, 0.96875, 0.9791666666666666, 0.9791666666666666, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9894736842105263, 1.0, 0.9789473684210527, 0.9791666666666666, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9791666666666666, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9791666666666666, 0.96875, 0.9791666666666666, 0.9895833333333334, 0.96875, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9894736842105263, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9791666666666666, 0.9894736842105263, 1.0, 0.96875, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9583333333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 0.96875, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9791666666666666, 1.0, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 0.9791666666666666, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9368421052631579, 0.9791666666666666, 0.9791666666666666, 0.96875, 0.9895833333333334, 0.9895833333333334, 1.0, 0.96875, 0.9791666666666666, 1.0, 0.9791666666666666, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 0.96875, 1.0, 0.9583333333333334, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9479166666666666, 1.0, 0.9895833333333334, 0.96875, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 0.9975216294160058]"
      ],
      "text/plain": [
       "[1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9583333333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9894736842105263,\n",
       " 0.9583333333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9789473684210527,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9583333333333334,\n",
       " 0.9479166666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9789473684210527,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9789473684210527,\n",
       " 0.9895833333333334,\n",
       " 0.9789473684210527,\n",
       " 0.9791666666666666,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9894736842105263,\n",
       " 0.9895833333333334,\n",
       " 0.9894736842105263,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9894736842105263,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9894736842105263,\n",
       " 1.0,\n",
       " 0.9789473684210527,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9894736842105263,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9894736842105263,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9583333333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9368421052631579,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 0.96875,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.9583333333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9479166666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9975216294160058]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of computes having SHAP: 225\n"
     ]
    }
   ],
   "source": [
    "print(\"num of computes having SHAP:\", (len(list(data.keys()))) - len(skip_list) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
