{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import colors as plt_colors\n",
    "\n",
    "## Measure time\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from json import JSONDecoder, JSONDecodeError  # for reading the JSON data files\n",
    "import re  # for regular expressions\n",
    "import os  # for os related operations\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from collections import Counter\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "data = json.load(open('/Users/chaupham/Downloads/17Feb2020.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compute-1-1', 'compute-1-10', 'compute-1-11', 'compute-1-12', 'compute-1-13', 'compute-1-14', 'compute-1-15', 'compute-1-16', 'compute-1-17', 'compute-1-18', 'compute-1-19', 'compute-1-2', 'compute-1-20', 'compute-1-21', 'compute-1-22', 'compute-1-23', 'compute-1-24', 'compute-1-25', 'compute-1-26', 'compute-1-27', 'compute-1-28', 'compute-1-29', 'compute-1-3', 'compute-1-30', 'compute-1-31', 'compute-1-32', 'compute-1-33', 'compute-1-34', 'compute-1-35', 'compute-1-36', 'compute-1-37', 'compute-1-38', 'compute-1-39', 'compute-1-4', 'compute-1-40', 'compute-1-41', 'compute-1-42', 'compute-1-43', 'compute-1-44', 'compute-1-45', 'compute-1-46', 'compute-1-47', 'compute-1-48', 'compute-1-49', 'compute-1-5', 'compute-1-50', 'compute-1-51', 'compute-1-52', 'compute-1-53', 'compute-1-54', 'compute-1-55', 'compute-1-56', 'compute-1-57', 'compute-1-58', 'compute-1-59', 'compute-1-6', 'compute-1-60', 'compute-1-7', 'compute-1-8', 'compute-1-9', 'compute-10-25', 'compute-10-26', 'compute-10-27', 'compute-10-28', 'compute-10-29', 'compute-10-30', 'compute-10-31', 'compute-10-32', 'compute-10-33', 'compute-10-34', 'compute-10-35', 'compute-10-36', 'compute-10-37', 'compute-10-38', 'compute-10-39', 'compute-10-40', 'compute-10-41', 'compute-10-42', 'compute-10-43', 'compute-10-44', 'compute-2-1', 'compute-2-10', 'compute-2-11', 'compute-2-12', 'compute-2-13', 'compute-2-14', 'compute-2-15', 'compute-2-16', 'compute-2-17', 'compute-2-18', 'compute-2-19', 'compute-2-2', 'compute-2-20', 'compute-2-21', 'compute-2-22', 'compute-2-23', 'compute-2-24', 'compute-2-25', 'compute-2-26', 'compute-2-27', 'compute-2-28', 'compute-2-29', 'compute-2-3', 'compute-2-30', 'compute-2-31', 'compute-2-32', 'compute-2-33', 'compute-2-34', 'compute-2-35', 'compute-2-36', 'compute-2-37', 'compute-2-38', 'compute-2-39', 'compute-2-4', 'compute-2-40', 'compute-2-41', 'compute-2-42', 'compute-2-43', 'compute-2-44', 'compute-2-45', 'compute-2-46', 'compute-2-47', 'compute-2-48', 'compute-2-49', 'compute-2-5', 'compute-2-50', 'compute-2-51', 'compute-2-52', 'compute-2-53', 'compute-2-54', 'compute-2-55', 'compute-2-56', 'compute-2-57', 'compute-2-58', 'compute-2-59', 'compute-2-6', 'compute-2-60', 'compute-2-7', 'compute-2-8', 'compute-2-9', 'compute-3-1', 'compute-3-10', 'compute-3-11', 'compute-3-12', 'compute-3-13', 'compute-3-14', 'compute-3-15', 'compute-3-16', 'compute-3-17', 'compute-3-18', 'compute-3-19', 'compute-3-2', 'compute-3-20', 'compute-3-21', 'compute-3-22', 'compute-3-23', 'compute-3-24', 'compute-3-25', 'compute-3-26', 'compute-3-27', 'compute-3-28', 'compute-3-29', 'compute-3-3', 'compute-3-30', 'compute-3-31', 'compute-3-32', 'compute-3-33', 'compute-3-34', 'compute-3-35', 'compute-3-36', 'compute-3-37', 'compute-3-38', 'compute-3-39', 'compute-3-4', 'compute-3-40', 'compute-3-41', 'compute-3-42', 'compute-3-43', 'compute-3-44', 'compute-3-45', 'compute-3-46', 'compute-3-47', 'compute-3-48', 'compute-3-49', 'compute-3-5', 'compute-3-50', 'compute-3-51', 'compute-3-52', 'compute-3-53', 'compute-3-54', 'compute-3-55', 'compute-3-56', 'compute-3-6', 'compute-3-7', 'compute-3-8', 'compute-3-9', 'compute-4-1', 'compute-4-10', 'compute-4-11', 'compute-4-12', 'compute-4-13', 'compute-4-14', 'compute-4-15', 'compute-4-16', 'compute-4-17', 'compute-4-18', 'compute-4-19', 'compute-4-2', 'compute-4-20', 'compute-4-21', 'compute-4-22', 'compute-4-23', 'compute-4-24', 'compute-4-25', 'compute-4-26', 'compute-4-27', 'compute-4-28', 'compute-4-29', 'compute-4-3', 'compute-4-30', 'compute-4-31', 'compute-4-32', 'compute-4-33', 'compute-4-34', 'compute-4-35', 'compute-4-36', 'compute-4-37', 'compute-4-38', 'compute-4-39', 'compute-4-4', 'compute-4-40', 'compute-4-41', 'compute-4-42', 'compute-4-43', 'compute-4-44', 'compute-4-45', 'compute-4-46', 'compute-4-47', 'compute-4-48', 'compute-4-5', 'compute-4-6', 'compute-4-7', 'compute-4-8', 'compute-4-9', 'compute-5-1', 'compute-5-10', 'compute-5-11', 'compute-5-12', 'compute-5-13', 'compute-5-14', 'compute-5-15', 'compute-5-16', 'compute-5-17', 'compute-5-18', 'compute-5-19', 'compute-5-2', 'compute-5-20', 'compute-5-21', 'compute-5-22', 'compute-5-23', 'compute-5-24', 'compute-5-3', 'compute-5-4', 'compute-5-5', 'compute-5-6', 'compute-5-7', 'compute-5-8', 'compute-5-9', 'compute-6-1', 'compute-6-10', 'compute-6-11', 'compute-6-12', 'compute-6-13', 'compute-6-14', 'compute-6-15', 'compute-6-16', 'compute-6-17', 'compute-6-18', 'compute-6-19', 'compute-6-2', 'compute-6-20', 'compute-6-3', 'compute-6-4', 'compute-6-5', 'compute-6-6', 'compute-6-7', 'compute-6-8', 'compute-6-9', 'compute-7-1', 'compute-7-10', 'compute-7-11', 'compute-7-12', 'compute-7-13', 'compute-7-14', 'compute-7-15', 'compute-7-16', 'compute-7-17', 'compute-7-18', 'compute-7-19', 'compute-7-2', 'compute-7-20', 'compute-7-21', 'compute-7-22', 'compute-7-23', 'compute-7-24', 'compute-7-25', 'compute-7-26', 'compute-7-27', 'compute-7-28', 'compute-7-29', 'compute-7-3', 'compute-7-30', 'compute-7-31', 'compute-7-32', 'compute-7-33', 'compute-7-34', 'compute-7-35', 'compute-7-36', 'compute-7-37', 'compute-7-38', 'compute-7-39', 'compute-7-40', 'compute-7-41', 'compute-7-42', 'compute-7-43', 'compute-7-44', 'compute-7-45', 'compute-7-46', 'compute-7-47', 'compute-7-48', 'compute-7-49', 'compute-7-5', 'compute-7-50', 'compute-7-51', 'compute-7-52', 'compute-7-53', 'compute-7-54', 'compute-7-55', 'compute-7-56', 'compute-7-57', 'compute-7-58', 'compute-7-59', 'compute-7-6', 'compute-7-60', 'compute-7-7', 'compute-7-8', 'compute-7-9', 'compute-8-1', 'compute-8-10', 'compute-8-11', 'compute-8-12', 'compute-8-13', 'compute-8-14', 'compute-8-15', 'compute-8-16', 'compute-8-17', 'compute-8-18', 'compute-8-19', 'compute-8-2', 'compute-8-20', 'compute-8-21', 'compute-8-22', 'compute-8-23', 'compute-8-24', 'compute-8-25', 'compute-8-26', 'compute-8-27', 'compute-8-28', 'compute-8-29', 'compute-8-3', 'compute-8-30', 'compute-8-31', 'compute-8-32', 'compute-8-33', 'compute-8-34', 'compute-8-35', 'compute-8-36', 'compute-8-37', 'compute-8-38', 'compute-8-39', 'compute-8-4', 'compute-8-40', 'compute-8-41', 'compute-8-42', 'compute-8-43', 'compute-8-44', 'compute-8-45', 'compute-8-46', 'compute-8-47', 'compute-8-48', 'compute-8-49', 'compute-8-5', 'compute-8-50', 'compute-8-51', 'compute-8-52', 'compute-8-53', 'compute-8-54', 'compute-8-55', 'compute-8-56', 'compute-8-57', 'compute-8-58', 'compute-8-59', 'compute-8-6', 'compute-8-60', 'compute-8-7', 'compute-8-8', 'compute-8-9', 'compute-9-1', 'compute-9-10', 'compute-9-11', 'compute-9-12', 'compute-9-13', 'compute-9-14', 'compute-9-15', 'compute-9-16', 'compute-9-17', 'compute-9-18', 'compute-9-19', 'compute-9-2', 'compute-9-20', 'compute-9-21', 'compute-9-22', 'compute-9-23', 'compute-9-24', 'compute-9-25', 'compute-9-26', 'compute-9-27', 'compute-9-28', 'compute-9-29', 'compute-9-3', 'compute-9-30', 'compute-9-31', 'compute-9-32', 'compute-9-33', 'compute-9-34', 'compute-9-35', 'compute-9-36', 'compute-9-37', 'compute-9-38', 'compute-9-39', 'compute-9-4', 'compute-9-40', 'compute-9-41', 'compute-9-42', 'compute-9-43', 'compute-9-44', 'compute-9-45', 'compute-9-46', 'compute-9-47', 'compute-9-48', 'compute-9-49', 'compute-9-5', 'compute-9-50', 'compute-9-51', 'compute-9-52', 'compute-9-53', 'compute-9-54', 'compute-9-55', 'compute-9-56', 'compute-9-57', 'compute-9-58', 'compute-9-59', 'compute-9-6', 'compute-9-60', 'compute-9-7', 'compute-9-8', 'compute-9-9']\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "print(list(data.keys()))\n",
    "print(len(list(data.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[[0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631313131313132, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0664040404040404, 0.5375, 0.5375, 0.5375, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06629292929292929, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631313131313132, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7375], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0663080808080808, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5458333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0662979797979798, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06630303030303031, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631313131313132, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0663080808080808, 0.5375, 0.5375, 0.5375, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632828282828282, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5458333333333333, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5375, 0.5375, 0.7475], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5375, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.1368421052631579, 0.06635858585858585, 0.5375, 0.5375, 0.5375, 0.5375, 0.74], [0.6105263157894737, 0.47368421052631576, 0.14736842105263157, 0.06635858585858585, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5333333333333333, 0.5458333333333333, 0.5375, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0664949494949495, 0.5375, 0.5458333333333333, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631313131313132, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632323232323233, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06653030303030304, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06635353535353536, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634343434343434, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06644949494949495, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.6105263157894737, 0.47368421052631576, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632828282828282, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06641919191919192, 0.5375, 0.5375, 0.525, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631818181818182, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632323232323233, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06642424242424243, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06642424242424243, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.47368421052631576, 0.1368421052631579, 0.06632828282828282, 0.5333333333333333, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06631818181818182, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06632828282828282, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06652020202020202, 0.5375, 0.5375, 0.5375, 0.5333333333333333, 0.7375], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.745], [0.6105263157894737, 0.47368421052631576, 0.14736842105263157, 0.06634343434343434, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.745], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6105263157894737, 0.47368421052631576, 0.1368421052631579, 0.06633838383838384, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5333333333333333, 0.7475], [0.6105263157894737, 0.4842105263157895, 0.1368421052631579, 0.06632828282828282, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.7475], [0.6105263157894737, 0.47368421052631576, 0.14736842105263157, 0.06634343434343434, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.6210526315789474, 0.4842105263157895, 0.1368421052631579, 0.06633838383838384, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.47368421052631576, 0.1368421052631579, 0.06634848484848485, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.1368421052631579, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7375], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06634343434343434, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06635353535353536, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.1368421052631579, 0.06635353535353536, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06635858585858585, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06635353535353536, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06636363636363636, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634343434343434, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06635353535353536, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06634848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.1368421052631579, 0.06634848484848485, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633838383838384, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06665656565656566, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06633333333333334, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06636363636363636, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06638383838383838, 0.5375, 0.5375, 0.5375, 0.5375, 0.745], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0663939393939394, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06638888888888889, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.06666666666666667, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.6210526315789474, 0.4842105263157895, 0.14736842105263157, 0.0663939393939394, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06638888888888889, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7375], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.06655555555555556, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.745], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.0663989898989899, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7425], [0.6105263157894737, 0.4842105263157895, 0.14736842105263157, 0.0663989898989899, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7425], [0.45263157894736844, 0.37894736842105264, 0.14736842105263157, 0.06365656565656565, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.755], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.11465151515151516, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.765], [0.5473684210526316, 0.4421052631578947, 0.14736842105263157, 0.12462626262626261, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7325], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12471212121212122, 0.5375, 0.5375, 0.5375, 0.5375, 0.7575], [0.5789473684210527, 0.45263157894736844, 0.1368421052631579, 0.12478282828282829, 0.5375, 0.5375, 0.5375, 0.5375, 0.7675], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12504040404040404, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7625], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12484848484848485, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.765], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.12491919191919193, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7625], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12503030303030302, 0.5375, 0.5375, 0.5375, 0.5375, 0.765], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12497979797979797, 0.5375, 0.5375, 0.5375, 0.5375, 0.735], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12496969696969697, 0.5375, 0.5375, 0.5375, 0.5375, 0.765], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.124989898989899, 0.5375, 0.5375, 0.5375, 0.5375, 0.6675], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.1250050505050505, 0.5375, 0.5375, 0.5375, 0.5375, 0.765], [0.5578947368421052, 0.45263157894736844, 0.14736842105263157, 0.12512121212121213, 0.5333333333333333, 0.5458333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12508080808080807, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.76], [0.5684210526315789, 0.4631578947368421, 0.1368421052631579, 0.12505555555555556, 0.5333333333333333, 0.5375, 0.5333333333333333, 0.5375, 0.7475], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12506060606060607, 0.5375, 0.5375, 0.5375, 0.5375, 0.7625], [0.5789473684210527, 0.47368421052631576, 0.14736842105263157, 0.12509090909090909, 0.5375, 0.5375, 0.5375, 0.5375, 0.765], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.12509090909090909, 0.5375, 0.5375, 0.5375, 0.5375, 0.72], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.12508585858585858, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.7625], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.1251060606060606, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.5789473684210527, 0.47368421052631576, 0.1368421052631579, 0.12513131313131312, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.74], [0.5789473684210527, 0.4421052631578947, 0.14736842105263157, 0.12515656565656566, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7225], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12516161616161617, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.765], [0.5578947368421052, 0.4631578947368421, 0.14736842105263157, 0.12515151515151515, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.7475], [0.5894736842105263, 0.47368421052631576, 0.14736842105263157, 0.12515151515151515, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7625], [0.5684210526315789, 0.4631578947368421, 0.14736842105263157, 0.12517171717171716, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.76], [0.5894736842105263, 0.47368421052631576, 0.1368421052631579, 0.12516666666666668, 0.5375, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.76], [0.5263157894736842, 0.4, 0.14736842105263157, 0.1251818181818182, 0.5375, 0.5375, 0.5375, 0.5333333333333333, 0.585], [0.3473684210526316, 0.28421052631578947, 0.14736842105263157, 0.05588888888888889, 0.5375, 0.5375, 0.5375, 0.5333333333333333, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.1368421052631579, 0.055868686868686866, 0.5375, 0.5375, 0.5375, 0.5375, 0.33], [0.3368421052631579, 0.28421052631578947, 0.1368421052631579, 0.0557929292929293, 0.5375, 0.5375, 0.5375, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.0557929292929293, 0.5375, 0.5375, 0.525, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.3325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.055767676767676765, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.1368421052631579, 0.055767676767676765, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.33], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578282828282828, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578282828282828, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578282828282828, 0.5375, 0.5333333333333333, 0.5375, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5375, 0.5375, 0.5375, 0.335], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578787878787878, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.3375], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05578787878787878, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.1368421052631579, 0.05578787878787878, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577272727272727, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.33], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.3275], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577777777777778, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.2736842105263158, 0.14736842105263157, 0.05577272727272727, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.325], [0.3368421052631579, 0.28421052631578947, 0.14736842105263157, 0.05577272727272727, 0.5375, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.325], [0.3894736842105263, 0.3368421052631579, 0.14736842105263157, 0.09537373737373737, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.5075], [0.3894736842105263, 0.3368421052631579, 0.14736842105263157, 0.1283939393939394, 0.5333333333333333, 0.5333333333333333, 0.5333333333333333, 0.5375, 0.78], [0.3473684210526316, 0.28421052631578947, 0.1368421052631579, 0.09613131313131312, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.33], [0.5473684210526316, 0.4421052631578947, 0.14736842105263157, 0.06465656565656565, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06501515151515151, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06501515151515151, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7025], [0.5789473684210527, 0.45263157894736844, 0.14736842105263157, 0.06510606060606061, 0.5375, 0.5375, 0.5375, 0.5375, 0.7], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06523232323232324, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.6975], [0.42105263157894735, 0.3473684210526316, 0.14736842105263157, 0.06523232323232324, 0.5375, 0.5375, 0.5375, 0.5458333333333333, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06097979797979798, 0.5375, 0.5458333333333333, 0.5333333333333333, 0.5375, 0.7125], [0.5368421052631579, 0.4421052631578947, 0.14736842105263157, 0.06445959595959595, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7075], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06476767676767677, 0.5375, 0.5458333333333333, 0.5333333333333333, 0.5375, 0.7125], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06474242424242424, 0.5375, 0.5375, 0.5333333333333333, 0.5458333333333333, 0.71], [0.5052631578947369, 0.3894736842105263, 0.14736842105263157, 0.06502020202020202, 0.5375, 0.5375, 0.5375, 0.5375, 0.38], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06502020202020202, 0.5375, 0.5375, 0.5375, 0.5375, 0.7125], [0.5368421052631579, 0.4421052631578947, 0.14736842105263157, 0.06487878787878788, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7175], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06442929292929293, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.45263157894736844, 0.14736842105263157, 0.0647020202020202, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.715], [0.47368421052631576, 0.35789473684210527, 0.14736842105263157, 0.06492929292929293, 0.5375, 0.5375, 0.5375, 0.5375, 0.6475], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.06507070707070707, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7175], [0.5368421052631579, 0.43157894736842106, 0.14736842105263157, 0.06452020202020202, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5684210526315789, 0.4631578947368421, 0.14736842105263157, 0.06481818181818182, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7275], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06460606060606061, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7175], [0.4421052631578947, 0.3368421052631579, 0.14736842105263157, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.67], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06466666666666666, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.725], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.0642979797979798, 0.5375, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0642979797979798, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06425252525252526, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.72], [0.5473684210526316, 0.45263157894736844, 0.15789473684210525, 0.06423232323232324, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06487373737373738, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.72], [0.5263157894736842, 0.43157894736842106, 0.15789473684210525, 0.06337878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5789473684210527, 0.47368421052631576, 0.15789473684210525, 0.06496969696969697, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.715], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.4842105263157895, 0.4, 0.15789473684210525, 0.06340404040404041, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7125], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06495959595959595, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06472727272727273, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7275], [0.4421052631578947, 0.3263157894736842, 0.15789473684210525, 0.06472727272727273, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.6825], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06481313131313131, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.72], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06433333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5052631578947369, 0.3894736842105263, 0.15789473684210525, 0.06471212121212122, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.335], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.061030303030303025, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5473684210526316, 0.4421052631578947, 0.14736842105263157, 0.06426262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.72], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.0646969696969697, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7175], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.065010101010101, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5368421052631579, 0.43157894736842106, 0.15789473684210525, 0.065010101010101, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.065010101010101, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06434343434343434, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5368421052631579, 0.43157894736842106, 0.15789473684210525, 0.06434343434343434, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7125], [0.5684210526315789, 0.45263157894736844, 0.14736842105263157, 0.06484848484848485, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5052631578947369, 0.42105263157894735, 0.15789473684210525, 0.06487878787878788, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06114646464646464, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06109090909090909, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.49473684210526314, 0.4105263157894737, 0.15789473684210525, 0.06109090909090909, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5473684210526316, 0.45263157894736844, 0.15789473684210525, 0.06104545454545454, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7125], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06485858585858587, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.49473684210526314, 0.4105263157894737, 0.15789473684210525, 0.06485858585858587, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7], [0.5578947368421052, 0.45263157894736844, 0.14736842105263157, 0.06476262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06462121212121212, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.71], [0.5052631578947369, 0.4105263157894737, 0.14736842105263157, 0.06462121212121212, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5473684210526316, 0.45263157894736844, 0.14736842105263157, 0.06338888888888888, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.6975], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.06098989898989899, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7025], [0.5157894736842106, 0.43157894736842106, 0.14736842105263157, 0.06098989898989899, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.705], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06423232323232324, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7025], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06110606060606061, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5473684210526316, 0.4421052631578947, 0.15789473684210525, 0.06110606060606061, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7125], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.064510101010101, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.335], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.061126262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06424242424242424, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.47368421052631576, 0.3684210526315789, 0.15789473684210525, 0.06424242424242424, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.33], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06105555555555556, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7175], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06106565656565656, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.4421052631578947, 0.3368421052631579, 0.15789473684210525, 0.06334848484848485, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.33], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06334848484848485, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7125], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.06332323232323232, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.71], [0.43157894736842106, 0.3368421052631579, 0.15789473684210525, 0.06332323232323232, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.695], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.0632979797979798, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7175], [0.5473684210526316, 0.4421052631578947, 0.15789473684210525, 0.0632979797979798, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.4842105263157895, 0.4105263157894737, 0.15789473684210525, 0.0647979797979798, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7], [0.5684210526315789, 0.4631578947368421, 0.15789473684210525, 0.06426262626262626, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06482323232323232, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5157894736842106, 0.42105263157894735, 0.15789473684210525, 0.06482323232323232, 0.5375, 0.5375, 0.5375, 0.5458333333333333, 0.7075], [0.5578947368421052, 0.45263157894736844, 0.15789473684210525, 0.06417676767676768, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5473684210526316, 0.4421052631578947, 0.15789473684210525, 0.06417676767676768, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5473684210526316, 0.4421052631578947, 0.15789473684210525, 0.06325757575757576, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.6975], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06325757575757576, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.335], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06104545454545454, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5684210526315789, 0.45263157894736844, 0.15789473684210525, 0.06104545454545454, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0645959595959596, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7275], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.0645959595959596, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06455555555555556, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06455555555555556, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06463131313131314, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06457070707070707, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7125], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06434848484848485, 0.5375, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7125], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06414141414141414, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7225], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06414141414141414, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.725], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.06455555555555556, 0.5375, 0.5375, 0.5375, 0.5375, 0.6975], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06466666666666666, 0.5375, 0.5375, 0.5375, 0.5375, 0.725], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.06466666666666666, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06462626262626263, 0.5458333333333333, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.7], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.05524747474747475, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.45263157894736844, 0.14736842105263157, 0.06442929292929293, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.06455050505050505, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.72], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06455050505050505, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7025], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06424747474747475, 0.5458333333333333, 0.5375, 0.5375, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06440909090909092, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.72], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06464646464646465, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.705], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06468181818181819, 0.5375, 0.5375, 0.5375, 0.5375, 0.705], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.0642020202020202, 0.5375, 0.5375, 0.5375, 0.5375, 0.72], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.0644949494949495, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.725], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0644949494949495, 0.5375, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.705], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06412121212121212, 0.5375, 0.5458333333333333, 0.5375, 0.55, 0.7075], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06446464646464646, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.7075], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06426262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06445454545454546, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.6975], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06433333333333333, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0643030303030303, 0.5375, 0.5375, 0.5375, 0.5375, 0.705], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0643030303030303, 0.5333333333333333, 0.5375, 0.5375, 0.5375, 0.7025], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06453030303030302, 0.5375, 0.5375, 0.5375, 0.5375, 0.71], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06410606060606061, 0.5375, 0.5375, 0.5375, 0.5375, 0.71], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.0645, 0.5375, 0.5375, 0.5375, 0.5375, 0.695], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06324242424242424, 0.5375, 0.5375, 0.5375, 0.5375, 0.7], [0.5789473684210527, 0.4631578947368421, 0.15789473684210525, 0.06448989898989899, 0.5375, 0.5375, 0.5375, 0.5375, 0.705], [0.5789473684210527, 0.45263157894736844, 0.15789473684210525, 0.06448989898989899, 0.5375, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.695], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06451515151515151, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7125], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06435858585858587, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.7075], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06432323232323232, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.55, 0.7075], [0.5894736842105263, 0.4631578947368421, 0.15789473684210525, 0.06476262626262626, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.71], [0.5789473684210527, 0.4631578947368421, 0.14736842105263157, 0.06427777777777778, 0.5458333333333333, 0.5458333333333333, 0.5458333333333333, 0.55, 0.6975], [0.5894736842105263, 0.4631578947368421, 0.14736842105263157, 0.07856060606060605, 0.5458333333333333, 0.5375, 0.5458333333333333, 0.5458333333333333, 0.7275], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07878787878787878, 0.5375, 0.5375, 0.5375, 0.5375, 0.7325], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07878787878787878, 0.5375, 0.5375, 0.5375, 0.5375, 0.7175], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07881313131313132, 0.5375, 0.5375, 0.5375, 0.5375, 0.7275], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07907575757575758, 0.5375, 0.5375, 0.5333333333333333, 0.5375, 0.7325], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07901010101010102, 0.5375, 0.5375, 0.5375, 0.5375, 0.72], [0.6, 0.47368421052631576, 0.14736842105263157, 0.0791111111111111, 0.5375, 0.5375, 0.5375, 0.5375, 0.715], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07913636363636364, 0.5375, 0.5375, 0.5375, 0.5375, 0.7175], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07923232323232324, 0.5375, 0.5375, 0.5375, 0.5375, 0.7175], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07872727272727273, 0.5375, 0.5375, 0.5375, 0.5375, 0.7425], [0.6, 0.47368421052631576, 0.15789473684210525, 0.0788030303030303, 0.5375, 0.5375, 0.5375, 0.5375, 0.74], [0.6, 0.47368421052631576, 0.14736842105263157, 0.07890909090909091, 0.5375, 0.5375, 0.5375, 0.5375, 0.72], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07896969696969697, 0.5375, 0.5375, 0.5375, 0.5375, 0.73], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07906565656565656, 0.5375, 0.5375, 0.5375, 0.5375, 0.7225], [0.6, 0.47368421052631576, 0.15789473684210525, 0.07911616161616161, 0.5375, 0.5458333333333333, 0.5375, 0.5375, 0.7225], [0.6, 0.47368421052631576, 0.14736842105263157, 0.07922727272727273, 0.5375, 0.5458333333333333, 0.5375, 0.5375, 0.725]]"
      ],
      "text/plain": [
       "[[0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0664040404040404,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06629292929292929,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663080808080808,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663080808080808,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0664949494949495,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632323232323233,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06653030303030304,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06644949494949495,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06641919191919192,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.525,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632323232323233,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06642424242424243,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06642424242424243,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06632828282828282,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06652020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.7375],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06636363636363636,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06634848484848485,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06665656565656566,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06636363636363636,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638383838383838,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663939393939394,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06666666666666667,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663939393939394,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06655555555555556,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663989898989899,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.45263157894736844,\n",
       "  0.37894736842105264,\n",
       "  0.14736842105263157,\n",
       "  0.06365656565656565,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.755],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.11465151515151516,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.12462626262626261,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12471212121212122,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7575],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.1368421052631579,\n",
       "  0.12478282828282829,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7675],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12504040404040404,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12484848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12491919191919193,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12503030303030302,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12497979797979797,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.735],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12496969696969697,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.124989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6675],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.1250050505050505,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.12512121212121213,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12508080808080807,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.76],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.1368421052631579,\n",
       "  0.12505555555555556,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12506060606060607,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12509090909090909,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12509090909090909,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12508585858585858,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.1251060606060606,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12513131313131312,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.5789473684210527,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.12515656565656566,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12516161616161617,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5578947368421052,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12515151515151515,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12515151515151515,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12517171717171716,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.76],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12516666666666668,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.76],\n",
       " [0.5263157894736842,\n",
       "  0.4,\n",
       "  0.14736842105263157,\n",
       "  0.1251818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.585],\n",
       " [0.3473684210526316,\n",
       "  0.28421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.05588888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.055868686868686866,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.28421052631578947,\n",
       "  0.1368421052631579,\n",
       "  0.0557929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.0557929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.525,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.055767676767676765,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.055767676767676765,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.335],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3375],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.28421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3894736842105263,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.09537373737373737,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5075],\n",
       " [0.3894736842105263,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.1283939393939394,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.78],\n",
       " [0.3473684210526316,\n",
       "  0.28421052631578947,\n",
       "  0.1368421052631579,\n",
       "  0.09613131313131312,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06465656565656565,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06501515151515151,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06501515151515151,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06510606060606061,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06523232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.42105263157894735,\n",
       "  0.3473684210526316,\n",
       "  0.14736842105263157,\n",
       "  0.06523232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06097979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5368421052631579,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06445959595959595,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7075],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06476767676767677,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06474242424242424,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5052631578947369,\n",
       "  0.3894736842105263,\n",
       "  0.14736842105263157,\n",
       "  0.06502020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.38],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06502020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5368421052631579,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06487878787878788,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06442929292929293,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.0647020202020202,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.715],\n",
       " [0.47368421052631576,\n",
       "  0.35789473684210527,\n",
       "  0.14736842105263157,\n",
       "  0.06492929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6475],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06507070707070707,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7175],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.14736842105263157,\n",
       "  0.06452020202020202,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06481818181818182,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06460606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.4421052631578947,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.67],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06466666666666666,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0642979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0642979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06425252525252526,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.72],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06423232323232324,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06487373737373738,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5263157894736842,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.06337878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.06496969696969697,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.715],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.4842105263157895,\n",
       "  0.4,\n",
       "  0.15789473684210525,\n",
       "  0.06340404040404041,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06495959595959595,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06472727272727273,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.4421052631578947,\n",
       "  0.3263157894736842,\n",
       "  0.15789473684210525,\n",
       "  0.06472727272727273,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6825],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06481313131313131,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06433333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5052631578947369,\n",
       "  0.3894736842105263,\n",
       "  0.15789473684210525,\n",
       "  0.06471212121212122,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.061030303030303025,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06426262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0646969696969697,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7175],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06434343434343434,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.06434343434343434,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06484848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5052631578947369,\n",
       "  0.42105263157894735,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06114646464646464,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06109090909090909,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.49473684210526314,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.06109090909090909,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06485858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.49473684210526314,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.06485858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06476262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06462121212121212,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.71],\n",
       " [0.5052631578947369,\n",
       "  0.4105263157894737,\n",
       "  0.14736842105263157,\n",
       "  0.06462121212121212,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06338888888888888,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06098989898989899,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7025],\n",
       " [0.5157894736842106,\n",
       "  0.43157894736842106,\n",
       "  0.14736842105263157,\n",
       "  0.06098989898989899,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.705],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06423232323232324,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7025],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06110606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06110606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.064510101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.061126262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06424242424242424,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.47368421052631576,\n",
       "  0.3684210526315789,\n",
       "  0.15789473684210525,\n",
       "  0.06424242424242424,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.33],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06105555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06106565656565656,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.4421052631578947,\n",
       "  0.3368421052631579,\n",
       "  0.15789473684210525,\n",
       "  0.06334848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.33],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06334848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06332323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.43157894736842106,\n",
       "  0.3368421052631579,\n",
       "  0.15789473684210525,\n",
       "  0.06332323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.695],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0632979797979798,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.0632979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.4842105263157895,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.0647979797979798,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06426262626262626,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06482323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5157894736842106,\n",
       "  0.42105263157894735,\n",
       "  0.15789473684210525,\n",
       "  0.06482323232323232,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06417676767676768,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06417676767676768,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06325757575757576,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06325757575757576,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645959595959596,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645959595959596,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06463131313131314,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06457070707070707,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06434848484848485,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06414141414141414,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7225],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06414141414141414,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06455555555555556,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06466666666666666,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.725],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06466666666666666,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06462626262626263,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.05524747474747475,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06442929292929293,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06455050505050505,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455050505050505,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06424747474747475,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06440909090909092,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06464646464646465,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06468181818181819,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0642020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0644949494949495,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0644949494949495,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06412121212121212,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06446464646464646,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06426262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06445454545454546,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06433333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0643030303030303,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0643030303030303,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06453030303030302,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06410606060606061,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.695],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06324242424242424,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06448989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06448989898989899,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.695],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06451515151515151,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06435858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06432323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06476262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06427777777777778,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.07856060606060605,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07878787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07878787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07881313131313132,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7275],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07907575757575758,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07901010101010102,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.0791111111111111,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.715],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07913636363636364,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07923232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07872727272727273,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.0788030303030303,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.07890909090909091,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07896969696969697,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.73],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07906565656565656,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07911616161616161,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.07922727272727273,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.725]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"compute-1-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "print(len(data[\"compute-1-1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['labels', 'CPU1 Temp', 'CPU1 Temp_min', 'CPU1 Temp_max', 'CPU2 Temp', 'CPU2 Temp_min', 'CPU2 Temp_max', 'Inlet Temp', 'Inlet Temp_min', 'Inlet Temp_max', 'Memory usage', 'Memory usage_min', 'Memory usage_max', 'Fan1 speed', 'Fan1 speed_min', 'Fan1 speed_max', 'Fan2 speed', 'Fan2 speed_min', 'Fan2 speed_max', 'Fan3 speed', 'Fan3 speed_min', 'Fan3 speed_max', 'Fan4 speed', 'Fan4 speed_min', 'Fan4 speed_max', 'Power consumption', 'Power consumption_min', 'Power consumption_max', 'mse', 'radius', 'description', '__metrics', 'index', 'axis', 'name', 'text', 'arr', 'total', 'leadername', 'orderG', 'x', 'y', 'vy', 'vx', 'order', 'x2', 'color']"
      ],
      "text/plain": [
       "['labels',\n",
       " 'CPU1 Temp',\n",
       " 'CPU1 Temp_min',\n",
       " 'CPU1 Temp_max',\n",
       " 'CPU2 Temp',\n",
       " 'CPU2 Temp_min',\n",
       " 'CPU2 Temp_max',\n",
       " 'Inlet Temp',\n",
       " 'Inlet Temp_min',\n",
       " 'Inlet Temp_max',\n",
       " 'Memory usage',\n",
       " 'Memory usage_min',\n",
       " 'Memory usage_max',\n",
       " 'Fan1 speed',\n",
       " 'Fan1 speed_min',\n",
       " 'Fan1 speed_max',\n",
       " 'Fan2 speed',\n",
       " 'Fan2 speed_min',\n",
       " 'Fan2 speed_max',\n",
       " 'Fan3 speed',\n",
       " 'Fan3 speed_min',\n",
       " 'Fan3 speed_max',\n",
       " 'Fan4 speed',\n",
       " 'Fan4 speed_min',\n",
       " 'Fan4 speed_max',\n",
       " 'Power consumption',\n",
       " 'Power consumption_min',\n",
       " 'Power consumption_max',\n",
       " 'mse',\n",
       " 'radius',\n",
       " 'description',\n",
       " '__metrics',\n",
       " 'index',\n",
       " 'axis',\n",
       " 'name',\n",
       " 'text',\n",
       " 'arr',\n",
       " 'total',\n",
       " 'leadername',\n",
       " 'orderG',\n",
       " 'x',\n",
       " 'y',\n",
       " 'vy',\n",
       " 'vx',\n",
       " 'order',\n",
       " 'x2',\n",
       " 'color']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = json.load(open('/Users/chaupham/Downloads/17Feb2020_cluster_info.json'))\n",
    "list(data2[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"#1f77b4\",\n",
      "\"#ff7f0e\",\n",
      "\"#2ca02c\",\n",
      "\"#d62728\",\n",
      "\"#9467bd\",\n",
      "\"#8c564b\",\n",
      "\"#e377c2\",\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data2)):\n",
    "    print(f'\"{data2[i][\"color\"]}\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['compute-1-1', 'compute-1-10', 'compute-1-11', 'compute-1-12', 'compute-1-13', 'compute-1-14', 'compute-1-15', 'compute-1-16', 'compute-1-17', 'compute-1-18', 'compute-1-19', 'compute-1-2', 'compute-1-20', 'compute-1-21', 'compute-1-22', 'compute-1-23', 'compute-1-24', 'compute-1-25', 'compute-1-26', 'compute-1-27', 'compute-1-28', 'compute-1-29', 'compute-1-3', 'compute-1-30', 'compute-1-31', 'compute-1-32', 'compute-1-33', 'compute-1-34', 'compute-1-35', 'compute-1-36', 'compute-1-37', 'compute-1-38', 'compute-1-39', 'compute-1-4', 'compute-1-40', 'compute-1-41', 'compute-1-42', 'compute-1-43', 'compute-1-44', 'compute-1-45', 'compute-1-46', 'compute-1-47', 'compute-1-48', 'compute-1-49', 'compute-1-5', 'compute-1-50', 'compute-1-51', 'compute-1-52', 'compute-1-53', 'compute-1-54', 'compute-1-55', 'compute-1-56', 'compute-1-57', 'compute-1-58', 'compute-1-59', 'compute-1-6', 'compute-1-60', 'compute-1-7', 'compute-1-8', 'compute-1-9', 'compute-10-25', 'compute-10-26', 'compute-10-27', 'compute-10-28', 'compute-10-29', 'compute-10-30', 'compute-10-31', 'compute-10-32', 'compute-10-33', 'compute-10-34', 'compute-10-35', 'compute-10-36', 'compute-10-37', 'compute-10-38', 'compute-10-39', 'compute-10-40', 'compute-10-41', 'compute-10-42', 'compute-10-43', 'compute-10-44', 'compute-2-1', 'compute-2-10', 'compute-2-11', 'compute-2-12', 'compute-2-13', 'compute-2-14', 'compute-2-15', 'compute-2-16', 'compute-2-17', 'compute-2-18', 'compute-2-19', 'compute-2-2', 'compute-2-20', 'compute-2-21', 'compute-2-22', 'compute-2-23', 'compute-2-24', 'compute-2-25', 'compute-2-26', 'compute-2-27', 'compute-2-28', 'compute-2-29', 'compute-2-3', 'compute-2-30', 'compute-2-31', 'compute-2-32', 'compute-2-33', 'compute-2-34', 'compute-2-35', 'compute-2-36', 'compute-2-37', 'compute-2-38', 'compute-2-39', 'compute-2-4', 'compute-2-40', 'compute-2-41', 'compute-2-42', 'compute-2-43', 'compute-2-44', 'compute-2-45', 'compute-2-46', 'compute-2-47', 'compute-2-48', 'compute-2-49', 'compute-2-5', 'compute-2-50', 'compute-2-51', 'compute-2-52', 'compute-2-53', 'compute-2-54', 'compute-2-55', 'compute-2-56', 'compute-2-57', 'compute-2-58', 'compute-2-59', 'compute-2-6', 'compute-2-60', 'compute-2-7', 'compute-2-8', 'compute-2-9', 'compute-3-1', 'compute-3-10', 'compute-3-11', 'compute-3-12', 'compute-3-13', 'compute-3-14', 'compute-3-15', 'compute-3-16', 'compute-3-17', 'compute-3-18', 'compute-3-19', 'compute-3-2', 'compute-3-20', 'compute-3-21', 'compute-3-22', 'compute-3-23', 'compute-3-24', 'compute-3-25', 'compute-3-26', 'compute-3-27', 'compute-3-28', 'compute-3-29', 'compute-3-3', 'compute-3-30', 'compute-3-31', 'compute-3-32', 'compute-3-33', 'compute-3-34', 'compute-3-35', 'compute-3-36', 'compute-3-37', 'compute-3-38', 'compute-3-39', 'compute-3-4', 'compute-3-40', 'compute-3-41', 'compute-3-42', 'compute-3-43', 'compute-3-44', 'compute-3-45', 'compute-3-46', 'compute-3-47', 'compute-3-48', 'compute-3-49', 'compute-3-5', 'compute-3-50', 'compute-3-51', 'compute-3-52', 'compute-3-53', 'compute-3-54', 'compute-3-55', 'compute-3-56', 'compute-3-6', 'compute-3-7', 'compute-3-8', 'compute-3-9', 'compute-4-1', 'compute-4-10', 'compute-4-11', 'compute-4-12', 'compute-4-13', 'compute-4-14', 'compute-4-15', 'compute-4-16', 'compute-4-17', 'compute-4-18', 'compute-4-19', 'compute-4-2', 'compute-4-20', 'compute-4-21', 'compute-4-22', 'compute-4-23', 'compute-4-24', 'compute-4-25', 'compute-4-26', 'compute-4-27', 'compute-4-28', 'compute-4-29', 'compute-4-3', 'compute-4-30', 'compute-4-31', 'compute-4-32', 'compute-4-33', 'compute-4-34', 'compute-4-35', 'compute-4-36', 'compute-4-37', 'compute-4-38', 'compute-4-39', 'compute-4-4', 'compute-4-40', 'compute-4-41', 'compute-4-42', 'compute-4-43', 'compute-4-44', 'compute-4-45', 'compute-4-46', 'compute-4-47', 'compute-4-48', 'compute-4-5', 'compute-4-6', 'compute-4-7', 'compute-4-8', 'compute-4-9', 'compute-5-1', 'compute-5-10', 'compute-5-11', 'compute-5-12', 'compute-5-13', 'compute-5-14', 'compute-5-15', 'compute-5-16', 'compute-5-17', 'compute-5-18', 'compute-5-19', 'compute-5-2', 'compute-5-20', 'compute-5-21', 'compute-5-22', 'compute-5-23', 'compute-5-24', 'compute-5-3', 'compute-5-4', 'compute-5-5', 'compute-5-6', 'compute-5-7', 'compute-5-8', 'compute-5-9', 'compute-6-1', 'compute-6-10', 'compute-6-11', 'compute-6-12', 'compute-6-13', 'compute-6-14', 'compute-6-15', 'compute-6-16', 'compute-6-17', 'compute-6-18', 'compute-6-19', 'compute-6-2', 'compute-6-20', 'compute-6-3', 'compute-6-4', 'compute-6-5', 'compute-6-6', 'compute-6-7', 'compute-6-8', 'compute-6-9', 'compute-7-1', 'compute-7-10', 'compute-7-11', 'compute-7-12', 'compute-7-13', 'compute-7-14', 'compute-7-15', 'compute-7-16', 'compute-7-17', 'compute-7-18', 'compute-7-19', 'compute-7-2', 'compute-7-20', 'compute-7-21', 'compute-7-22', 'compute-7-23', 'compute-7-24', 'compute-7-25', 'compute-7-26', 'compute-7-27', 'compute-7-28', 'compute-7-29', 'compute-7-3', 'compute-7-30', 'compute-7-31', 'compute-7-32', 'compute-7-33', 'compute-7-34', 'compute-7-35', 'compute-7-36', 'compute-7-37', 'compute-7-38', 'compute-7-39', 'compute-7-40', 'compute-7-41', 'compute-7-42', 'compute-7-43', 'compute-7-44', 'compute-7-45', 'compute-7-46', 'compute-7-47', 'compute-7-48', 'compute-7-49', 'compute-7-5', 'compute-7-50', 'compute-7-51', 'compute-7-52', 'compute-7-53', 'compute-7-54', 'compute-7-55', 'compute-7-56', 'compute-7-57', 'compute-7-58', 'compute-7-59', 'compute-7-6', 'compute-7-60', 'compute-7-7', 'compute-7-8', 'compute-7-9', 'compute-8-1', 'compute-8-10', 'compute-8-11', 'compute-8-12', 'compute-8-13', 'compute-8-14', 'compute-8-15', 'compute-8-16', 'compute-8-17', 'compute-8-18', 'compute-8-19', 'compute-8-2', 'compute-8-20', 'compute-8-21', 'compute-8-22', 'compute-8-23', 'compute-8-24', 'compute-8-25', 'compute-8-26', 'compute-8-27', 'compute-8-28', 'compute-8-29', 'compute-8-3', 'compute-8-30', 'compute-8-31', 'compute-8-32', 'compute-8-33', 'compute-8-34', 'compute-8-35', 'compute-8-36', 'compute-8-37', 'compute-8-38', 'compute-8-39', 'compute-8-4', 'compute-8-40', 'compute-8-41', 'compute-8-42', 'compute-8-43', 'compute-8-44', 'compute-8-45', 'compute-8-46', 'compute-8-47', 'compute-8-48', 'compute-8-49', 'compute-8-5', 'compute-8-50', 'compute-8-51', 'compute-8-52', 'compute-8-53', 'compute-8-54', 'compute-8-55', 'compute-8-56', 'compute-8-57', 'compute-8-58', 'compute-8-59', 'compute-8-6', 'compute-8-60', 'compute-8-7', 'compute-8-8', 'compute-8-9', 'compute-9-1', 'compute-9-10', 'compute-9-11', 'compute-9-12', 'compute-9-13', 'compute-9-14', 'compute-9-15', 'compute-9-16', 'compute-9-17', 'compute-9-18', 'compute-9-19', 'compute-9-2', 'compute-9-20', 'compute-9-21', 'compute-9-22', 'compute-9-23', 'compute-9-24', 'compute-9-25', 'compute-9-26', 'compute-9-27', 'compute-9-28', 'compute-9-29', 'compute-9-3', 'compute-9-30', 'compute-9-31', 'compute-9-32', 'compute-9-33', 'compute-9-34', 'compute-9-35', 'compute-9-36', 'compute-9-37', 'compute-9-38', 'compute-9-39', 'compute-9-4', 'compute-9-40', 'compute-9-41', 'compute-9-42', 'compute-9-43', 'compute-9-44', 'compute-9-45', 'compute-9-46', 'compute-9-47', 'compute-9-48', 'compute-9-49', 'compute-9-5', 'compute-9-50', 'compute-9-51', 'compute-9-52', 'compute-9-53', 'compute-9-54', 'compute-9-55', 'compute-9-56', 'compute-9-57', 'compute-9-58', 'compute-9-59', 'compute-9-6', 'compute-9-60', 'compute-9-7', 'compute-9-8', 'compute-9-9']"
      ],
      "text/plain": [
       "['compute-1-1',\n",
       " 'compute-1-10',\n",
       " 'compute-1-11',\n",
       " 'compute-1-12',\n",
       " 'compute-1-13',\n",
       " 'compute-1-14',\n",
       " 'compute-1-15',\n",
       " 'compute-1-16',\n",
       " 'compute-1-17',\n",
       " 'compute-1-18',\n",
       " 'compute-1-19',\n",
       " 'compute-1-2',\n",
       " 'compute-1-20',\n",
       " 'compute-1-21',\n",
       " 'compute-1-22',\n",
       " 'compute-1-23',\n",
       " 'compute-1-24',\n",
       " 'compute-1-25',\n",
       " 'compute-1-26',\n",
       " 'compute-1-27',\n",
       " 'compute-1-28',\n",
       " 'compute-1-29',\n",
       " 'compute-1-3',\n",
       " 'compute-1-30',\n",
       " 'compute-1-31',\n",
       " 'compute-1-32',\n",
       " 'compute-1-33',\n",
       " 'compute-1-34',\n",
       " 'compute-1-35',\n",
       " 'compute-1-36',\n",
       " 'compute-1-37',\n",
       " 'compute-1-38',\n",
       " 'compute-1-39',\n",
       " 'compute-1-4',\n",
       " 'compute-1-40',\n",
       " 'compute-1-41',\n",
       " 'compute-1-42',\n",
       " 'compute-1-43',\n",
       " 'compute-1-44',\n",
       " 'compute-1-45',\n",
       " 'compute-1-46',\n",
       " 'compute-1-47',\n",
       " 'compute-1-48',\n",
       " 'compute-1-49',\n",
       " 'compute-1-5',\n",
       " 'compute-1-50',\n",
       " 'compute-1-51',\n",
       " 'compute-1-52',\n",
       " 'compute-1-53',\n",
       " 'compute-1-54',\n",
       " 'compute-1-55',\n",
       " 'compute-1-56',\n",
       " 'compute-1-57',\n",
       " 'compute-1-58',\n",
       " 'compute-1-59',\n",
       " 'compute-1-6',\n",
       " 'compute-1-60',\n",
       " 'compute-1-7',\n",
       " 'compute-1-8',\n",
       " 'compute-1-9',\n",
       " 'compute-10-25',\n",
       " 'compute-10-26',\n",
       " 'compute-10-27',\n",
       " 'compute-10-28',\n",
       " 'compute-10-29',\n",
       " 'compute-10-30',\n",
       " 'compute-10-31',\n",
       " 'compute-10-32',\n",
       " 'compute-10-33',\n",
       " 'compute-10-34',\n",
       " 'compute-10-35',\n",
       " 'compute-10-36',\n",
       " 'compute-10-37',\n",
       " 'compute-10-38',\n",
       " 'compute-10-39',\n",
       " 'compute-10-40',\n",
       " 'compute-10-41',\n",
       " 'compute-10-42',\n",
       " 'compute-10-43',\n",
       " 'compute-10-44',\n",
       " 'compute-2-1',\n",
       " 'compute-2-10',\n",
       " 'compute-2-11',\n",
       " 'compute-2-12',\n",
       " 'compute-2-13',\n",
       " 'compute-2-14',\n",
       " 'compute-2-15',\n",
       " 'compute-2-16',\n",
       " 'compute-2-17',\n",
       " 'compute-2-18',\n",
       " 'compute-2-19',\n",
       " 'compute-2-2',\n",
       " 'compute-2-20',\n",
       " 'compute-2-21',\n",
       " 'compute-2-22',\n",
       " 'compute-2-23',\n",
       " 'compute-2-24',\n",
       " 'compute-2-25',\n",
       " 'compute-2-26',\n",
       " 'compute-2-27',\n",
       " 'compute-2-28',\n",
       " 'compute-2-29',\n",
       " 'compute-2-3',\n",
       " 'compute-2-30',\n",
       " 'compute-2-31',\n",
       " 'compute-2-32',\n",
       " 'compute-2-33',\n",
       " 'compute-2-34',\n",
       " 'compute-2-35',\n",
       " 'compute-2-36',\n",
       " 'compute-2-37',\n",
       " 'compute-2-38',\n",
       " 'compute-2-39',\n",
       " 'compute-2-4',\n",
       " 'compute-2-40',\n",
       " 'compute-2-41',\n",
       " 'compute-2-42',\n",
       " 'compute-2-43',\n",
       " 'compute-2-44',\n",
       " 'compute-2-45',\n",
       " 'compute-2-46',\n",
       " 'compute-2-47',\n",
       " 'compute-2-48',\n",
       " 'compute-2-49',\n",
       " 'compute-2-5',\n",
       " 'compute-2-50',\n",
       " 'compute-2-51',\n",
       " 'compute-2-52',\n",
       " 'compute-2-53',\n",
       " 'compute-2-54',\n",
       " 'compute-2-55',\n",
       " 'compute-2-56',\n",
       " 'compute-2-57',\n",
       " 'compute-2-58',\n",
       " 'compute-2-59',\n",
       " 'compute-2-6',\n",
       " 'compute-2-60',\n",
       " 'compute-2-7',\n",
       " 'compute-2-8',\n",
       " 'compute-2-9',\n",
       " 'compute-3-1',\n",
       " 'compute-3-10',\n",
       " 'compute-3-11',\n",
       " 'compute-3-12',\n",
       " 'compute-3-13',\n",
       " 'compute-3-14',\n",
       " 'compute-3-15',\n",
       " 'compute-3-16',\n",
       " 'compute-3-17',\n",
       " 'compute-3-18',\n",
       " 'compute-3-19',\n",
       " 'compute-3-2',\n",
       " 'compute-3-20',\n",
       " 'compute-3-21',\n",
       " 'compute-3-22',\n",
       " 'compute-3-23',\n",
       " 'compute-3-24',\n",
       " 'compute-3-25',\n",
       " 'compute-3-26',\n",
       " 'compute-3-27',\n",
       " 'compute-3-28',\n",
       " 'compute-3-29',\n",
       " 'compute-3-3',\n",
       " 'compute-3-30',\n",
       " 'compute-3-31',\n",
       " 'compute-3-32',\n",
       " 'compute-3-33',\n",
       " 'compute-3-34',\n",
       " 'compute-3-35',\n",
       " 'compute-3-36',\n",
       " 'compute-3-37',\n",
       " 'compute-3-38',\n",
       " 'compute-3-39',\n",
       " 'compute-3-4',\n",
       " 'compute-3-40',\n",
       " 'compute-3-41',\n",
       " 'compute-3-42',\n",
       " 'compute-3-43',\n",
       " 'compute-3-44',\n",
       " 'compute-3-45',\n",
       " 'compute-3-46',\n",
       " 'compute-3-47',\n",
       " 'compute-3-48',\n",
       " 'compute-3-49',\n",
       " 'compute-3-5',\n",
       " 'compute-3-50',\n",
       " 'compute-3-51',\n",
       " 'compute-3-52',\n",
       " 'compute-3-53',\n",
       " 'compute-3-54',\n",
       " 'compute-3-55',\n",
       " 'compute-3-56',\n",
       " 'compute-3-6',\n",
       " 'compute-3-7',\n",
       " 'compute-3-8',\n",
       " 'compute-3-9',\n",
       " 'compute-4-1',\n",
       " 'compute-4-10',\n",
       " 'compute-4-11',\n",
       " 'compute-4-12',\n",
       " 'compute-4-13',\n",
       " 'compute-4-14',\n",
       " 'compute-4-15',\n",
       " 'compute-4-16',\n",
       " 'compute-4-17',\n",
       " 'compute-4-18',\n",
       " 'compute-4-19',\n",
       " 'compute-4-2',\n",
       " 'compute-4-20',\n",
       " 'compute-4-21',\n",
       " 'compute-4-22',\n",
       " 'compute-4-23',\n",
       " 'compute-4-24',\n",
       " 'compute-4-25',\n",
       " 'compute-4-26',\n",
       " 'compute-4-27',\n",
       " 'compute-4-28',\n",
       " 'compute-4-29',\n",
       " 'compute-4-3',\n",
       " 'compute-4-30',\n",
       " 'compute-4-31',\n",
       " 'compute-4-32',\n",
       " 'compute-4-33',\n",
       " 'compute-4-34',\n",
       " 'compute-4-35',\n",
       " 'compute-4-36',\n",
       " 'compute-4-37',\n",
       " 'compute-4-38',\n",
       " 'compute-4-39',\n",
       " 'compute-4-4',\n",
       " 'compute-4-40',\n",
       " 'compute-4-41',\n",
       " 'compute-4-42',\n",
       " 'compute-4-43',\n",
       " 'compute-4-44',\n",
       " 'compute-4-45',\n",
       " 'compute-4-46',\n",
       " 'compute-4-47',\n",
       " 'compute-4-48',\n",
       " 'compute-4-5',\n",
       " 'compute-4-6',\n",
       " 'compute-4-7',\n",
       " 'compute-4-8',\n",
       " 'compute-4-9',\n",
       " 'compute-5-1',\n",
       " 'compute-5-10',\n",
       " 'compute-5-11',\n",
       " 'compute-5-12',\n",
       " 'compute-5-13',\n",
       " 'compute-5-14',\n",
       " 'compute-5-15',\n",
       " 'compute-5-16',\n",
       " 'compute-5-17',\n",
       " 'compute-5-18',\n",
       " 'compute-5-19',\n",
       " 'compute-5-2',\n",
       " 'compute-5-20',\n",
       " 'compute-5-21',\n",
       " 'compute-5-22',\n",
       " 'compute-5-23',\n",
       " 'compute-5-24',\n",
       " 'compute-5-3',\n",
       " 'compute-5-4',\n",
       " 'compute-5-5',\n",
       " 'compute-5-6',\n",
       " 'compute-5-7',\n",
       " 'compute-5-8',\n",
       " 'compute-5-9',\n",
       " 'compute-6-1',\n",
       " 'compute-6-10',\n",
       " 'compute-6-11',\n",
       " 'compute-6-12',\n",
       " 'compute-6-13',\n",
       " 'compute-6-14',\n",
       " 'compute-6-15',\n",
       " 'compute-6-16',\n",
       " 'compute-6-17',\n",
       " 'compute-6-18',\n",
       " 'compute-6-19',\n",
       " 'compute-6-2',\n",
       " 'compute-6-20',\n",
       " 'compute-6-3',\n",
       " 'compute-6-4',\n",
       " 'compute-6-5',\n",
       " 'compute-6-6',\n",
       " 'compute-6-7',\n",
       " 'compute-6-8',\n",
       " 'compute-6-9',\n",
       " 'compute-7-1',\n",
       " 'compute-7-10',\n",
       " 'compute-7-11',\n",
       " 'compute-7-12',\n",
       " 'compute-7-13',\n",
       " 'compute-7-14',\n",
       " 'compute-7-15',\n",
       " 'compute-7-16',\n",
       " 'compute-7-17',\n",
       " 'compute-7-18',\n",
       " 'compute-7-19',\n",
       " 'compute-7-2',\n",
       " 'compute-7-20',\n",
       " 'compute-7-21',\n",
       " 'compute-7-22',\n",
       " 'compute-7-23',\n",
       " 'compute-7-24',\n",
       " 'compute-7-25',\n",
       " 'compute-7-26',\n",
       " 'compute-7-27',\n",
       " 'compute-7-28',\n",
       " 'compute-7-29',\n",
       " 'compute-7-3',\n",
       " 'compute-7-30',\n",
       " 'compute-7-31',\n",
       " 'compute-7-32',\n",
       " 'compute-7-33',\n",
       " 'compute-7-34',\n",
       " 'compute-7-35',\n",
       " 'compute-7-36',\n",
       " 'compute-7-37',\n",
       " 'compute-7-38',\n",
       " 'compute-7-39',\n",
       " 'compute-7-40',\n",
       " 'compute-7-41',\n",
       " 'compute-7-42',\n",
       " 'compute-7-43',\n",
       " 'compute-7-44',\n",
       " 'compute-7-45',\n",
       " 'compute-7-46',\n",
       " 'compute-7-47',\n",
       " 'compute-7-48',\n",
       " 'compute-7-49',\n",
       " 'compute-7-5',\n",
       " 'compute-7-50',\n",
       " 'compute-7-51',\n",
       " 'compute-7-52',\n",
       " 'compute-7-53',\n",
       " 'compute-7-54',\n",
       " 'compute-7-55',\n",
       " 'compute-7-56',\n",
       " 'compute-7-57',\n",
       " 'compute-7-58',\n",
       " 'compute-7-59',\n",
       " 'compute-7-6',\n",
       " 'compute-7-60',\n",
       " 'compute-7-7',\n",
       " 'compute-7-8',\n",
       " 'compute-7-9',\n",
       " 'compute-8-1',\n",
       " 'compute-8-10',\n",
       " 'compute-8-11',\n",
       " 'compute-8-12',\n",
       " 'compute-8-13',\n",
       " 'compute-8-14',\n",
       " 'compute-8-15',\n",
       " 'compute-8-16',\n",
       " 'compute-8-17',\n",
       " 'compute-8-18',\n",
       " 'compute-8-19',\n",
       " 'compute-8-2',\n",
       " 'compute-8-20',\n",
       " 'compute-8-21',\n",
       " 'compute-8-22',\n",
       " 'compute-8-23',\n",
       " 'compute-8-24',\n",
       " 'compute-8-25',\n",
       " 'compute-8-26',\n",
       " 'compute-8-27',\n",
       " 'compute-8-28',\n",
       " 'compute-8-29',\n",
       " 'compute-8-3',\n",
       " 'compute-8-30',\n",
       " 'compute-8-31',\n",
       " 'compute-8-32',\n",
       " 'compute-8-33',\n",
       " 'compute-8-34',\n",
       " 'compute-8-35',\n",
       " 'compute-8-36',\n",
       " 'compute-8-37',\n",
       " 'compute-8-38',\n",
       " 'compute-8-39',\n",
       " 'compute-8-4',\n",
       " 'compute-8-40',\n",
       " 'compute-8-41',\n",
       " 'compute-8-42',\n",
       " 'compute-8-43',\n",
       " 'compute-8-44',\n",
       " 'compute-8-45',\n",
       " 'compute-8-46',\n",
       " 'compute-8-47',\n",
       " 'compute-8-48',\n",
       " 'compute-8-49',\n",
       " 'compute-8-5',\n",
       " 'compute-8-50',\n",
       " 'compute-8-51',\n",
       " 'compute-8-52',\n",
       " 'compute-8-53',\n",
       " 'compute-8-54',\n",
       " 'compute-8-55',\n",
       " 'compute-8-56',\n",
       " 'compute-8-57',\n",
       " 'compute-8-58',\n",
       " 'compute-8-59',\n",
       " 'compute-8-6',\n",
       " 'compute-8-60',\n",
       " 'compute-8-7',\n",
       " 'compute-8-8',\n",
       " 'compute-8-9',\n",
       " 'compute-9-1',\n",
       " 'compute-9-10',\n",
       " 'compute-9-11',\n",
       " 'compute-9-12',\n",
       " 'compute-9-13',\n",
       " 'compute-9-14',\n",
       " 'compute-9-15',\n",
       " 'compute-9-16',\n",
       " 'compute-9-17',\n",
       " 'compute-9-18',\n",
       " 'compute-9-19',\n",
       " 'compute-9-2',\n",
       " 'compute-9-20',\n",
       " 'compute-9-21',\n",
       " 'compute-9-22',\n",
       " 'compute-9-23',\n",
       " 'compute-9-24',\n",
       " 'compute-9-25',\n",
       " 'compute-9-26',\n",
       " 'compute-9-27',\n",
       " 'compute-9-28',\n",
       " 'compute-9-29',\n",
       " 'compute-9-3',\n",
       " 'compute-9-30',\n",
       " 'compute-9-31',\n",
       " 'compute-9-32',\n",
       " 'compute-9-33',\n",
       " 'compute-9-34',\n",
       " 'compute-9-35',\n",
       " 'compute-9-36',\n",
       " 'compute-9-37',\n",
       " 'compute-9-38',\n",
       " 'compute-9-39',\n",
       " 'compute-9-4',\n",
       " 'compute-9-40',\n",
       " 'compute-9-41',\n",
       " 'compute-9-42',\n",
       " 'compute-9-43',\n",
       " 'compute-9-44',\n",
       " 'compute-9-45',\n",
       " 'compute-9-46',\n",
       " 'compute-9-47',\n",
       " 'compute-9-48',\n",
       " 'compute-9-49',\n",
       " 'compute-9-5',\n",
       " 'compute-9-50',\n",
       " 'compute-9-51',\n",
       " 'compute-9-52',\n",
       " 'compute-9-53',\n",
       " 'compute-9-54',\n",
       " 'compute-9-55',\n",
       " 'compute-9-56',\n",
       " 'compute-9-57',\n",
       " 'compute-9-58',\n",
       " 'compute-9-59',\n",
       " 'compute-9-6',\n",
       " 'compute-9-60',\n",
       " 'compute-9-7',\n",
       " 'compute-9-8',\n",
       " 'compute-9-9']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = json.load(open('/Users/chaupham/Downloads/17Feb2020_clusterarr.json'))\n",
    "list(label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ],
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['compute-1-1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['CPU1 Temp', 'CPU2 Temp', 'Inlet Temp', 'Memory usage', 'Fan1 speed', 'Fan2 speed', 'Fan3 speed', 'Fan4 speed', 'Power consumption']"
      ],
      "text/plain": [
       "['CPU1 Temp',\n",
       " 'CPU2 Temp',\n",
       " 'Inlet Temp',\n",
       " 'Memory usage',\n",
       " 'Fan1 speed',\n",
       " 'Fan2 speed',\n",
       " 'Fan3 speed',\n",
       " 'Fan4 speed',\n",
       " 'Power consumption']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = list(map(lambda x: x.split(\"_\")[0] ,['CPU1 Temp',\n",
    " 'CPU1 Temp_min',\n",
    " 'CPU1 Temp_max',\n",
    " 'CPU2 Temp',\n",
    " 'CPU2 Temp_min',\n",
    " 'CPU2 Temp_max',\n",
    " 'Inlet Temp',\n",
    " 'Inlet Temp_min',\n",
    " 'Inlet Temp_max',\n",
    " 'Memory usage',\n",
    " 'Memory usage_min',\n",
    " 'Memory usage_max',\n",
    " 'Fan1 speed',\n",
    " 'Fan1 speed_min',\n",
    " 'Fan1 speed_max',\n",
    " 'Fan2 speed',\n",
    " 'Fan2 speed_min',\n",
    " 'Fan2 speed_max',\n",
    " 'Fan3 speed',\n",
    " 'Fan3 speed_min',\n",
    " 'Fan3 speed_max',\n",
    " 'Fan4 speed',\n",
    " 'Fan4 speed_min',\n",
    " 'Fan4 speed_max',\n",
    " 'Power consumption',\n",
    " 'Power consumption_min',\n",
    " 'Power consumption_max']))\n",
    "\n",
    "from  more_itertools import unique_everseen\n",
    "feature_names = list(unique_everseen(feature_names))\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num computes = 467\n"
     ]
    }
   ],
   "source": [
    "data_list = list()\n",
    "key_to_index = dict()\n",
    "for i, key in enumerate(data.keys()):\n",
    "    key_to_index[key] = i\n",
    "    tmp_data = data[key]\n",
    "    tmp_label = label[key]\n",
    "    tmp  =[data_ + [label_] for data_, label_ in zip(tmp_data, tmp_label)]\n",
    "    data_list.append(pd.DataFrame(data=tmp, columns=feature_names + [\"cluster\"]))\n",
    "\n",
    "print(\"Num computes =\",len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_to_index[\"compute-7-55\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    260\n",
       "2     28\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0][\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[1][\"cluster\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict ={0:\"#1f77b4\",\n",
    "1:\"#ff7f0e\",\n",
    "2:\"#2ca02c\",\n",
    "3:\"#d62728\",\n",
    "4:\"#9467bd\",\n",
    "5:\"#8c564b\",\n",
    "6:\"#e377c2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb_and_shap(name, i, df, feature_names, train_error_list, valid_error_list, skip_list, colors_dict, lgb_params = None,):\n",
    "    print(df[\"cluster\"].value_counts())\n",
    "    while df[\"cluster\"].value_counts().min() == 1:\n",
    "        clus = df[\"cluster\"].value_counts().reset_index().tail(1)[\"index\"].values[0]\n",
    "        df = pd.concat([df[df.cluster == clus], df],0)\n",
    "        \n",
    "        \n",
    "    if  df[\"cluster\"].nunique() <= 1:\n",
    "        print(\"Just 1 class. Skipped!\")\n",
    "        skip_list.append(name + \"_\" + str(i))\n",
    "        print(\"\\n\")\n",
    "        return\n",
    "        \n",
    "    elif df[\"cluster\"].nunique() == 2:\n",
    "        metric = \"binary_logloss\"\n",
    "        objective = \"binary\"\n",
    "    else:\n",
    "        metric = \"multi_logloss\"\n",
    "        objective = \"multiclass\"\n",
    "        \n",
    "    print('df[\"cluster\"].nunique()',  df[\"cluster\"].nunique())\n",
    "    class_order = list(set(df.cluster))\n",
    "    \n",
    "    # def run_lgb(df)\n",
    "    feature_cols = feature_names\n",
    "    early_stopping = 200\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df[feature_cols], df[\"cluster\"], test_size=0.33, random_state=2020, stratify = df[\"cluster\"])\n",
    "    random_seed = 2020\n",
    "\n",
    "    ## prepare the model\n",
    "    if lgb_params == None:\n",
    "        lgb_params = {\n",
    "            'boosting_type':'gbdt', 'colsample_bytree':0.8, #'class_weight': {0 : 1. , 1: weight},\n",
    "            'importance_type':'gain', 'learning_rate':0.01, 'max_depth':2,\n",
    "            'min_child_samples':5,# 'min_child_weight':0.001, 'min_split_gain':0.0,\n",
    "            'n_estimators':3000, 'n_jobs': 16, 'num_leaves':15, 'subsample_freq':16,\n",
    "            'seed': random_seed, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "            'subsample':1, 'subsample_for_bin':200000,  \"metric\":metric ,'objective':objective\n",
    "        }\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    record_store = dict()\n",
    "    lgb_model.fit( X=X_train, y=y_train, feature_name = feature_cols, #categorical_feature = cate_cols, \n",
    "                  early_stopping_rounds= early_stopping, eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "                  eval_names=[\"train\", \"valid\"],\n",
    "                  eval_metric= \"multi_logloss\",\n",
    "                  verbose = -1, callbacks = [lgb.record_evaluation(record_store)])\n",
    "\n",
    "    pred_train = lgb_model.predict(X_train)\n",
    "    pred_valid = lgb_model.predict(X_valid)\n",
    "\n",
    "    train_error_list.append(accuracy_score(y_train, pred_train))\n",
    "    valid_error_list.append(accuracy_score(y_valid, pred_valid))\n",
    "    \n",
    "    len_cluster = df[\"cluster\"].nunique()\n",
    "    explainer = shap.TreeExplainer(lgb_model)\n",
    "    shap_values = explainer.shap_values(X=X_valid, y=y_valid)\n",
    "\n",
    "    shap_values_np = np.array(shap_values)\n",
    "\n",
    "    shap_importance = pd.DataFrame({\"feature_name\": feature_cols})\n",
    "    for i, cluster in enumerate(class_order):\n",
    "        shap_class_i = pd.DataFrame({\"feature_name\": feature_cols, cluster: np.abs(shap_values[i]).sum(axis=0)})\n",
    "        shap_importance = pd.merge(shap_importance, shap_class_i, on=\"feature_name\", how=\"inner\")\n",
    "\n",
    "    tmp = shap_importance.drop(\"feature_name\", 1).sum(axis=1).sort_values(ascending=True).index\n",
    "    shap_importance = shap_importance.reindex(tmp)\n",
    "    shap_importance.to_csv(f\"shap_dataframe/shap_importance_{name}.csv\", index=False)\n",
    "    \n",
    "    colors = list()\n",
    "    for c in class_order:\n",
    "        colors.append(colors_dict[c])\n",
    "\n",
    "    # get class ordering from shap values\n",
    "    class_inds = np.argsort([-np.abs(shap_values[i]).mean() for i in range(len(shap_values))])\n",
    "\n",
    "    # create listed colormap\n",
    "    cmap = plt_colors.ListedColormap(np.array(colors)[class_inds])\n",
    "\n",
    "    shap.summary_plot(shap_values, X_valid, max_display =  len(feature_cols), show=False, \n",
    "                      plot_type = \"bar\", color=cmap,\n",
    "                      class_names=[\"cluster \" + str(i) for i in class_order])\n",
    "    \n",
    "    plt.savefig(f\"shap_pictures/dot_plot_{name}\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    260\n",
      "2     28\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1361]\ttrain's binary_logloss: 1.77829e-05\tvalid's binary_logloss: 0.00249396\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    285\n",
      "6      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[99]\ttrain's binary_logloss: 0.0189321\tvalid's binary_logloss: 0.024569\n",
      "\n",
      "\n",
      "0    172\n",
      "3    109\n",
      "2      5\n",
      "6      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[418]\ttrain's multi_logloss: 0.0150477\tvalid's multi_logloss: 0.103939\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    285\n",
      "2      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttrain's binary_logloss: 0.0190579\tvalid's binary_logloss: 0.0479315\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttrain's binary_logloss: 0.0135926\tvalid's binary_logloss: 0.0379162\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's binary_logloss: 0.0323171\tvalid's binary_logloss: 0.0600358\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    145\n",
      "3    139\n",
      "2      3\n",
      "4      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 1.07052e-05\tvalid's multi_logloss: 7.31188e-06\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[369]\ttrain's multi_logloss: 0.0291877\tvalid's multi_logloss: 0.0807475\n",
      "\n",
      "\n",
      "2    180\n",
      "0    108\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[468]\ttrain's binary_logloss: 0.00436636\tvalid's binary_logloss: 0.0392072\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    279\n",
      "2      9\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\ttrain's binary_logloss: 0.040999\tvalid's binary_logloss: 0.0875211\n",
      "\n",
      "\n",
      "1    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    273\n",
      "2      8\n",
      "3      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[404]\ttrain's multi_logloss: 0.0162741\tvalid's multi_logloss: 0.0399368\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttrain's binary_logloss: 0.024273\tvalid's binary_logloss: 0.0589151\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    198\n",
      "3     66\n",
      "2     24\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[880]\ttrain's multi_logloss: 0.00649495\tvalid's multi_logloss: 0.0701566\n",
      "\n",
      "\n",
      "0    285\n",
      "2      2\n",
      "3      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[340]\ttrain's multi_logloss: 0.00497816\tvalid's multi_logloss: 0.028043\n",
      "\n",
      "\n",
      "0    287\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1245]\ttrain's binary_logloss: 2.03991e-05\tvalid's binary_logloss: 2.33618e-05\n",
      "\n",
      "\n",
      "3    272\n",
      "4     10\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttrain's multi_logloss: 0.0316338\tvalid's multi_logloss: 0.0922022\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    263\n",
      "2     25\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[965]\ttrain's binary_logloss: 1.04375e-05\tvalid's binary_logloss: 1.01117e-05\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[99]\ttrain's binary_logloss: 0.00749203\tvalid's binary_logloss: 0.0530468\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    263\n",
      "2     25\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[965]\ttrain's binary_logloss: 1.0387e-05\tvalid's binary_logloss: 1.00961e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    224\n",
      "2     64\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 2.1589e-05\tvalid's binary_logloss: 8.58069e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    274\n",
      "2     14\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1547]\ttrain's binary_logloss: 6.67926e-05\tvalid's binary_logloss: 0.0237652\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    155\n",
      "3    129\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[630]\ttrain's multi_logloss: 0.00754619\tvalid's multi_logloss: 0.0303677\n",
      "\n",
      "\n",
      "0    285\n",
      "2      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2686]\ttrain's binary_logloss: 2.18964e-05\tvalid's binary_logloss: 0.000307182\n",
      "\n",
      "\n",
      "0    281\n",
      "2      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[243]\ttrain's binary_logloss: 0.00713319\tvalid's binary_logloss: 0.00763597\n",
      "\n",
      "\n",
      "0    265\n",
      "3     23\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2303]\ttrain's binary_logloss: 1.54035e-05\tvalid's binary_logloss: 4.47782e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    155\n",
      "3    129\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2187]\ttrain's multi_logloss: 1.46523e-05\tvalid's multi_logloss: 0.0178915\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    274\n",
      "2     14\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[252]\ttrain's binary_logloss: 0.00848172\tvalid's binary_logloss: 0.0261623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0    257\n",
      "2     31\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[984]\ttrain's binary_logloss: 1.03728e-05\tvalid's binary_logloss: 1.02237e-05\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "6    161\n",
      "3     70\n",
      "0     46\n",
      "4      7\n",
      "2      3\n",
      "5      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 6\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[656]\ttrain's multi_logloss: 0.00799191\tvalid's multi_logloss: 0.0338847\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's binary_logloss: 0.0306305\tvalid's binary_logloss: 0.0601121\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    282\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[306]\ttrain's binary_logloss: 0.00598484\tvalid's binary_logloss: 0.0163075\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    148\n",
      "3    136\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[526]\ttrain's multi_logloss: 0.0115916\tvalid's multi_logloss: 0.0310562\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    181\n",
      "3    103\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[508]\ttrain's multi_logloss: 0.0155509\tvalid's multi_logloss: 0.0530817\n",
      "\n",
      "\n",
      "2    241\n",
      "0     47\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[364]\ttrain's binary_logloss: 0.0391815\tvalid's binary_logloss: 0.122989\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1833]\ttrain's binary_logloss: 2.08297e-05\tvalid's binary_logloss: 3.38753e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "6      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttrain's binary_logloss: 0.0089576\tvalid's binary_logloss: 0.0451842\n",
      "\n",
      "\n",
      "0    222\n",
      "2     66\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.15928e-05\tvalid's binary_logloss: 1.36714e-05\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttrain's binary_logloss: 0.00370497\tvalid's binary_logloss: 0.0402993\n",
      "\n",
      "\n",
      "3    177\n",
      "2     97\n",
      "0     14\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1180]\ttrain's multi_logloss: 6.75948e-06\tvalid's multi_logloss: 6.90948e-06\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttrain's binary_logloss: 0.0030695\tvalid's binary_logloss: 0.0433812\n",
      "\n",
      "\n",
      "0    216\n",
      "2     72\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[534]\ttrain's binary_logloss: 0.0186193\tvalid's binary_logloss: 0.0573623\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    206\n",
      "3     67\n",
      "2     15\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 7.42392e-06\tvalid's multi_logloss: 6.55111e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[567]\ttrain's multi_logloss: 0.0077092\tvalid's multi_logloss: 0.0329072\n",
      "\n",
      "\n",
      "0    283\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2784]\ttrain's binary_logloss: 1.64726e-05\tvalid's binary_logloss: 0.000120897\n",
      "\n",
      "\n",
      "0    280\n",
      "2      8\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[860]\ttrain's binary_logloss: 1.04865e-05\tvalid's binary_logloss: 1.68421e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    270\n",
      "2     18\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[932]\ttrain's binary_logloss: 1.04266e-05\tvalid's binary_logloss: 1.04266e-05\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 0.0076567\tvalid's multi_logloss: 0.0224683\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    149\n",
      "0    139\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1134]\ttrain's binary_logloss: 2.08137e-05\tvalid's binary_logloss: 0.00186213\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[164]\ttrain's binary_logloss: 0.0133992\tvalid's binary_logloss: 0.0212578\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    260\n",
      "0     28\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[566]\ttrain's binary_logloss: 0.00436749\tvalid's binary_logloss: 0.0153355\n",
      "\n",
      "\n",
      "0    237\n",
      "2     45\n",
      "3      5\n",
      "4      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1343]\ttrain's multi_logloss: 1.51023e-05\tvalid's multi_logloss: 0.000894979\n",
      "\n",
      "\n",
      "0    285\n",
      "2      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1729]\ttrain's binary_logloss: 2.07365e-05\tvalid's binary_logloss: 0.000588162\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "2     57\n",
      "0     54\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1160]\ttrain's multi_logloss: 2.42046e-05\tvalid's multi_logloss: 0.0127018\n",
      "\n",
      "\n",
      "0    267\n",
      "2     19\n",
      "3      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[410]\ttrain's multi_logloss: 0.00688152\tvalid's multi_logloss: 0.0304718\n",
      "\n",
      "\n",
      "0    287\n",
      "3      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1110]\ttrain's binary_logloss: 1.78069e-05\tvalid's binary_logloss: 0.00114705\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    249\n",
      "2     39\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1036]\ttrain's binary_logloss: 1.03913e-05\tvalid's binary_logloss: 1.18714e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2298]\ttrain's binary_logloss: 3.10526e-05\tvalid's binary_logloss: 0.00239921\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1607]\ttrain's binary_logloss: 1.65349e-05\tvalid's binary_logloss: 3.05581e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's binary_logloss: 0.0312014\tvalid's binary_logloss: 0.060088\n",
      "\n",
      "\n",
      "2    145\n",
      "0    143\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1454]\ttrain's binary_logloss: 0.00833882\tvalid's binary_logloss: 0.0802553\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    144\n",
      "0    139\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[429]\ttrain's multi_logloss: 0.0116649\tvalid's multi_logloss: 0.0518903\n",
      "\n",
      "\n",
      "0    282\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[295]\ttrain's binary_logloss: 0.00641926\tvalid's binary_logloss: 0.00761295\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0     70\n",
      "2     41\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1337]\ttrain's multi_logloss: 8.33546e-06\tvalid's multi_logloss: 1.37866e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    149\n",
      "0    131\n",
      "2      8\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[531]\ttrain's multi_logloss: 0.00885\tvalid's multi_logloss: 0.044281\n",
      "\n",
      "\n",
      "0    145\n",
      "3    139\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1952]\ttrain's multi_logloss: 3.83798e-05\tvalid's multi_logloss: 0.000232625\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.49813e-05\tvalid's binary_logloss: 0.00053633\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    260\n",
      "2     28\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[975]\ttrain's binary_logloss: 1.03871e-05\tvalid's binary_logloss: 1.01028e-05\n",
      "\n",
      "\n",
      "0    193\n",
      "3     95\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1297]\ttrain's binary_logloss: 1.35694e-05\tvalid's binary_logloss: 7.44344e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    281\n",
      "2      6\n",
      "3      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2221]\ttrain's multi_logloss: 9.21897e-06\tvalid's multi_logloss: 6.30653e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    251\n",
      "3     35\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[393]\ttrain's multi_logloss: 0.0128095\tvalid's multi_logloss: 0.0370981\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    161\n",
      "3     99\n",
      "2     14\n",
      "4     13\n",
      "6      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1153]\ttrain's multi_logloss: 0.0100346\tvalid's multi_logloss: 0.103006\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    125\n",
      "0     97\n",
      "2     66\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[449]\ttrain's multi_logloss: 0.0208112\tvalid's multi_logloss: 0.0704245\n",
      "\n",
      "\n",
      "2    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    212\n",
      "2     69\n",
      "4      5\n",
      "3      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[516]\ttrain's multi_logloss: 0.0104179\tvalid's multi_logloss: 0.0678562\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttrain's binary_logloss: 0.0251956\tvalid's binary_logloss: 0.0539083\n",
      "\n",
      "\n",
      "0    223\n",
      "2     65\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1042]\ttrain's binary_logloss: 1.04465e-05\tvalid's binary_logloss: 1.04416e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    262\n",
      "3     22\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1749]\ttrain's multi_logloss: 4.66463e-05\tvalid's multi_logloss: 0.00112787\n",
      "\n",
      "\n",
      "0    282\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 2.06386e-05\tvalid's binary_logloss: 1.85154e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttrain's binary_logloss: 0.0249457\tvalid's binary_logloss: 0.0195474\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\ttrain's binary_logloss: 0.0105567\tvalid's binary_logloss: 0.0265597\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    186\n",
      "2    101\n",
      "4      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[532]\ttrain's multi_logloss: 0.013561\tvalid's multi_logloss: 0.0731368\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[908]\ttrain's binary_logloss: 0.01173\tvalid's binary_logloss: 0.0204616\n",
      "\n",
      "\n",
      "0    283\n",
      "3      4\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1406]\ttrain's multi_logloss: 0.000845074\tvalid's multi_logloss: 0.00542127\n",
      "\n",
      "\n",
      "0    271\n",
      "2     17\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[194]\ttrain's binary_logloss: 0.0162448\tvalid's binary_logloss: 0.056097\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    277\n",
      "2     11\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[899]\ttrain's binary_logloss: 1.04183e-05\tvalid's binary_logloss: 1.49618e-05\n",
      "\n",
      "\n",
      "0    272\n",
      "2     16\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[924]\ttrain's binary_logloss: 1.04091e-05\tvalid's binary_logloss: 9.96826e-06\n",
      "\n",
      "\n",
      "3    263\n",
      "0     21\n",
      "2      3\n",
      "5      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[418]\ttrain's multi_logloss: 0.0146764\tvalid's multi_logloss: 0.030405\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    285\n",
      "2      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1179]\ttrain's binary_logloss: 1.94894e-05\tvalid's binary_logloss: 0.00806124\n",
      "\n",
      "\n",
      "2    281\n",
      "0      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[849]\ttrain's binary_logloss: 1.03874e-05\tvalid's binary_logloss: 9.35577e-06\n",
      "\n",
      "\n",
      "0    285\n",
      "3      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.14059e-05\tvalid's binary_logloss: 6.15882e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    248\n",
      "3     39\n",
      "4      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1859]\ttrain's multi_logloss: 1.3599e-05\tvalid's multi_logloss: 0.00172481\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    207\n",
      "3     44\n",
      "2     37\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttrain's multi_logloss: 0.0159077\tvalid's multi_logloss: 0.0732584\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    234\n",
      "2     44\n",
      "3     10\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[412]\ttrain's multi_logloss: 0.00634831\tvalid's multi_logloss: 0.0276427\n",
      "\n",
      "\n",
      "0    256\n",
      "2     22\n",
      "3     10\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttrain's multi_logloss: 0.0182408\tvalid's multi_logloss: 0.0872521\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    144\n",
      "0    138\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[408]\ttrain's multi_logloss: 0.015405\tvalid's multi_logloss: 0.0517325\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1014]\ttrain's binary_logloss: 1.6544e-05\tvalid's binary_logloss: 0.00707457\n",
      "\n",
      "\n",
      "0    287\n",
      "6      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1066]\ttrain's binary_logloss: 2.99649e-05\tvalid's binary_logloss: 0.000125576\n",
      "\n",
      "\n",
      "0    150\n",
      "2    138\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1079]\ttrain's binary_logloss: 1.0331e-05\tvalid's binary_logloss: 1.03192e-05\n",
      "\n",
      "\n",
      "2    145\n",
      "0    143\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1226]\ttrain's binary_logloss: 1.06057e-05\tvalid's binary_logloss: 5.21107e-06\n",
      "\n",
      "\n",
      "2    206\n",
      "0     82\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1068]\ttrain's binary_logloss: 1.04233e-05\tvalid's binary_logloss: 1.20601e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    166\n",
      "3    121\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[740]\ttrain's multi_logloss: 0.0029383\tvalid's multi_logloss: 0.0450496\n",
      "\n",
      "\n",
      "0    258\n",
      "2     30\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[991]\ttrain's binary_logloss: 1.04133e-05\tvalid's binary_logloss: 1.01653e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    153\n",
      "2    133\n",
      "6      1\n",
      "4      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[592]\ttrain's multi_logloss: 0.0105464\tvalid's multi_logloss: 0.0803784\n",
      "\n",
      "\n",
      "0    281\n",
      "2      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttrain's binary_logloss: 0.0210425\tvalid's binary_logloss: 0.037124\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[207]\ttrain's binary_logloss: 0.00846084\tvalid's binary_logloss: 0.0207762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3    177\n",
      "0    111\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1085]\ttrain's binary_logloss: 1.04085e-05\tvalid's binary_logloss: 1.21419e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    109\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 6.57964e-06\tvalid's multi_logloss: 0.00013436\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    271\n",
      "2     17\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.12729e-05\tvalid's binary_logloss: 4.95693e-06\n",
      "\n",
      "\n",
      "3    144\n",
      "0    130\n",
      "2     14\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[796]\ttrain's multi_logloss: 0.00185988\tvalid's multi_logloss: 0.0248559\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    194\n",
      "0     94\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1379]\ttrain's binary_logloss: 0.0101734\tvalid's binary_logloss: 0.0266837\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    268\n",
      "2     20\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.74606e-05\tvalid's binary_logloss: 1.2123e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    285\n",
      "2      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[488]\ttrain's binary_logloss: 0.00175917\tvalid's binary_logloss: 0.0118566\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    243\n",
      "2     45\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1015]\ttrain's binary_logloss: 1.03553e-05\tvalid's binary_logloss: 1.06953e-05\n",
      "\n",
      "\n",
      "4    178\n",
      "2    109\n",
      "0      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1858]\ttrain's multi_logloss: 1.17328e-05\tvalid's multi_logloss: 0.00133552\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    149\n",
      "0    139\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 2.51656e-05\tvalid's binary_logloss: 5.44096e-06\n",
      "\n",
      "\n",
      "0    249\n",
      "2     39\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1002]\ttrain's binary_logloss: 1.03643e-05\tvalid's binary_logloss: 1.03643e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    248\n",
      "2     40\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1019]\ttrain's binary_logloss: 1.04061e-05\tvalid's binary_logloss: 9.49864e-06\n",
      "\n",
      "\n",
      "2    276\n",
      "4     12\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\ttrain's binary_logloss: 0.0106444\tvalid's binary_logloss: 0.0325061\n",
      "\n",
      "\n",
      "2    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    142\n",
      "0     78\n",
      "2     68\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1325]\ttrain's multi_logloss: 0.00623728\tvalid's multi_logloss: 0.0788595\n",
      "\n",
      "\n",
      "0    179\n",
      "2    109\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[405]\ttrain's binary_logloss: 0.0165803\tvalid's binary_logloss: 0.0786554\n",
      "\n",
      "\n",
      "2    159\n",
      "0    107\n",
      "3     17\n",
      "4      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[495]\ttrain's multi_logloss: 0.0111818\tvalid's multi_logloss: 0.0753039\n",
      "\n",
      "\n",
      "3    177\n",
      "0     90\n",
      "2     21\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[463]\ttrain's multi_logloss: 0.00922508\tvalid's multi_logloss: 0.0234808\n",
      "\n",
      "\n",
      "0    137\n",
      "2    133\n",
      "3     18\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[441]\ttrain's multi_logloss: 0.0208315\tvalid's multi_logloss: 0.0825896\n",
      "\n",
      "\n",
      "0    283\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttrain's binary_logloss: 0.0272532\tvalid's binary_logloss: 0.0534282\n",
      "\n",
      "\n",
      "3    213\n",
      "2     60\n",
      "0     15\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1835]\ttrain's multi_logloss: 8.67741e-06\tvalid's multi_logloss: 0.000475458\n",
      "\n",
      "\n",
      "0    285\n",
      "2      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[189]\ttrain's binary_logloss: 0.0128591\tvalid's binary_logloss: 0.028766\n",
      "\n",
      "\n",
      "0    187\n",
      "2    101\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1916]\ttrain's binary_logloss: 1.15339e-05\tvalid's binary_logloss: 9.78264e-06\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    107\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[417]\ttrain's multi_logloss: 0.0119837\tvalid's multi_logloss: 0.057949\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[168]\ttrain's binary_logloss: 0.0126025\tvalid's binary_logloss: 0.0210358\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttrain's binary_logloss: 0.00374089\tvalid's binary_logloss: 0.0401784\n",
      "\n",
      "\n",
      "0    261\n",
      "3     26\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1549]\ttrain's multi_logloss: 1.03678e-05\tvalid's multi_logloss: 4.45773e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    236\n",
      "2     34\n",
      "3     18\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2026]\ttrain's multi_logloss: 0.000134507\tvalid's multi_logloss: 0.00106843\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    281\n",
      "2      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[417]\ttrain's binary_logloss: 0.00983943\tvalid's binary_logloss: 0.007035\n",
      "\n",
      "\n",
      "0    285\n",
      "2      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1086]\ttrain's binary_logloss: 1.92835e-05\tvalid's binary_logloss: 0.000112816\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    227\n",
      "2     61\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1038]\ttrain's binary_logloss: 1.03817e-05\tvalid's binary_logloss: 1.05395e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    181\n",
      "2    107\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[462]\ttrain's binary_logloss: 0.0051163\tvalid's binary_logloss: 0.0393207\n",
      "\n",
      "\n",
      "0    282\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.35057e-05\tvalid's binary_logloss: 5.08822e-05\n",
      "\n",
      "\n",
      "0    285\n",
      "3      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.44463e-05\tvalid's binary_logloss: 1.78684e-05\n",
      "\n",
      "\n",
      "3    177\n",
      "0    107\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[521]\ttrain's multi_logloss: 0.00603372\tvalid's multi_logloss: 0.030993\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    257\n",
      "2     30\n",
      "3      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[324]\ttrain's multi_logloss: 0.0100556\tvalid's multi_logloss: 0.0472263\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    263\n",
      "0     25\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[965]\ttrain's binary_logloss: 1.0334e-05\tvalid's binary_logloss: 1.06382e-05\n",
      "\n",
      "\n",
      "0    287\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[981]\ttrain's binary_logloss: 1.63131e-05\tvalid's binary_logloss: 0.000567811\n",
      "\n",
      "\n",
      "0    281\n",
      "2      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[951]\ttrain's binary_logloss: 1.57918e-05\tvalid's binary_logloss: 5.07013e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[99]\ttrain's binary_logloss: 0.00726985\tvalid's binary_logloss: 0.0292549\n",
      "\n",
      "\n",
      "0    280\n",
      "2      8\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[197]\ttrain's binary_logloss: 0.00781109\tvalid's binary_logloss: 0.0308339\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    280\n",
      "2      8\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[353]\ttrain's binary_logloss: 0.00463712\tvalid's binary_logloss: 0.031877\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[317]\ttrain's binary_logloss: 0.0034168\tvalid's binary_logloss: 0.00826485\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    229\n",
      "2     31\n",
      "3     28\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1205]\ttrain's multi_logloss: 8.67201e-06\tvalid's multi_logloss: 2.75906e-05\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttrain's binary_logloss: 0.0273249\tvalid's binary_logloss: 0.0567522\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    281\n",
      "0      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[297]\ttrain's binary_logloss: 0.0097257\tvalid's binary_logloss: 0.0237494\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    249\n",
      "3     36\n",
      "2      2\n",
      "4      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 0.00990933\tvalid's multi_logloss: 0.0114748\n",
      "\n",
      "\n",
      "0    282\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1928]\ttrain's binary_logloss: 2.07566e-05\tvalid's binary_logloss: 1.82156e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    279\n",
      "3      9\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttrain's binary_logloss: 0.029882\tvalid's binary_logloss: 0.108832\n",
      "\n",
      "\n",
      "3    145\n",
      "0    137\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[486]\ttrain's multi_logloss: 0.0119582\tvalid's multi_logloss: 0.0228552\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[968]\ttrain's binary_logloss: 0.00795188\tvalid's binary_logloss: 0.0108687\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[474]\ttrain's multi_logloss: 0.00917136\tvalid's multi_logloss: 0.0510341\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    145\n",
      "0    143\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1092]\ttrain's binary_logloss: 1.0339e-05\tvalid's binary_logloss: 9.99975e-06\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttrain's binary_logloss: 0.0117717\tvalid's binary_logloss: 0.023394\n",
      "\n",
      "\n",
      "0    199\n",
      "2     65\n",
      "3     24\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[382]\ttrain's multi_logloss: 0.0181744\tvalid's multi_logloss: 0.106228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    276\n",
      "2     12\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.36121e-05\tvalid's binary_logloss: 3.77501e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    269\n",
      "2     19\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[975]\ttrain's binary_logloss: 1.04591e-05\tvalid's binary_logloss: 1.96936e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    228\n",
      "2     60\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1057]\ttrain's binary_logloss: 1.03945e-05\tvalid's binary_logloss: 9.07888e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    262\n",
      "2     26\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[250]\ttrain's binary_logloss: 0.0195065\tvalid's binary_logloss: 0.0847425\n",
      "\n",
      "\n",
      "0    148\n",
      "3    140\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.48158e-05\tvalid's binary_logloss: 4.99123e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    281\n",
      "2      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1270]\ttrain's binary_logloss: 1.24065e-05\tvalid's binary_logloss: 1.05174e-05\n",
      "\n",
      "\n",
      "3    177\n",
      "0    101\n",
      "2     10\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1309]\ttrain's multi_logloss: 8.39304e-06\tvalid's multi_logloss: 2.63224e-05\n",
      "\n",
      "\n",
      "0    271\n",
      "2     17\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[745]\ttrain's binary_logloss: 0.000337955\tvalid's binary_logloss: 0.000670827\n",
      "\n",
      "\n",
      "3    144\n",
      "0    139\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1234]\ttrain's multi_logloss: 0.000408469\tvalid's multi_logloss: 0.0434615\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2369]\ttrain's multi_logloss: 2.1383e-05\tvalid's multi_logloss: 0.000509385\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1075]\ttrain's binary_logloss: 1.98783e-05\tvalid's binary_logloss: 0.00309947\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttrain's binary_logloss: 0.0262718\tvalid's binary_logloss: 0.0322692\n",
      "\n",
      "\n",
      "0    249\n",
      "3     37\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[435]\ttrain's multi_logloss: 0.00842178\tvalid's multi_logloss: 0.028384\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    273\n",
      "2     15\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 2.7977e-05\tvalid's binary_logloss: 2.55526e-05\n",
      "\n",
      "\n",
      "0    207\n",
      "2     81\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.3059e-05\tvalid's binary_logloss: 1.86248e-06\n",
      "\n",
      "\n",
      "0    287\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1321]\ttrain's binary_logloss: 2.18828e-05\tvalid's binary_logloss: 0.000740153\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2335]\ttrain's multi_logloss: 1.39052e-05\tvalid's multi_logloss: 6.78466e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    287\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[861]\ttrain's binary_logloss: 1.67998e-05\tvalid's binary_logloss: 2.46119e-05\n",
      "\n",
      "\n",
      "0    205\n",
      "2     83\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1278]\ttrain's binary_logloss: 1.67644e-05\tvalid's binary_logloss: 1.35553e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    106\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[454]\ttrain's multi_logloss: 0.00755118\tvalid's multi_logloss: 0.0411209\n",
      "\n",
      "\n",
      "0    274\n",
      "3      9\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 7.06976e-06\tvalid's multi_logloss: 8.00439e-06\n",
      "\n",
      "\n",
      "0    247\n",
      "2     41\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttrain's binary_logloss: 0.016594\tvalid's binary_logloss: 0.0888264\n",
      "\n",
      "\n",
      "3    182\n",
      "2     49\n",
      "0     41\n",
      "4     16\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[638]\ttrain's multi_logloss: 0.00316607\tvalid's multi_logloss: 0.0774039\n",
      "\n",
      "\n",
      "3    177\n",
      "2     63\n",
      "0     48\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[428]\ttrain's multi_logloss: 0.00922381\tvalid's multi_logloss: 0.0658494\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 5.49282e-06\tvalid's multi_logloss: 1.15073e-05\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[465]\ttrain's binary_logloss: 0.00041603\tvalid's binary_logloss: 0.0180354\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    236\n",
      "3     47\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 8.63668e-06\tvalid's multi_logloss: 6.49942e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    278\n",
      "2     10\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[420]\ttrain's binary_logloss: 0.00510827\tvalid's binary_logloss: 0.00503803\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "5    137\n",
      "3    102\n",
      "0     35\n",
      "4     13\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttrain's multi_logloss: 0.0563588\tvalid's multi_logloss: 0.279261\n",
      "\n",
      "\n",
      "0    261\n",
      "2     27\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[733]\ttrain's binary_logloss: 0.00270863\tvalid's binary_logloss: 0.0671862\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    107\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2518]\ttrain's multi_logloss: 4.47382e-05\tvalid's multi_logloss: 0.000913404\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[526]\ttrain's multi_logloss: 0.00769557\tvalid's multi_logloss: 0.0349484\n",
      "\n",
      "\n",
      "0    217\n",
      "2     71\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\ttrain's binary_logloss: 0.00497937\tvalid's binary_logloss: 0.0312053\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    221\n",
      "2     67\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\ttrain's binary_logloss: 0.00482645\tvalid's binary_logloss: 0.0334453\n",
      "\n",
      "\n",
      "0    221\n",
      "2     67\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1086]\ttrain's binary_logloss: 1.04326e-05\tvalid's binary_logloss: 9.4382e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    278\n",
      "2     10\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[223]\ttrain's binary_logloss: 0.00934271\tvalid's binary_logloss: 0.0354059\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    106\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2202]\ttrain's multi_logloss: 0.000134721\tvalid's multi_logloss: 0.0157169\n",
      "\n",
      "\n",
      "0    247\n",
      "2     41\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[806]\ttrain's binary_logloss: 0.0106705\tvalid's binary_logloss: 0.0204236\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    107\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 0.0117224\tvalid's multi_logloss: 0.00303058\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\ttrain's binary_logloss: 0.0141049\tvalid's binary_logloss: 0.0214406\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    271\n",
      "2     17\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[956]\ttrain's binary_logloss: 1.04721e-05\tvalid's binary_logloss: 1.00933e-05\n",
      "\n",
      "\n",
      "2    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    261\n",
      "2     27\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[990]\ttrain's binary_logloss: 1.0407e-05\tvalid's binary_logloss: 1.4107e-05\n",
      "\n",
      "\n",
      "0    255\n",
      "2     33\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1023]\ttrain's binary_logloss: 1.04094e-05\tvalid's binary_logloss: 5.38355e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "4    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    276\n",
      "2     12\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[617]\ttrain's binary_logloss: 0.00169795\tvalid's binary_logloss: 0.00249541\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    202\n",
      "2     86\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1060]\ttrain's binary_logloss: 1.03612e-05\tvalid's binary_logloss: 1.04134e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttrain's binary_logloss: 0.00355075\tvalid's binary_logloss: 0.051221\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    282\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[309]\ttrain's binary_logloss: 0.00287248\tvalid's binary_logloss: 0.025654\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    145\n",
      "0    135\n",
      "3      8\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 9.72494e-06\tvalid's multi_logloss: 6.49261e-06\n",
      "\n",
      "\n",
      "3    177\n",
      "0    106\n",
      "2      5\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[521]\ttrain's multi_logloss: 0.00967911\tvalid's multi_logloss: 0.0354698\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    287\n",
      "2      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1028]\ttrain's binary_logloss: 1.95249e-05\tvalid's binary_logloss: 0.000389998\n",
      "\n",
      "\n",
      "3    177\n",
      "0    102\n",
      "2      9\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 6.70596e-06\tvalid's multi_logloss: 5.5514e-06\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "2    213\n",
      "0     75\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[995]\ttrain's binary_logloss: 0.0305375\tvalid's binary_logloss: 0.114274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0    200\n",
      "2     88\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1074]\ttrain's binary_logloss: 1.03931e-05\tvalid's binary_logloss: 1.14623e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[967]\ttrain's binary_logloss: 0.000114509\tvalid's binary_logloss: 0.00880219\n",
      "\n",
      "\n",
      "0    146\n",
      "3    134\n",
      "2      8\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[741]\ttrain's multi_logloss: 0.00394352\tvalid's multi_logloss: 0.0243915\n",
      "\n",
      "\n",
      "0    282\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.6512e-05\tvalid's binary_logloss: 8.76253e-06\n",
      "\n",
      "\n",
      "0    233\n",
      "2     55\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[950]\ttrain's binary_logloss: 0.00013105\tvalid's binary_logloss: 0.00208709\n",
      "\n",
      "\n",
      "2    168\n",
      "0    120\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1758]\ttrain's binary_logloss: 0.0735055\tvalid's binary_logloss: 0.184819\n",
      "\n",
      "\n",
      "0    268\n",
      "2     20\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[947]\ttrain's binary_logloss: 1.04279e-05\tvalid's binary_logloss: 1.08899e-05\n",
      "\n",
      "\n",
      "0    276\n",
      "2     12\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1319]\ttrain's binary_logloss: 2.02382e-05\tvalid's binary_logloss: 3.9475e-07\n",
      "\n",
      "\n",
      "0    280\n",
      "2      8\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttrain's binary_logloss: 0.0173732\tvalid's binary_logloss: 0.0667141\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    144\n",
      "0    103\n",
      "6     33\n",
      "2      8\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1402]\ttrain's multi_logloss: 9.48286e-06\tvalid's multi_logloss: 1.03393e-05\n",
      "\n",
      "\n",
      "0    249\n",
      "2     39\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[364]\ttrain's binary_logloss: 0.00648092\tvalid's binary_logloss: 0.0433448\n",
      "\n",
      "\n",
      "0    279\n",
      "3      5\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1565]\ttrain's multi_logloss: 0.000196938\tvalid's multi_logloss: 0.0252882\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    281\n",
      "2      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[654]\ttrain's binary_logloss: 0.000397444\tvalid's binary_logloss: 0.00494407\n",
      "\n",
      "\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    243\n",
      "2     45\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1348]\ttrain's binary_logloss: 1.95265e-05\tvalid's binary_logloss: 0.000334036\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "3    177\n",
      "0    105\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 7.80021e-06\tvalid's multi_logloss: 2.51954e-05\n",
      "\n",
      "\n",
      "0    282\n",
      "2      6\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1638]\ttrain's binary_logloss: 0.0072544\tvalid's binary_logloss: 0.00724479\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    286\n",
      "2      2\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttrain's binary_logloss: 0.00198928\tvalid's binary_logloss: 0.0344163\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    248\n",
      "3     40\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1005]\ttrain's binary_logloss: 1.04441e-05\tvalid's binary_logloss: 1.03783e-05\n",
      "\n",
      "\n",
      "2    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    144\n",
      "2    141\n",
      "6      3\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[492]\ttrain's multi_logloss: 0.0134536\tvalid's multi_logloss: 0.0470759\n",
      "\n",
      "\n",
      "0    284\n",
      "2      4\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1866]\ttrain's binary_logloss: 1.37193e-05\tvalid's binary_logloss: 2.18924e-05\n",
      "\n",
      "\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n",
      "Just 1 class. Skipped!\n",
      "\n",
      "\n",
      "0    287\n",
      "3      1\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[223]\ttrain's binary_logloss: 0.00200346\tvalid's binary_logloss: 0.010279\n",
      "\n",
      "\n",
      "0    281\n",
      "2      7\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1152]\ttrain's binary_logloss: 1.26083e-05\tvalid's binary_logloss: 1.20662e-05\n",
      "\n",
      "\n",
      "265.25 seconds\n"
     ]
    }
   ],
   "source": [
    "train_error_list = list()\n",
    "valid_error_list = list()\n",
    "skip_list = list()\n",
    "\n",
    "\n",
    "\n",
    "tick = time.time()\n",
    "\n",
    "for i, name in enumerate(list(data.keys())):\n",
    "    run_lgb_and_shap(name, i, data_list[i], feature_names = feature_names, train_error_list = train_error_list, valid_error_list=valid_error_list, skip_list=skip_list, colors_dict=colors_dict)\n",
    "\n",
    "tock = time.time()\n",
    "print(f\"{np.round(tock - tick, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['compute-1-10_1', 'compute-1-13_4', 'compute-1-14_5', 'compute-1-18_9', 'compute-1-19_10', 'compute-1-22_14', 'compute-1-23_15', 'compute-1-24_16', 'compute-1-26_18', 'compute-1-29_21', 'compute-1-3_22', 'compute-1-30_23', 'compute-1-31_24', 'compute-1-36_29', 'compute-1-37_30', 'compute-1-4_33', 'compute-1-40_34', 'compute-1-42_36', 'compute-1-43_37', 'compute-1-45_39', 'compute-1-46_40', 'compute-1-47_41', 'compute-1-49_43', 'compute-1-53_48', 'compute-1-55_50', 'compute-1-58_53', 'compute-1-59_54', 'compute-1-6_55', 'compute-1-60_56', 'compute-1-7_57', 'compute-1-8_58', 'compute-10-26_61', 'compute-10-27_62', 'compute-10-28_63', 'compute-10-30_65', 'compute-10-31_66', 'compute-10-33_68', 'compute-10-34_69', 'compute-10-35_70', 'compute-10-36_71', 'compute-10-37_72', 'compute-10-38_73', 'compute-10-39_74', 'compute-10-40_75', 'compute-10-41_76', 'compute-10-44_79', 'compute-2-1_80', 'compute-2-10_81', 'compute-2-12_83', 'compute-2-19_90', 'compute-2-2_91', 'compute-2-20_92', 'compute-2-22_94', 'compute-2-26_98', 'compute-2-29_101', 'compute-2-30_103', 'compute-2-32_105', 'compute-2-33_106', 'compute-2-37_110', 'compute-2-40_114', 'compute-2-42_116', 'compute-2-43_117', 'compute-2-46_120', 'compute-2-49_123', 'compute-2-5_124', 'compute-2-50_125', 'compute-2-53_128', 'compute-2-54_129', 'compute-2-56_131', 'compute-2-57_132', 'compute-2-60_136', 'compute-2-9_139', 'compute-3-10_141', 'compute-3-11_142', 'compute-3-13_144', 'compute-3-14_145', 'compute-3-15_146', 'compute-3-16_147', 'compute-3-17_148', 'compute-3-19_150', 'compute-3-2_151', 'compute-3-20_152', 'compute-3-22_154', 'compute-3-23_155', 'compute-3-25_157', 'compute-3-26_158', 'compute-3-29_161', 'compute-3-31_164', 'compute-3-33_166', 'compute-3-35_168', 'compute-3-36_169', 'compute-3-40_174', 'compute-3-44_178', 'compute-3-45_179', 'compute-3-46_180', 'compute-3-5_184', 'compute-3-51_186', 'compute-3-53_188', 'compute-3-54_189', 'compute-3-55_190', 'compute-3-7_193', 'compute-3-8_194', 'compute-3-9_195', 'compute-4-1_196', 'compute-4-10_197', 'compute-4-12_199', 'compute-4-18_205', 'compute-4-19_206', 'compute-4-2_207', 'compute-4-22_210', 'compute-4-23_211', 'compute-4-24_212', 'compute-4-27_215', 'compute-4-3_218', 'compute-4-31_220', 'compute-4-34_223', 'compute-4-36_225', 'compute-4-37_226', 'compute-4-38_227', 'compute-4-39_228', 'compute-4-40_230', 'compute-4-42_232', 'compute-4-43_233', 'compute-4-44_234', 'compute-4-47_237', 'compute-4-48_238', 'compute-4-5_239', 'compute-4-6_240', 'compute-4-7_241', 'compute-4-8_242', 'compute-4-9_243', 'compute-5-11_246', 'compute-5-14_249', 'compute-5-15_250', 'compute-5-16_251', 'compute-5-17_252', 'compute-5-4_262', 'compute-5-6_264', 'compute-5-8_266', 'compute-6-10_269', 'compute-6-12_271', 'compute-6-15_274', 'compute-6-17_276', 'compute-6-18_277', 'compute-6-19_278', 'compute-6-5_283', 'compute-6-6_284', 'compute-6-8_286', 'compute-7-11_290', 'compute-7-14_293', 'compute-7-15_294', 'compute-7-18_297', 'compute-7-19_298', 'compute-7-21_301', 'compute-7-23_303', 'compute-7-26_306', 'compute-7-27_307', 'compute-7-28_308', 'compute-7-32_313', 'compute-7-33_314', 'compute-7-34_315', 'compute-7-35_316', 'compute-7-36_317', 'compute-7-40_321', 'compute-7-42_323', 'compute-7-43_324', 'compute-7-45_326', 'compute-7-46_327', 'compute-7-47_328', 'compute-7-49_330', 'compute-7-51_333', 'compute-7-6_342', 'compute-7-9_346', 'compute-8-10_348', 'compute-8-11_349', 'compute-8-12_350', 'compute-8-15_353', 'compute-8-16_354', 'compute-8-17_355', 'compute-8-18_356', 'compute-8-19_357', 'compute-8-24_363', 'compute-8-27_366', 'compute-8-28_367', 'compute-8-29_368', 'compute-8-3_369', 'compute-8-31_371', 'compute-8-32_372', 'compute-8-34_374', 'compute-8-35_375', 'compute-8-38_378', 'compute-8-39_379', 'compute-8-42_383', 'compute-8-43_384', 'compute-8-46_387', 'compute-8-47_388', 'compute-8-48_389', 'compute-8-5_391', 'compute-8-50_392', 'compute-8-53_395', 'compute-8-54_396', 'compute-8-57_399', 'compute-8-59_401', 'compute-8-6_402', 'compute-8-8_405', 'compute-8-9_406', 'compute-9-1_407', 'compute-9-11_409', 'compute-9-13_411', 'compute-9-14_412', 'compute-9-16_414', 'compute-9-17_415', 'compute-9-19_417', 'compute-9-2_418', 'compute-9-20_419', 'compute-9-23_422', 'compute-9-24_423', 'compute-9-25_424', 'compute-9-26_425', 'compute-9-27_426', 'compute-9-3_429', 'compute-9-32_432', 'compute-9-33_433', 'compute-9-41_442', 'compute-9-45_446', 'compute-9-47_448', 'compute-9-48_449', 'compute-9-49_450', 'compute-9-5_451', 'compute-9-50_452', 'compute-9-52_454', 'compute-9-55_457', 'compute-9-57_459', 'compute-9-59_461', 'compute-9-7_464']"
      ],
      "text/plain": [
       "['compute-1-10_1',\n",
       " 'compute-1-13_4',\n",
       " 'compute-1-14_5',\n",
       " 'compute-1-18_9',\n",
       " 'compute-1-19_10',\n",
       " 'compute-1-22_14',\n",
       " 'compute-1-23_15',\n",
       " 'compute-1-24_16',\n",
       " 'compute-1-26_18',\n",
       " 'compute-1-29_21',\n",
       " 'compute-1-3_22',\n",
       " 'compute-1-30_23',\n",
       " 'compute-1-31_24',\n",
       " 'compute-1-36_29',\n",
       " 'compute-1-37_30',\n",
       " 'compute-1-4_33',\n",
       " 'compute-1-40_34',\n",
       " 'compute-1-42_36',\n",
       " 'compute-1-43_37',\n",
       " 'compute-1-45_39',\n",
       " 'compute-1-46_40',\n",
       " 'compute-1-47_41',\n",
       " 'compute-1-49_43',\n",
       " 'compute-1-53_48',\n",
       " 'compute-1-55_50',\n",
       " 'compute-1-58_53',\n",
       " 'compute-1-59_54',\n",
       " 'compute-1-6_55',\n",
       " 'compute-1-60_56',\n",
       " 'compute-1-7_57',\n",
       " 'compute-1-8_58',\n",
       " 'compute-10-26_61',\n",
       " 'compute-10-27_62',\n",
       " 'compute-10-28_63',\n",
       " 'compute-10-30_65',\n",
       " 'compute-10-31_66',\n",
       " 'compute-10-33_68',\n",
       " 'compute-10-34_69',\n",
       " 'compute-10-35_70',\n",
       " 'compute-10-36_71',\n",
       " 'compute-10-37_72',\n",
       " 'compute-10-38_73',\n",
       " 'compute-10-39_74',\n",
       " 'compute-10-40_75',\n",
       " 'compute-10-41_76',\n",
       " 'compute-10-44_79',\n",
       " 'compute-2-1_80',\n",
       " 'compute-2-10_81',\n",
       " 'compute-2-12_83',\n",
       " 'compute-2-19_90',\n",
       " 'compute-2-2_91',\n",
       " 'compute-2-20_92',\n",
       " 'compute-2-22_94',\n",
       " 'compute-2-26_98',\n",
       " 'compute-2-29_101',\n",
       " 'compute-2-30_103',\n",
       " 'compute-2-32_105',\n",
       " 'compute-2-33_106',\n",
       " 'compute-2-37_110',\n",
       " 'compute-2-40_114',\n",
       " 'compute-2-42_116',\n",
       " 'compute-2-43_117',\n",
       " 'compute-2-46_120',\n",
       " 'compute-2-49_123',\n",
       " 'compute-2-5_124',\n",
       " 'compute-2-50_125',\n",
       " 'compute-2-53_128',\n",
       " 'compute-2-54_129',\n",
       " 'compute-2-56_131',\n",
       " 'compute-2-57_132',\n",
       " 'compute-2-60_136',\n",
       " 'compute-2-9_139',\n",
       " 'compute-3-10_141',\n",
       " 'compute-3-11_142',\n",
       " 'compute-3-13_144',\n",
       " 'compute-3-14_145',\n",
       " 'compute-3-15_146',\n",
       " 'compute-3-16_147',\n",
       " 'compute-3-17_148',\n",
       " 'compute-3-19_150',\n",
       " 'compute-3-2_151',\n",
       " 'compute-3-20_152',\n",
       " 'compute-3-22_154',\n",
       " 'compute-3-23_155',\n",
       " 'compute-3-25_157',\n",
       " 'compute-3-26_158',\n",
       " 'compute-3-29_161',\n",
       " 'compute-3-31_164',\n",
       " 'compute-3-33_166',\n",
       " 'compute-3-35_168',\n",
       " 'compute-3-36_169',\n",
       " 'compute-3-40_174',\n",
       " 'compute-3-44_178',\n",
       " 'compute-3-45_179',\n",
       " 'compute-3-46_180',\n",
       " 'compute-3-5_184',\n",
       " 'compute-3-51_186',\n",
       " 'compute-3-53_188',\n",
       " 'compute-3-54_189',\n",
       " 'compute-3-55_190',\n",
       " 'compute-3-7_193',\n",
       " 'compute-3-8_194',\n",
       " 'compute-3-9_195',\n",
       " 'compute-4-1_196',\n",
       " 'compute-4-10_197',\n",
       " 'compute-4-12_199',\n",
       " 'compute-4-18_205',\n",
       " 'compute-4-19_206',\n",
       " 'compute-4-2_207',\n",
       " 'compute-4-22_210',\n",
       " 'compute-4-23_211',\n",
       " 'compute-4-24_212',\n",
       " 'compute-4-27_215',\n",
       " 'compute-4-3_218',\n",
       " 'compute-4-31_220',\n",
       " 'compute-4-34_223',\n",
       " 'compute-4-36_225',\n",
       " 'compute-4-37_226',\n",
       " 'compute-4-38_227',\n",
       " 'compute-4-39_228',\n",
       " 'compute-4-40_230',\n",
       " 'compute-4-42_232',\n",
       " 'compute-4-43_233',\n",
       " 'compute-4-44_234',\n",
       " 'compute-4-47_237',\n",
       " 'compute-4-48_238',\n",
       " 'compute-4-5_239',\n",
       " 'compute-4-6_240',\n",
       " 'compute-4-7_241',\n",
       " 'compute-4-8_242',\n",
       " 'compute-4-9_243',\n",
       " 'compute-5-11_246',\n",
       " 'compute-5-14_249',\n",
       " 'compute-5-15_250',\n",
       " 'compute-5-16_251',\n",
       " 'compute-5-17_252',\n",
       " 'compute-5-4_262',\n",
       " 'compute-5-6_264',\n",
       " 'compute-5-8_266',\n",
       " 'compute-6-10_269',\n",
       " 'compute-6-12_271',\n",
       " 'compute-6-15_274',\n",
       " 'compute-6-17_276',\n",
       " 'compute-6-18_277',\n",
       " 'compute-6-19_278',\n",
       " 'compute-6-5_283',\n",
       " 'compute-6-6_284',\n",
       " 'compute-6-8_286',\n",
       " 'compute-7-11_290',\n",
       " 'compute-7-14_293',\n",
       " 'compute-7-15_294',\n",
       " 'compute-7-18_297',\n",
       " 'compute-7-19_298',\n",
       " 'compute-7-21_301',\n",
       " 'compute-7-23_303',\n",
       " 'compute-7-26_306',\n",
       " 'compute-7-27_307',\n",
       " 'compute-7-28_308',\n",
       " 'compute-7-32_313',\n",
       " 'compute-7-33_314',\n",
       " 'compute-7-34_315',\n",
       " 'compute-7-35_316',\n",
       " 'compute-7-36_317',\n",
       " 'compute-7-40_321',\n",
       " 'compute-7-42_323',\n",
       " 'compute-7-43_324',\n",
       " 'compute-7-45_326',\n",
       " 'compute-7-46_327',\n",
       " 'compute-7-47_328',\n",
       " 'compute-7-49_330',\n",
       " 'compute-7-51_333',\n",
       " 'compute-7-6_342',\n",
       " 'compute-7-9_346',\n",
       " 'compute-8-10_348',\n",
       " 'compute-8-11_349',\n",
       " 'compute-8-12_350',\n",
       " 'compute-8-15_353',\n",
       " 'compute-8-16_354',\n",
       " 'compute-8-17_355',\n",
       " 'compute-8-18_356',\n",
       " 'compute-8-19_357',\n",
       " 'compute-8-24_363',\n",
       " 'compute-8-27_366',\n",
       " 'compute-8-28_367',\n",
       " 'compute-8-29_368',\n",
       " 'compute-8-3_369',\n",
       " 'compute-8-31_371',\n",
       " 'compute-8-32_372',\n",
       " 'compute-8-34_374',\n",
       " 'compute-8-35_375',\n",
       " 'compute-8-38_378',\n",
       " 'compute-8-39_379',\n",
       " 'compute-8-42_383',\n",
       " 'compute-8-43_384',\n",
       " 'compute-8-46_387',\n",
       " 'compute-8-47_388',\n",
       " 'compute-8-48_389',\n",
       " 'compute-8-5_391',\n",
       " 'compute-8-50_392',\n",
       " 'compute-8-53_395',\n",
       " 'compute-8-54_396',\n",
       " 'compute-8-57_399',\n",
       " 'compute-8-59_401',\n",
       " 'compute-8-6_402',\n",
       " 'compute-8-8_405',\n",
       " 'compute-8-9_406',\n",
       " 'compute-9-1_407',\n",
       " 'compute-9-11_409',\n",
       " 'compute-9-13_411',\n",
       " 'compute-9-14_412',\n",
       " 'compute-9-16_414',\n",
       " 'compute-9-17_415',\n",
       " 'compute-9-19_417',\n",
       " 'compute-9-2_418',\n",
       " 'compute-9-20_419',\n",
       " 'compute-9-23_422',\n",
       " 'compute-9-24_423',\n",
       " 'compute-9-25_424',\n",
       " 'compute-9-26_425',\n",
       " 'compute-9-27_426',\n",
       " 'compute-9-3_429',\n",
       " 'compute-9-32_432',\n",
       " 'compute-9-33_433',\n",
       " 'compute-9-41_442',\n",
       " 'compute-9-45_446',\n",
       " 'compute-9-47_448',\n",
       " 'compute-9-48_449',\n",
       " 'compute-9-49_450',\n",
       " 'compute-9-5_451',\n",
       " 'compute-9-50_452',\n",
       " 'compute-9-52_454',\n",
       " 'compute-9-55_457',\n",
       " 'compute-9-57_459',\n",
       " 'compute-9-59_461',\n",
       " 'compute-9-7_464']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "3    288\n",
      "Name: cluster, dtype: int64\n",
      "0    288\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## check some computes having been skipped\n",
    "print(data_list[262].cluster.value_counts())\n",
    "\n",
    "print(data_list[415].cluster.value_counts())\n",
    "\n",
    "print(data_list[464].cluster.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    105101\n",
      "3     18173\n",
      "2     10035\n",
      "4       553\n",
      "1       288\n",
      "6       207\n",
      "5       139\n",
      "Name: cluster, dtype: int64\n",
      "df[\"cluster\"].nunique() 7\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4000]\ttrain's multi_logloss: 0.00470968\tvalid's multi_logloss: 0.00769448\n",
      "\n",
      "\n",
      "111.13 seconds\n"
     ]
    }
   ],
   "source": [
    "## run with all data (combine all the computes)\n",
    "tick = time.time()\n",
    "metric = \"multi_logloss\"\n",
    "objective = \"multiclass\"\n",
    "lgb_params = {\n",
    "        'boosting_type':'gbdt', 'colsample_bytree':0.8, #'class_weight': {0 : 1. , 1: weight},\n",
    "        'importance_type':'gain', 'learning_rate':0.01, 'max_depth':2,\n",
    "        'min_child_samples':15,# 'min_child_weight':0.001, 'min_split_gain':0.0,\n",
    "        'n_estimators':4000, 'n_jobs': 16, 'num_leaves':31, 'subsample_freq':16,\n",
    "        'seed': 2020, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "        'subsample':.9, 'subsample_for_bin':200000,  \"metric\":metric ,'objective':objective\n",
    "    }\n",
    "\n",
    "run_lgb_and_shap(name=\"TotalData\", i=\"None\", df=pd.concat(data_list,0), feature_names=feature_names, train_error_list = train_error_list, valid_error_list=valid_error_list, \n",
    "                 skip_list=skip_list, colors_dict=colors_dict,lgb_params=lgb_params)\n",
    "tock = time.time()\n",
    "print(f\"{np.round(tock - tick, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[1.0, 0.9895833333333334, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9583333333333334, 0.9895833333333334, 0.96875, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9479166666666666, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 0.9791666666666666, 0.9375, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.96875, 1.0, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9583333333333334, 0.9791666666666666, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9583333333333334, 0.96875, 0.9791666666666666, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 0.9583333333333334, 0.9895833333333334, 1.0, 0.96875, 1.0, 1.0, 0.9791666666666666, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9791666666666666, 0.9895833333333334, 0.9791666666666666, 0.9791666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9791666666666666, 0.9895833333333334, 0.96875, 0.9791666666666666, 0.9895833333333334, 0.9791666666666666, 0.9791666666666666, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9895833333333334, 1.0, 0.9791666666666666, 1.0, 0.9895833333333334, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9791666666666666, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9270833333333334, 0.96875, 1.0, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9479166666666666, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9479166666666666, 1.0, 1.0, 0.9791666666666666, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 0.9791666666666666, 1.0, 1.0, 1.0, 0.9975216294160058]"
      ],
      "text/plain": [
       "[1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9583333333333334,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9479166666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9583333333333334,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9583333333333334,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9583333333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9270833333333334,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9479166666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9479166666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9975216294160058]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of computes having SHAP: 233\n"
     ]
    }
   ],
   "source": [
    "print(\"num of computes having SHAP:\", (len(list(data.keys()))) - len(skip_list) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
