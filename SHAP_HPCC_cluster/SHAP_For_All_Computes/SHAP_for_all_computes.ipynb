{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import colors as plt_colors\n",
    "\n",
    "## Measure time\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from json import JSONDecoder, JSONDecodeError  # for reading the JSON data files\n",
    "import re  # for regular expressions\n",
    "import os  # for os related operations\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from collections import Counter\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "data = json.load(open('/Users/chaupham/Downloads/17Feb2020.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compute-1-1', 'compute-1-10', 'compute-1-11', 'compute-1-12', 'compute-1-13', 'compute-1-14', 'compute-1-15', 'compute-1-16', 'compute-1-17', 'compute-1-18', 'compute-1-19', 'compute-1-2', 'compute-1-20', 'compute-1-21', 'compute-1-22', 'compute-1-23', 'compute-1-24', 'compute-1-25', 'compute-1-26', 'compute-1-27', 'compute-1-28', 'compute-1-29', 'compute-1-3', 'compute-1-30', 'compute-1-31', 'compute-1-32', 'compute-1-33', 'compute-1-34', 'compute-1-35', 'compute-1-36', 'compute-1-37', 'compute-1-38', 'compute-1-39', 'compute-1-4', 'compute-1-40', 'compute-1-41', 'compute-1-42', 'compute-1-43', 'compute-1-44', 'compute-1-45', 'compute-1-46', 'compute-1-47', 'compute-1-48', 'compute-1-49', 'compute-1-5', 'compute-1-50', 'compute-1-51', 'compute-1-52', 'compute-1-53', 'compute-1-54', 'compute-1-55', 'compute-1-56', 'compute-1-57', 'compute-1-58', 'compute-1-59', 'compute-1-6', 'compute-1-60', 'compute-1-7', 'compute-1-8', 'compute-1-9', 'compute-10-25', 'compute-10-26', 'compute-10-27', 'compute-10-28', 'compute-10-29', 'compute-10-30', 'compute-10-31', 'compute-10-32', 'compute-10-33', 'compute-10-34', 'compute-10-35', 'compute-10-36', 'compute-10-37', 'compute-10-38', 'compute-10-39', 'compute-10-40', 'compute-10-41', 'compute-10-42', 'compute-10-43', 'compute-10-44', 'compute-2-1', 'compute-2-10', 'compute-2-11', 'compute-2-12', 'compute-2-13', 'compute-2-14', 'compute-2-15', 'compute-2-16', 'compute-2-17', 'compute-2-18', 'compute-2-19', 'compute-2-2', 'compute-2-20', 'compute-2-21', 'compute-2-22', 'compute-2-23', 'compute-2-24', 'compute-2-25', 'compute-2-26', 'compute-2-27', 'compute-2-28', 'compute-2-29', 'compute-2-3', 'compute-2-30', 'compute-2-31', 'compute-2-32', 'compute-2-33', 'compute-2-34', 'compute-2-35', 'compute-2-36', 'compute-2-37', 'compute-2-38', 'compute-2-39', 'compute-2-4', 'compute-2-40', 'compute-2-41', 'compute-2-42', 'compute-2-43', 'compute-2-44', 'compute-2-45', 'compute-2-46', 'compute-2-47', 'compute-2-48', 'compute-2-49', 'compute-2-5', 'compute-2-50', 'compute-2-51', 'compute-2-52', 'compute-2-53', 'compute-2-54', 'compute-2-55', 'compute-2-56', 'compute-2-57', 'compute-2-58', 'compute-2-59', 'compute-2-6', 'compute-2-60', 'compute-2-7', 'compute-2-8', 'compute-2-9', 'compute-3-1', 'compute-3-10', 'compute-3-11', 'compute-3-12', 'compute-3-13', 'compute-3-14', 'compute-3-15', 'compute-3-16', 'compute-3-17', 'compute-3-18', 'compute-3-19', 'compute-3-2', 'compute-3-20', 'compute-3-21', 'compute-3-22', 'compute-3-23', 'compute-3-24', 'compute-3-25', 'compute-3-26', 'compute-3-27', 'compute-3-28', 'compute-3-29', 'compute-3-3', 'compute-3-30', 'compute-3-31', 'compute-3-32', 'compute-3-33', 'compute-3-34', 'compute-3-35', 'compute-3-36', 'compute-3-37', 'compute-3-38', 'compute-3-39', 'compute-3-4', 'compute-3-40', 'compute-3-41', 'compute-3-42', 'compute-3-43', 'compute-3-44', 'compute-3-45', 'compute-3-46', 'compute-3-47', 'compute-3-48', 'compute-3-49', 'compute-3-5', 'compute-3-50', 'compute-3-51', 'compute-3-52', 'compute-3-53', 'compute-3-54', 'compute-3-55', 'compute-3-56', 'compute-3-6', 'compute-3-7', 'compute-3-8', 'compute-3-9', 'compute-4-1', 'compute-4-10', 'compute-4-11', 'compute-4-12', 'compute-4-13', 'compute-4-14', 'compute-4-15', 'compute-4-16', 'compute-4-17', 'compute-4-18', 'compute-4-19', 'compute-4-2', 'compute-4-20', 'compute-4-21', 'compute-4-22', 'compute-4-23', 'compute-4-24', 'compute-4-25', 'compute-4-26', 'compute-4-27', 'compute-4-28', 'compute-4-29', 'compute-4-3', 'compute-4-30', 'compute-4-31', 'compute-4-32', 'compute-4-33', 'compute-4-34', 'compute-4-35', 'compute-4-36', 'compute-4-37', 'compute-4-38', 'compute-4-39', 'compute-4-4', 'compute-4-40', 'compute-4-41', 'compute-4-42', 'compute-4-43', 'compute-4-44', 'compute-4-45', 'compute-4-46', 'compute-4-47', 'compute-4-48', 'compute-4-5', 'compute-4-6', 'compute-4-7', 'compute-4-8', 'compute-4-9', 'compute-5-1', 'compute-5-10', 'compute-5-11', 'compute-5-12', 'compute-5-13', 'compute-5-14', 'compute-5-15', 'compute-5-16', 'compute-5-17', 'compute-5-18', 'compute-5-19', 'compute-5-2', 'compute-5-20', 'compute-5-21', 'compute-5-22', 'compute-5-23', 'compute-5-24', 'compute-5-3', 'compute-5-4', 'compute-5-5', 'compute-5-6', 'compute-5-7', 'compute-5-8', 'compute-5-9', 'compute-6-1', 'compute-6-10', 'compute-6-11', 'compute-6-12', 'compute-6-13', 'compute-6-14', 'compute-6-15', 'compute-6-16', 'compute-6-17', 'compute-6-18', 'compute-6-19', 'compute-6-2', 'compute-6-20', 'compute-6-3', 'compute-6-4', 'compute-6-5', 'compute-6-6', 'compute-6-7', 'compute-6-8', 'compute-6-9', 'compute-7-1', 'compute-7-10', 'compute-7-11', 'compute-7-12', 'compute-7-13', 'compute-7-14', 'compute-7-15', 'compute-7-16', 'compute-7-17', 'compute-7-18', 'compute-7-19', 'compute-7-2', 'compute-7-20', 'compute-7-21', 'compute-7-22', 'compute-7-23', 'compute-7-24', 'compute-7-25', 'compute-7-26', 'compute-7-27', 'compute-7-28', 'compute-7-29', 'compute-7-3', 'compute-7-30', 'compute-7-31', 'compute-7-32', 'compute-7-33', 'compute-7-34', 'compute-7-35', 'compute-7-36', 'compute-7-37', 'compute-7-38', 'compute-7-39', 'compute-7-40', 'compute-7-41', 'compute-7-42', 'compute-7-43', 'compute-7-44', 'compute-7-45', 'compute-7-46', 'compute-7-47', 'compute-7-48', 'compute-7-49', 'compute-7-5', 'compute-7-50', 'compute-7-51', 'compute-7-52', 'compute-7-53', 'compute-7-54', 'compute-7-55', 'compute-7-56', 'compute-7-57', 'compute-7-58', 'compute-7-59', 'compute-7-6', 'compute-7-60', 'compute-7-7', 'compute-7-8', 'compute-7-9', 'compute-8-1', 'compute-8-10', 'compute-8-11', 'compute-8-12', 'compute-8-13', 'compute-8-14', 'compute-8-15', 'compute-8-16', 'compute-8-17', 'compute-8-18', 'compute-8-19', 'compute-8-2', 'compute-8-20', 'compute-8-21', 'compute-8-22', 'compute-8-23', 'compute-8-24', 'compute-8-25', 'compute-8-26', 'compute-8-27', 'compute-8-28', 'compute-8-29', 'compute-8-3', 'compute-8-30', 'compute-8-31', 'compute-8-32', 'compute-8-33', 'compute-8-34', 'compute-8-35', 'compute-8-36', 'compute-8-37', 'compute-8-38', 'compute-8-39', 'compute-8-4', 'compute-8-40', 'compute-8-41', 'compute-8-42', 'compute-8-43', 'compute-8-44', 'compute-8-45', 'compute-8-46', 'compute-8-47', 'compute-8-48', 'compute-8-49', 'compute-8-5', 'compute-8-50', 'compute-8-51', 'compute-8-52', 'compute-8-53', 'compute-8-54', 'compute-8-55', 'compute-8-56', 'compute-8-57', 'compute-8-58', 'compute-8-59', 'compute-8-6', 'compute-8-60', 'compute-8-7', 'compute-8-8', 'compute-8-9', 'compute-9-1', 'compute-9-10', 'compute-9-11', 'compute-9-12', 'compute-9-13', 'compute-9-14', 'compute-9-15', 'compute-9-16', 'compute-9-17', 'compute-9-18', 'compute-9-19', 'compute-9-2', 'compute-9-20', 'compute-9-21', 'compute-9-22', 'compute-9-23', 'compute-9-24', 'compute-9-25', 'compute-9-26', 'compute-9-27', 'compute-9-28', 'compute-9-29', 'compute-9-3', 'compute-9-30', 'compute-9-31', 'compute-9-32', 'compute-9-33', 'compute-9-34', 'compute-9-35', 'compute-9-36', 'compute-9-37', 'compute-9-38', 'compute-9-39', 'compute-9-4', 'compute-9-40', 'compute-9-41', 'compute-9-42', 'compute-9-43', 'compute-9-44', 'compute-9-45', 'compute-9-46', 'compute-9-47', 'compute-9-48', 'compute-9-49', 'compute-9-5', 'compute-9-50', 'compute-9-51', 'compute-9-52', 'compute-9-53', 'compute-9-54', 'compute-9-55', 'compute-9-56', 'compute-9-57', 'compute-9-58', 'compute-9-59', 'compute-9-6', 'compute-9-60', 'compute-9-7', 'compute-9-8', 'compute-9-9']\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "print(list(data.keys()))\n",
    "print(len(list(data.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0664040404040404,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06629292929292929,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663080808080808,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0662979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06630303030303031,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663080808080808,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0664949494949495,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631313131313132,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632323232323233,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06653030303030304,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06644949494949495,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06641919191919192,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.525,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632323232323233,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06642424242424243,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06642424242424243,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06632828282828282,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06631818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06652020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.7375],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06632828282828282,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.7475],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635858585858585,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06636363636363636,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634343434343434,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06635353535353536,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06634848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.1368421052631579,\n",
       "  0.06634848484848485,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633838383838384,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06665656565656566,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06633333333333334,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06636363636363636,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638383838383838,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663939393939394,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06666666666666667,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6210526315789474,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663939393939394,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06638888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7375],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.06655555555555556,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.745],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6105263157894737,\n",
       "  0.4842105263157895,\n",
       "  0.14736842105263157,\n",
       "  0.0663989898989899,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.45263157894736844,\n",
       "  0.37894736842105264,\n",
       "  0.14736842105263157,\n",
       "  0.06365656565656565,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.755],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.11465151515151516,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.12462626262626261,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12471212121212122,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7575],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.1368421052631579,\n",
       "  0.12478282828282829,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7675],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12504040404040404,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12484848484848485,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12491919191919193,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12503030303030302,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12497979797979797,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.735],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12496969696969697,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.124989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6675],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.1250050505050505,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.12512121212121213,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12508080808080807,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.76],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.1368421052631579,\n",
       "  0.12505555555555556,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12506060606060607,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12509090909090909,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12509090909090909,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12508585858585858,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.1251060606060606,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12513131313131312,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.5789473684210527,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.12515656565656566,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12516161616161617,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.765],\n",
       " [0.5578947368421052,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12515151515151515,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7475],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.12515151515151515,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7625],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.12517171717171716,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.76],\n",
       " [0.5894736842105263,\n",
       "  0.47368421052631576,\n",
       "  0.1368421052631579,\n",
       "  0.12516666666666668,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.76],\n",
       " [0.5263157894736842,\n",
       "  0.4,\n",
       "  0.14736842105263157,\n",
       "  0.1251818181818182,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.585],\n",
       " [0.3473684210526316,\n",
       "  0.28421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.05588888888888889,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.055868686868686866,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.28421052631578947,\n",
       "  0.1368421052631579,\n",
       "  0.0557929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.0557929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.525,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.055767676767676765,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.055767676767676765,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578282828282828,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.335],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3375],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.1368421052631579,\n",
       "  0.05578787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.3275],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577777777777778,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.2736842105263158,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3368421052631579,\n",
       "  0.28421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.05577272727272727,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.325],\n",
       " [0.3894736842105263,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.09537373737373737,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5075],\n",
       " [0.3894736842105263,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.1283939393939394,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.78],\n",
       " [0.3473684210526316,\n",
       "  0.28421052631578947,\n",
       "  0.1368421052631579,\n",
       "  0.09613131313131312,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.33],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06465656565656565,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06501515151515151,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06501515151515151,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06510606060606061,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06523232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.42105263157894735,\n",
       "  0.3473684210526316,\n",
       "  0.14736842105263157,\n",
       "  0.06523232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06097979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5368421052631579,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06445959595959595,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7075],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06476767676767677,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06474242424242424,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5052631578947369,\n",
       "  0.3894736842105263,\n",
       "  0.14736842105263157,\n",
       "  0.06502020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.38],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06502020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7125],\n",
       " [0.5368421052631579,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06487878787878788,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06442929292929293,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.0647020202020202,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.715],\n",
       " [0.47368421052631576,\n",
       "  0.35789473684210527,\n",
       "  0.14736842105263157,\n",
       "  0.06492929292929293,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6475],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06507070707070707,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7175],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.14736842105263157,\n",
       "  0.06452020202020202,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06481818181818182,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06460606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.4421052631578947,\n",
       "  0.3368421052631579,\n",
       "  0.14736842105263157,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.67],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06466666666666666,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0642979797979798,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0642979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06425252525252526,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.72],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06423232323232324,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06487373737373738,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5263157894736842,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.06337878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.06496969696969697,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.715],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.4842105263157895,\n",
       "  0.4,\n",
       "  0.15789473684210525,\n",
       "  0.06340404040404041,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06495959595959595,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06472727272727273,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.4421052631578947,\n",
       "  0.3263157894736842,\n",
       "  0.15789473684210525,\n",
       "  0.06472727272727273,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6825],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06481313131313131,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06433333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5052631578947369,\n",
       "  0.3894736842105263,\n",
       "  0.15789473684210525,\n",
       "  0.06471212121212122,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.061030303030303025,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.14736842105263157,\n",
       "  0.06426262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.72],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0646969696969697,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7175],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.065010101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06434343434343434,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5368421052631579,\n",
       "  0.43157894736842106,\n",
       "  0.15789473684210525,\n",
       "  0.06434343434343434,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06484848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5052631578947369,\n",
       "  0.42105263157894735,\n",
       "  0.15789473684210525,\n",
       "  0.06487878787878788,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06114646464646464,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06109090909090909,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.49473684210526314,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.06109090909090909,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06485858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.49473684210526314,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.06485858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06476262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06462121212121212,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.71],\n",
       " [0.5052631578947369,\n",
       "  0.4105263157894737,\n",
       "  0.14736842105263157,\n",
       "  0.06462121212121212,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06338888888888888,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06098989898989899,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7025],\n",
       " [0.5157894736842106,\n",
       "  0.43157894736842106,\n",
       "  0.14736842105263157,\n",
       "  0.06098989898989899,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.705],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06423232323232324,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7025],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06110606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06110606060606061,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.064510101010101,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.061126262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06424242424242424,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.47368421052631576,\n",
       "  0.3684210526315789,\n",
       "  0.15789473684210525,\n",
       "  0.06424242424242424,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.33],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06105555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06106565656565656,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.4421052631578947,\n",
       "  0.3368421052631579,\n",
       "  0.15789473684210525,\n",
       "  0.06334848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.33],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06334848484848485,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7125],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06332323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.43157894736842106,\n",
       "  0.3368421052631579,\n",
       "  0.15789473684210525,\n",
       "  0.06332323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.695],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.0632979797979798,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7175],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.0632979797979798,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.4842105263157895,\n",
       "  0.4105263157894737,\n",
       "  0.15789473684210525,\n",
       "  0.0647979797979798,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7],\n",
       " [0.5684210526315789,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06426262626262626,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06482323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5157894736842106,\n",
       "  0.42105263157894735,\n",
       "  0.15789473684210525,\n",
       "  0.06482323232323232,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5578947368421052,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06417676767676768,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06417676767676768,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5473684210526316,\n",
       "  0.4421052631578947,\n",
       "  0.15789473684210525,\n",
       "  0.06325757575757576,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06325757575757576,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.335],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5684210526315789,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06104545454545454,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645959595959596,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645959595959596,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455555555555556,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06463131313131314,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06457070707070707,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06434848484848485,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06414141414141414,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7225],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06414141414141414,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06455555555555556,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.6975],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06466666666666666,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.725],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06466666666666666,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06462626262626263,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.05524747474747475,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.14736842105263157,\n",
       "  0.06442929292929293,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06455050505050505,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06455050505050505,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06424747474747475,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06440909090909092,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06464646464646465,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06468181818181819,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0642020202020202,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0644949494949495,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.725],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0644949494949495,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06412121212121212,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06446464646464646,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06426262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06445454545454546,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.6975],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06433333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0643030303030303,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0643030303030303,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7025],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06453030303030302,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06410606060606061,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.0645,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.695],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06324242424242424,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06448989898989899,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.705],\n",
       " [0.5789473684210527,\n",
       "  0.45263157894736844,\n",
       "  0.15789473684210525,\n",
       "  0.06448989898989899,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.695],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06451515151515151,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7125],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06435858585858587,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7075],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06432323232323232,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.7075],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.15789473684210525,\n",
       "  0.06476262626262626,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.71],\n",
       " [0.5789473684210527,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.06427777777777778,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.55,\n",
       "  0.6975],\n",
       " [0.5894736842105263,\n",
       "  0.4631578947368421,\n",
       "  0.14736842105263157,\n",
       "  0.07856060606060605,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5458333333333333,\n",
       "  0.7275],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07878787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07878787878787878,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07881313131313132,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7275],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07907575757575758,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5333333333333333,\n",
       "  0.5375,\n",
       "  0.7325],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07901010101010102,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.0791111111111111,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.715],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07913636363636364,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07923232323232324,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7175],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07872727272727273,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7425],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.0788030303030303,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.74],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.07890909090909091,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.72],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07896969696969697,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.73],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07906565656565656,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.15789473684210525,\n",
       "  0.07911616161616161,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.7225],\n",
       " [0.6,\n",
       "  0.47368421052631576,\n",
       "  0.14736842105263157,\n",
       "  0.07922727272727273,\n",
       "  0.5375,\n",
       "  0.5458333333333333,\n",
       "  0.5375,\n",
       "  0.5375,\n",
       "  0.725]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"compute-1-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "print(len(data[\"compute-1-1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['labels', 'CPU1 Temp', 'CPU1 Temp_min', 'CPU1 Temp_max', 'CPU2 Temp', 'CPU2 Temp_min', 'CPU2 Temp_max', 'Inlet Temp', 'Inlet Temp_min', 'Inlet Temp_max', 'Memory usage', 'Memory usage_min', 'Memory usage_max', 'Fan1 speed', 'Fan1 speed_min', 'Fan1 speed_max', 'Fan2 speed', 'Fan2 speed_min', 'Fan2 speed_max', 'Fan3 speed', 'Fan3 speed_min', 'Fan3 speed_max', 'Fan4 speed', 'Fan4 speed_min', 'Fan4 speed_max', 'Power consumption', 'Power consumption_min', 'Power consumption_max', 'mse', 'radius', 'description', '__metrics', 'index', 'axis', 'name', 'text', 'arr', 'total', 'leadername', 'orderG', 'x', 'y', 'vy', 'vx', 'order', 'x2', 'color']"
      ],
      "text/plain": [
       "['labels',\n",
       " 'CPU1 Temp',\n",
       " 'CPU1 Temp_min',\n",
       " 'CPU1 Temp_max',\n",
       " 'CPU2 Temp',\n",
       " 'CPU2 Temp_min',\n",
       " 'CPU2 Temp_max',\n",
       " 'Inlet Temp',\n",
       " 'Inlet Temp_min',\n",
       " 'Inlet Temp_max',\n",
       " 'Memory usage',\n",
       " 'Memory usage_min',\n",
       " 'Memory usage_max',\n",
       " 'Fan1 speed',\n",
       " 'Fan1 speed_min',\n",
       " 'Fan1 speed_max',\n",
       " 'Fan2 speed',\n",
       " 'Fan2 speed_min',\n",
       " 'Fan2 speed_max',\n",
       " 'Fan3 speed',\n",
       " 'Fan3 speed_min',\n",
       " 'Fan3 speed_max',\n",
       " 'Fan4 speed',\n",
       " 'Fan4 speed_min',\n",
       " 'Fan4 speed_max',\n",
       " 'Power consumption',\n",
       " 'Power consumption_min',\n",
       " 'Power consumption_max',\n",
       " 'mse',\n",
       " 'radius',\n",
       " 'description',\n",
       " '__metrics',\n",
       " 'index',\n",
       " 'axis',\n",
       " 'name',\n",
       " 'text',\n",
       " 'arr',\n",
       " 'total',\n",
       " 'leadername',\n",
       " 'orderG',\n",
       " 'x',\n",
       " 'y',\n",
       " 'vy',\n",
       " 'vx',\n",
       " 'order',\n",
       " 'x2',\n",
       " 'color']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = json.load(open('/Users/chaupham/Downloads/17Feb2020_cluster_info.json'))\n",
    "list(data2[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"#1f77b4\",\n",
      "\"#ff7f0e\",\n",
      "\"#2ca02c\",\n",
      "\"#d62728\",\n",
      "\"#9467bd\",\n",
      "\"#8c564b\",\n",
      "\"#e377c2\",\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data2)):\n",
    "    print(f'\"{data2[i][\"color\"]}\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['compute-1-1',\n",
       " 'compute-1-10',\n",
       " 'compute-1-11',\n",
       " 'compute-1-12',\n",
       " 'compute-1-13',\n",
       " 'compute-1-14',\n",
       " 'compute-1-15',\n",
       " 'compute-1-16',\n",
       " 'compute-1-17',\n",
       " 'compute-1-18',\n",
       " 'compute-1-19',\n",
       " 'compute-1-2',\n",
       " 'compute-1-20',\n",
       " 'compute-1-21',\n",
       " 'compute-1-22',\n",
       " 'compute-1-23',\n",
       " 'compute-1-24',\n",
       " 'compute-1-25',\n",
       " 'compute-1-26',\n",
       " 'compute-1-27',\n",
       " 'compute-1-28',\n",
       " 'compute-1-29',\n",
       " 'compute-1-3',\n",
       " 'compute-1-30',\n",
       " 'compute-1-31',\n",
       " 'compute-1-32',\n",
       " 'compute-1-33',\n",
       " 'compute-1-34',\n",
       " 'compute-1-35',\n",
       " 'compute-1-36',\n",
       " 'compute-1-37',\n",
       " 'compute-1-38',\n",
       " 'compute-1-39',\n",
       " 'compute-1-4',\n",
       " 'compute-1-40',\n",
       " 'compute-1-41',\n",
       " 'compute-1-42',\n",
       " 'compute-1-43',\n",
       " 'compute-1-44',\n",
       " 'compute-1-45',\n",
       " 'compute-1-46',\n",
       " 'compute-1-47',\n",
       " 'compute-1-48',\n",
       " 'compute-1-49',\n",
       " 'compute-1-5',\n",
       " 'compute-1-50',\n",
       " 'compute-1-51',\n",
       " 'compute-1-52',\n",
       " 'compute-1-53',\n",
       " 'compute-1-54',\n",
       " 'compute-1-55',\n",
       " 'compute-1-56',\n",
       " 'compute-1-57',\n",
       " 'compute-1-58',\n",
       " 'compute-1-59',\n",
       " 'compute-1-6',\n",
       " 'compute-1-60',\n",
       " 'compute-1-7',\n",
       " 'compute-1-8',\n",
       " 'compute-1-9',\n",
       " 'compute-10-25',\n",
       " 'compute-10-26',\n",
       " 'compute-10-27',\n",
       " 'compute-10-28',\n",
       " 'compute-10-29',\n",
       " 'compute-10-30',\n",
       " 'compute-10-31',\n",
       " 'compute-10-32',\n",
       " 'compute-10-33',\n",
       " 'compute-10-34',\n",
       " 'compute-10-35',\n",
       " 'compute-10-36',\n",
       " 'compute-10-37',\n",
       " 'compute-10-38',\n",
       " 'compute-10-39',\n",
       " 'compute-10-40',\n",
       " 'compute-10-41',\n",
       " 'compute-10-42',\n",
       " 'compute-10-43',\n",
       " 'compute-10-44',\n",
       " 'compute-2-1',\n",
       " 'compute-2-10',\n",
       " 'compute-2-11',\n",
       " 'compute-2-12',\n",
       " 'compute-2-13',\n",
       " 'compute-2-14',\n",
       " 'compute-2-15',\n",
       " 'compute-2-16',\n",
       " 'compute-2-17',\n",
       " 'compute-2-18',\n",
       " 'compute-2-19',\n",
       " 'compute-2-2',\n",
       " 'compute-2-20',\n",
       " 'compute-2-21',\n",
       " 'compute-2-22',\n",
       " 'compute-2-23',\n",
       " 'compute-2-24',\n",
       " 'compute-2-25',\n",
       " 'compute-2-26',\n",
       " 'compute-2-27',\n",
       " 'compute-2-28',\n",
       " 'compute-2-29',\n",
       " 'compute-2-3',\n",
       " 'compute-2-30',\n",
       " 'compute-2-31',\n",
       " 'compute-2-32',\n",
       " 'compute-2-33',\n",
       " 'compute-2-34',\n",
       " 'compute-2-35',\n",
       " 'compute-2-36',\n",
       " 'compute-2-37',\n",
       " 'compute-2-38',\n",
       " 'compute-2-39',\n",
       " 'compute-2-4',\n",
       " 'compute-2-40',\n",
       " 'compute-2-41',\n",
       " 'compute-2-42',\n",
       " 'compute-2-43',\n",
       " 'compute-2-44',\n",
       " 'compute-2-45',\n",
       " 'compute-2-46',\n",
       " 'compute-2-47',\n",
       " 'compute-2-48',\n",
       " 'compute-2-49',\n",
       " 'compute-2-5',\n",
       " 'compute-2-50',\n",
       " 'compute-2-51',\n",
       " 'compute-2-52',\n",
       " 'compute-2-53',\n",
       " 'compute-2-54',\n",
       " 'compute-2-55',\n",
       " 'compute-2-56',\n",
       " 'compute-2-57',\n",
       " 'compute-2-58',\n",
       " 'compute-2-59',\n",
       " 'compute-2-6',\n",
       " 'compute-2-60',\n",
       " 'compute-2-7',\n",
       " 'compute-2-8',\n",
       " 'compute-2-9',\n",
       " 'compute-3-1',\n",
       " 'compute-3-10',\n",
       " 'compute-3-11',\n",
       " 'compute-3-12',\n",
       " 'compute-3-13',\n",
       " 'compute-3-14',\n",
       " 'compute-3-15',\n",
       " 'compute-3-16',\n",
       " 'compute-3-17',\n",
       " 'compute-3-18',\n",
       " 'compute-3-19',\n",
       " 'compute-3-2',\n",
       " 'compute-3-20',\n",
       " 'compute-3-21',\n",
       " 'compute-3-22',\n",
       " 'compute-3-23',\n",
       " 'compute-3-24',\n",
       " 'compute-3-25',\n",
       " 'compute-3-26',\n",
       " 'compute-3-27',\n",
       " 'compute-3-28',\n",
       " 'compute-3-29',\n",
       " 'compute-3-3',\n",
       " 'compute-3-30',\n",
       " 'compute-3-31',\n",
       " 'compute-3-32',\n",
       " 'compute-3-33',\n",
       " 'compute-3-34',\n",
       " 'compute-3-35',\n",
       " 'compute-3-36',\n",
       " 'compute-3-37',\n",
       " 'compute-3-38',\n",
       " 'compute-3-39',\n",
       " 'compute-3-4',\n",
       " 'compute-3-40',\n",
       " 'compute-3-41',\n",
       " 'compute-3-42',\n",
       " 'compute-3-43',\n",
       " 'compute-3-44',\n",
       " 'compute-3-45',\n",
       " 'compute-3-46',\n",
       " 'compute-3-47',\n",
       " 'compute-3-48',\n",
       " 'compute-3-49',\n",
       " 'compute-3-5',\n",
       " 'compute-3-50',\n",
       " 'compute-3-51',\n",
       " 'compute-3-52',\n",
       " 'compute-3-53',\n",
       " 'compute-3-54',\n",
       " 'compute-3-55',\n",
       " 'compute-3-56',\n",
       " 'compute-3-6',\n",
       " 'compute-3-7',\n",
       " 'compute-3-8',\n",
       " 'compute-3-9',\n",
       " 'compute-4-1',\n",
       " 'compute-4-10',\n",
       " 'compute-4-11',\n",
       " 'compute-4-12',\n",
       " 'compute-4-13',\n",
       " 'compute-4-14',\n",
       " 'compute-4-15',\n",
       " 'compute-4-16',\n",
       " 'compute-4-17',\n",
       " 'compute-4-18',\n",
       " 'compute-4-19',\n",
       " 'compute-4-2',\n",
       " 'compute-4-20',\n",
       " 'compute-4-21',\n",
       " 'compute-4-22',\n",
       " 'compute-4-23',\n",
       " 'compute-4-24',\n",
       " 'compute-4-25',\n",
       " 'compute-4-26',\n",
       " 'compute-4-27',\n",
       " 'compute-4-28',\n",
       " 'compute-4-29',\n",
       " 'compute-4-3',\n",
       " 'compute-4-30',\n",
       " 'compute-4-31',\n",
       " 'compute-4-32',\n",
       " 'compute-4-33',\n",
       " 'compute-4-34',\n",
       " 'compute-4-35',\n",
       " 'compute-4-36',\n",
       " 'compute-4-37',\n",
       " 'compute-4-38',\n",
       " 'compute-4-39',\n",
       " 'compute-4-4',\n",
       " 'compute-4-40',\n",
       " 'compute-4-41',\n",
       " 'compute-4-42',\n",
       " 'compute-4-43',\n",
       " 'compute-4-44',\n",
       " 'compute-4-45',\n",
       " 'compute-4-46',\n",
       " 'compute-4-47',\n",
       " 'compute-4-48',\n",
       " 'compute-4-5',\n",
       " 'compute-4-6',\n",
       " 'compute-4-7',\n",
       " 'compute-4-8',\n",
       " 'compute-4-9',\n",
       " 'compute-5-1',\n",
       " 'compute-5-10',\n",
       " 'compute-5-11',\n",
       " 'compute-5-12',\n",
       " 'compute-5-13',\n",
       " 'compute-5-14',\n",
       " 'compute-5-15',\n",
       " 'compute-5-16',\n",
       " 'compute-5-17',\n",
       " 'compute-5-18',\n",
       " 'compute-5-19',\n",
       " 'compute-5-2',\n",
       " 'compute-5-20',\n",
       " 'compute-5-21',\n",
       " 'compute-5-22',\n",
       " 'compute-5-23',\n",
       " 'compute-5-24',\n",
       " 'compute-5-3',\n",
       " 'compute-5-4',\n",
       " 'compute-5-5',\n",
       " 'compute-5-6',\n",
       " 'compute-5-7',\n",
       " 'compute-5-8',\n",
       " 'compute-5-9',\n",
       " 'compute-6-1',\n",
       " 'compute-6-10',\n",
       " 'compute-6-11',\n",
       " 'compute-6-12',\n",
       " 'compute-6-13',\n",
       " 'compute-6-14',\n",
       " 'compute-6-15',\n",
       " 'compute-6-16',\n",
       " 'compute-6-17',\n",
       " 'compute-6-18',\n",
       " 'compute-6-19',\n",
       " 'compute-6-2',\n",
       " 'compute-6-20',\n",
       " 'compute-6-3',\n",
       " 'compute-6-4',\n",
       " 'compute-6-5',\n",
       " 'compute-6-6',\n",
       " 'compute-6-7',\n",
       " 'compute-6-8',\n",
       " 'compute-6-9',\n",
       " 'compute-7-1',\n",
       " 'compute-7-10',\n",
       " 'compute-7-11',\n",
       " 'compute-7-12',\n",
       " 'compute-7-13',\n",
       " 'compute-7-14',\n",
       " 'compute-7-15',\n",
       " 'compute-7-16',\n",
       " 'compute-7-17',\n",
       " 'compute-7-18',\n",
       " 'compute-7-19',\n",
       " 'compute-7-2',\n",
       " 'compute-7-20',\n",
       " 'compute-7-21',\n",
       " 'compute-7-22',\n",
       " 'compute-7-23',\n",
       " 'compute-7-24',\n",
       " 'compute-7-25',\n",
       " 'compute-7-26',\n",
       " 'compute-7-27',\n",
       " 'compute-7-28',\n",
       " 'compute-7-29',\n",
       " 'compute-7-3',\n",
       " 'compute-7-30',\n",
       " 'compute-7-31',\n",
       " 'compute-7-32',\n",
       " 'compute-7-33',\n",
       " 'compute-7-34',\n",
       " 'compute-7-35',\n",
       " 'compute-7-36',\n",
       " 'compute-7-37',\n",
       " 'compute-7-38',\n",
       " 'compute-7-39',\n",
       " 'compute-7-40',\n",
       " 'compute-7-41',\n",
       " 'compute-7-42',\n",
       " 'compute-7-43',\n",
       " 'compute-7-44',\n",
       " 'compute-7-45',\n",
       " 'compute-7-46',\n",
       " 'compute-7-47',\n",
       " 'compute-7-48',\n",
       " 'compute-7-49',\n",
       " 'compute-7-5',\n",
       " 'compute-7-50',\n",
       " 'compute-7-51',\n",
       " 'compute-7-52',\n",
       " 'compute-7-53',\n",
       " 'compute-7-54',\n",
       " 'compute-7-55',\n",
       " 'compute-7-56',\n",
       " 'compute-7-57',\n",
       " 'compute-7-58',\n",
       " 'compute-7-59',\n",
       " 'compute-7-6',\n",
       " 'compute-7-60',\n",
       " 'compute-7-7',\n",
       " 'compute-7-8',\n",
       " 'compute-7-9',\n",
       " 'compute-8-1',\n",
       " 'compute-8-10',\n",
       " 'compute-8-11',\n",
       " 'compute-8-12',\n",
       " 'compute-8-13',\n",
       " 'compute-8-14',\n",
       " 'compute-8-15',\n",
       " 'compute-8-16',\n",
       " 'compute-8-17',\n",
       " 'compute-8-18',\n",
       " 'compute-8-19',\n",
       " 'compute-8-2',\n",
       " 'compute-8-20',\n",
       " 'compute-8-21',\n",
       " 'compute-8-22',\n",
       " 'compute-8-23',\n",
       " 'compute-8-24',\n",
       " 'compute-8-25',\n",
       " 'compute-8-26',\n",
       " 'compute-8-27',\n",
       " 'compute-8-28',\n",
       " 'compute-8-29',\n",
       " 'compute-8-3',\n",
       " 'compute-8-30',\n",
       " 'compute-8-31',\n",
       " 'compute-8-32',\n",
       " 'compute-8-33',\n",
       " 'compute-8-34',\n",
       " 'compute-8-35',\n",
       " 'compute-8-36',\n",
       " 'compute-8-37',\n",
       " 'compute-8-38',\n",
       " 'compute-8-39',\n",
       " 'compute-8-4',\n",
       " 'compute-8-40',\n",
       " 'compute-8-41',\n",
       " 'compute-8-42',\n",
       " 'compute-8-43',\n",
       " 'compute-8-44',\n",
       " 'compute-8-45',\n",
       " 'compute-8-46',\n",
       " 'compute-8-47',\n",
       " 'compute-8-48',\n",
       " 'compute-8-49',\n",
       " 'compute-8-5',\n",
       " 'compute-8-50',\n",
       " 'compute-8-51',\n",
       " 'compute-8-52',\n",
       " 'compute-8-53',\n",
       " 'compute-8-54',\n",
       " 'compute-8-55',\n",
       " 'compute-8-56',\n",
       " 'compute-8-57',\n",
       " 'compute-8-58',\n",
       " 'compute-8-59',\n",
       " 'compute-8-6',\n",
       " 'compute-8-60',\n",
       " 'compute-8-7',\n",
       " 'compute-8-8',\n",
       " 'compute-8-9',\n",
       " 'compute-9-1',\n",
       " 'compute-9-10',\n",
       " 'compute-9-11',\n",
       " 'compute-9-12',\n",
       " 'compute-9-13',\n",
       " 'compute-9-14',\n",
       " 'compute-9-15',\n",
       " 'compute-9-16',\n",
       " 'compute-9-17',\n",
       " 'compute-9-18',\n",
       " 'compute-9-19',\n",
       " 'compute-9-2',\n",
       " 'compute-9-20',\n",
       " 'compute-9-21',\n",
       " 'compute-9-22',\n",
       " 'compute-9-23',\n",
       " 'compute-9-24',\n",
       " 'compute-9-25',\n",
       " 'compute-9-26',\n",
       " 'compute-9-27',\n",
       " 'compute-9-28',\n",
       " 'compute-9-29',\n",
       " 'compute-9-3',\n",
       " 'compute-9-30',\n",
       " 'compute-9-31',\n",
       " 'compute-9-32',\n",
       " 'compute-9-33',\n",
       " 'compute-9-34',\n",
       " 'compute-9-35',\n",
       " 'compute-9-36',\n",
       " 'compute-9-37',\n",
       " 'compute-9-38',\n",
       " 'compute-9-39',\n",
       " 'compute-9-4',\n",
       " 'compute-9-40',\n",
       " 'compute-9-41',\n",
       " 'compute-9-42',\n",
       " 'compute-9-43',\n",
       " 'compute-9-44',\n",
       " 'compute-9-45',\n",
       " 'compute-9-46',\n",
       " 'compute-9-47',\n",
       " 'compute-9-48',\n",
       " 'compute-9-49',\n",
       " 'compute-9-5',\n",
       " 'compute-9-50',\n",
       " 'compute-9-51',\n",
       " 'compute-9-52',\n",
       " 'compute-9-53',\n",
       " 'compute-9-54',\n",
       " 'compute-9-55',\n",
       " 'compute-9-56',\n",
       " 'compute-9-57',\n",
       " 'compute-9-58',\n",
       " 'compute-9-59',\n",
       " 'compute-9-6',\n",
       " 'compute-9-60',\n",
       " 'compute-9-7',\n",
       " 'compute-9-8',\n",
       " 'compute-9-9']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = json.load(open('/Users/chaupham/Downloads/17Feb2020_clusterarr.json'))\n",
    "list(label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['compute-1-1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(map(lambda x: x.split(\"_\")[0] ,['CPU1 Temp',\n",
    " 'CPU1 Temp_min',\n",
    " 'CPU1 Temp_max',\n",
    " 'CPU2 Temp',\n",
    " 'CPU2 Temp_min',\n",
    " 'CPU2 Temp_max',\n",
    " 'Inlet Temp',\n",
    " 'Inlet Temp_min',\n",
    " 'Inlet Temp_max',\n",
    " 'Memory usage',\n",
    " 'Memory usage_min',\n",
    " 'Memory usage_max',\n",
    " 'Fan1 speed',\n",
    " 'Fan1 speed_min',\n",
    " 'Fan1 speed_max',\n",
    " 'Fan2 speed',\n",
    " 'Fan2 speed_min',\n",
    " 'Fan2 speed_max',\n",
    " 'Fan3 speed',\n",
    " 'Fan3 speed_min',\n",
    " 'Fan3 speed_max',\n",
    " 'Fan4 speed',\n",
    " 'Fan4 speed_min',\n",
    " 'Fan4 speed_max',\n",
    " 'Power consumption',\n",
    " 'Power consumption_min',\n",
    " 'Power consumption_max']))\n",
    "\n",
    "from  more_itertools import unique_everseen\n",
    "feature_names = list(unique_everseen(feature_names))\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x120962500>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num computes = 467\n"
     ]
    }
   ],
   "source": [
    "data_list = list()\n",
    "for key in data.keys():\n",
    "    tmp_data = data[key]\n",
    "    tmp_label = label[key]\n",
    "    tmp  =[data_ + [label_] for data_, label_ in zip(tmp_data, tmp_label)]\n",
    "    data_list.append(pd.DataFrame(data=tmp, columns=feature_names + [\"cluster\"]))\n",
    "\n",
    "print(\"Num computes =\",len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    260\n",
       "2     28\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0][\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[1][\"cluster\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict ={0:\"#1f77b4\",\n",
    "1:\"#ff7f0e\",\n",
    "2:\"#2ca02c\",\n",
    "3:\"#d62728\",\n",
    "4:\"#9467bd\",\n",
    "5:\"#8c564b\",\n",
    "6:\"#e377c2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb_and_shap(name, df, feature_names, train_error_list, valid_error_list, skip_list, colors_dict, lgb_params = None):\n",
    "    while df[\"cluster\"].value_counts().min() == 1:\n",
    "        clus = df[\"cluster\"].value_counts().reset_index().tail(1)[\"index\"].values[0]\n",
    "        df = df[df.cluster != clus]\n",
    "        \n",
    "    if  df[\"cluster\"].nunique() <= 1:\n",
    "        print(\"Just 1 class. Skipped!\")\n",
    "        skip_list.append(name)\n",
    "        return\n",
    "        \n",
    "    elif df[\"cluster\"].nunique() == 2:\n",
    "        metric = \"binary_logloss\"\n",
    "        objective = \"binary\"\n",
    "    else:\n",
    "        metric = \"multi_logloss\"\n",
    "        objective = \"multiclass\"\n",
    "        \n",
    "    print('df[\"cluster\"].nunique()',  df[\"cluster\"].nunique())\n",
    "    \n",
    "    # def run_lgb(df)\n",
    "    feature_cols = feature_names\n",
    "    early_stopping = 200\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df[feature_cols], df[\"cluster\"], test_size=0.33, random_state=2020, stratify = df[\"cluster\"])\n",
    "    random_seed = 2020\n",
    "\n",
    "    ## prepare the model\n",
    "    if lgb_params == None:\n",
    "        lgb_params = {\n",
    "            'boosting_type':'gbdt', 'colsample_bytree':0.8, #'class_weight': {0 : 1. , 1: weight},\n",
    "            'importance_type':'gain', 'learning_rate':0.01, 'max_depth':2,\n",
    "            'min_child_samples':15,# 'min_child_weight':0.001, 'min_split_gain':0.0,\n",
    "            'n_estimators':3000, 'n_jobs': 16, 'num_leaves':31, 'subsample_freq':16,\n",
    "            'seed': random_seed, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "            'subsample':.9, 'subsample_for_bin':200000,  \"metric\":metric ,'objective':objective\n",
    "        }\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    record_store = dict()\n",
    "    lgb_model.fit( X=X_train, y=y_train, feature_name = feature_cols, #categorical_feature = cate_cols, \n",
    "                  early_stopping_rounds= early_stopping, eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "                  eval_names=[\"train\", \"valid\"],\n",
    "                  eval_metric= \"multi_logloss\",\n",
    "                  verbose = -1, callbacks = [lgb.record_evaluation(record_store)])\n",
    "\n",
    "    pred_train = lgb_model.predict(X_train)\n",
    "    pred_valid = lgb_model.predict(X_valid)\n",
    "\n",
    "    train_error_list.append(accuracy_score(y_train, pred_train))\n",
    "    valid_error_list.append(accuracy_score(y_valid, pred_valid))\n",
    "    \n",
    "    len_cluster = df[\"cluster\"].nunique()\n",
    "    explainer = shap.TreeExplainer(lgb_model)\n",
    "    shap_values = explainer.shap_values(X=X_valid.head(1), y=y_valid.head(1))\n",
    "\n",
    "    shap_values_np = np.array(shap_values)\n",
    "\n",
    "    shap_importance = pd.DataFrame({\"feature_name\": feature_cols})\n",
    "    for i, cluster in enumerate(df.cluster.unique()):\n",
    "        shap_class_i = pd.DataFrame({\"feature_name\": feature_cols, cluster: np.abs(shap_values[i]).sum(axis=0)})\n",
    "        shap_importance = pd.merge(shap_importance, shap_class_i, on=\"feature_name\", how=\"inner\")\n",
    "\n",
    "    tmp = shap_importance.drop(\"feature_name\", 1).sum(axis=1).sort_values(ascending=True).index\n",
    "    shap_importance = shap_importance.reindex(tmp)\n",
    "    shap_importance.to_csv(f\"shap_dataframe/shap_importance_{name}.csv\", index=False)\n",
    "    \n",
    "    colors = list()\n",
    "    for c in df.cluster.unique():\n",
    "        colors.append(colors_dict[c])\n",
    "\n",
    "    # get class ordering from shap values\n",
    "    class_inds = np.argsort([-np.abs(shap_values[i]).mean() for i in range(len(shap_values))])\n",
    "\n",
    "    # create listed colormap\n",
    "    cmap = plt_colors.ListedColormap(np.array(colors)[class_inds])\n",
    "\n",
    "    shap.summary_plot(shap_values, X_valid.head(1), max_display =  len(feature_cols), show=False, \n",
    "                      plot_type = \"bar\", color=cmap,\n",
    "                      class_names=[\"cluster \" + str(i) for i in list(set(df.cluster))])\n",
    "    \n",
    "    plt.savefig(f\"shap_pictures/dot_plot_{name}\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttrain's binary_logloss: 0.0006435\tvalid's binary_logloss: 0.0042481\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttrain's binary_logloss: 0.0418978\tvalid's binary_logloss: 0.0400076\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttrain's multi_logloss: 0.0184096\tvalid's multi_logloss: 0.124481\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttrain's binary_logloss: 0.00352904\tvalid's binary_logloss: 0.0192643\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttrain's binary_logloss: 0.0108341\tvalid's binary_logloss: 0.0536556\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's binary_logloss: 0.0322903\tvalid's binary_logloss: 0.0600412\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1072]\ttrain's multi_logloss: 0.000946029\tvalid's multi_logloss: 0.00166458\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[542]\ttrain's multi_logloss: 0.038044\tvalid's multi_logloss: 0.0558807\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[468]\ttrain's binary_logloss: 0.00437022\tvalid's binary_logloss: 0.038548\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttrain's binary_logloss: 0.0583895\tvalid's binary_logloss: 0.0758699\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[544]\ttrain's multi_logloss: 0.0408563\tvalid's multi_logloss: 0.0819128\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's binary_logloss: 0.0322152\tvalid's binary_logloss: 0.0600532\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[661]\ttrain's multi_logloss: 0.0197739\tvalid's multi_logloss: 0.0789935\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttrain's binary_logloss: 0.0171922\tvalid's binary_logloss: 0.0508588\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[215]\ttrain's multi_logloss: 0.0972085\tvalid's multi_logloss: 0.143137\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[991]\ttrain's binary_logloss: 1.17271e-05\tvalid's binary_logloss: 1.04766e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttrain's binary_logloss: 0.0153215\tvalid's binary_logloss: 0.0587957\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's binary_logloss: 1.15161e-05\tvalid's binary_logloss: 9.85563e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2325]\ttrain's binary_logloss: 2.15455e-05\tvalid's binary_logloss: 8.42678e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0194145\tvalid's binary_logloss: 0.0298663\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[544]\ttrain's multi_logloss: 0.0223173\tvalid's multi_logloss: 0.0487411\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1891]\ttrain's binary_logloss: 3.48974e-05\tvalid's binary_logloss: 0.00108648\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttrain's binary_logloss: 0.0600349\tvalid's binary_logloss: 0.0643009\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1522]\ttrain's binary_logloss: 0.000145775\tvalid's binary_logloss: 0.00027969\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[768]\ttrain's multi_logloss: 0.0126208\tvalid's multi_logloss: 0.0345973\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[848]\ttrain's binary_logloss: 0.0128909\tvalid's binary_logloss: 0.0137042\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[983]\ttrain's binary_logloss: 1.13771e-05\tvalid's binary_logloss: 1.17115e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[945]\ttrain's multi_logloss: 0.00245201\tvalid's multi_logloss: 0.0329904\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttrain's binary_logloss: 0.0168548\tvalid's binary_logloss: 0.048899\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[578]\ttrain's binary_logloss: 0.00520671\tvalid's binary_logloss: 0.0110625\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[385]\ttrain's multi_logloss: 0.0384247\tvalid's multi_logloss: 0.0661381\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttrain's multi_logloss: 0.0226938\tvalid's multi_logloss: 0.116554\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[736]\ttrain's binary_logloss: 0.0179672\tvalid's binary_logloss: 0.1176\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttrain's binary_logloss: 0.0252943\tvalid's binary_logloss: 0.0258662\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[642]\ttrain's binary_logloss: 0.00116128\tvalid's binary_logloss: 0.0222591\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1502]\ttrain's binary_logloss: 1.73606e-05\tvalid's binary_logloss: 1.85651e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttrain's binary_logloss: 0.00474392\tvalid's binary_logloss: 0.03724\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1193]\ttrain's multi_logloss: 8.68821e-06\tvalid's multi_logloss: 8.87121e-06\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[672]\ttrain's binary_logloss: 0.00144391\tvalid's binary_logloss: 0.027171\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[561]\ttrain's binary_logloss: 0.0192825\tvalid's binary_logloss: 0.0631404\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 9.13003e-06\tvalid's multi_logloss: 2.22004e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[608]\ttrain's multi_logloss: 0.0333314\tvalid's multi_logloss: 0.061615\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1312]\ttrain's binary_logloss: 0.000277061\tvalid's binary_logloss: 0.00566797\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2545]\ttrain's binary_logloss: 1.92311e-05\tvalid's binary_logloss: 0.00346643\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1520]\ttrain's binary_logloss: 2.02671e-05\tvalid's binary_logloss: 0.000207613\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2064]\ttrain's multi_logloss: 0.0205454\tvalid's multi_logloss: 0.0321171\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[881]\ttrain's binary_logloss: 0.000422398\tvalid's binary_logloss: 0.00255245\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttrain's binary_logloss: 0.0207002\tvalid's binary_logloss: 0.0270064\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[496]\ttrain's binary_logloss: 0.0228585\tvalid's binary_logloss: 0.0292074\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[848]\ttrain's multi_logloss: 0.00246918\tvalid's multi_logloss: 0.0305797\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[672]\ttrain's binary_logloss: 0.00214625\tvalid's binary_logloss: 0.023677\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[592]\ttrain's multi_logloss: 0.00372049\tvalid's multi_logloss: 0.0273035\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[960]\ttrain's multi_logloss: 0.00917246\tvalid's multi_logloss: 0.0226684\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1040]\ttrain's binary_logloss: 1.11836e-05\tvalid's binary_logloss: 1.17047e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0188961\tvalid's binary_logloss: 0.0313876\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1843]\ttrain's binary_logloss: 2.12832e-05\tvalid's binary_logloss: 4.34909e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttrain's binary_logloss: 0.0172194\tvalid's binary_logloss: 0.0589416\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1840]\ttrain's binary_logloss: 0.00642548\tvalid's binary_logloss: 0.0682498\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[391]\ttrain's multi_logloss: 0.0365747\tvalid's multi_logloss: 0.0914069\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[208]\ttrain's binary_logloss: 0.0450903\tvalid's binary_logloss: 0.045458\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1334]\ttrain's multi_logloss: 9.16231e-06\tvalid's multi_logloss: 1.29618e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1168]\ttrain's multi_logloss: 0.014107\tvalid's multi_logloss: 0.0345079\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[848]\ttrain's multi_logloss: 0.00822505\tvalid's multi_logloss: 0.0209363\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1857]\ttrain's binary_logloss: 3.29365e-05\tvalid's binary_logloss: 0.000189117\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[967]\ttrain's binary_logloss: 1.16293e-05\tvalid's binary_logloss: 1.11898e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1425]\ttrain's binary_logloss: 1.38677e-05\tvalid's binary_logloss: 7.92847e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1248]\ttrain's binary_logloss: 0.0201482\tvalid's binary_logloss: 0.0238816\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[357]\ttrain's multi_logloss: 0.0166951\tvalid's multi_logloss: 0.0454001\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[769]\ttrain's multi_logloss: 0.0529502\tvalid's multi_logloss: 0.102586\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[449]\ttrain's multi_logloss: 0.0231632\tvalid's multi_logloss: 0.0637419\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[832]\ttrain's multi_logloss: 0.00754691\tvalid's multi_logloss: 0.085582\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[566]\ttrain's binary_logloss: 0.00201649\tvalid's binary_logloss: 0.0437178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1056]\ttrain's binary_logloss: 1.16757e-05\tvalid's binary_logloss: 1.01045e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1569]\ttrain's multi_logloss: 0.0159658\tvalid's multi_logloss: 0.0103695\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttrain's binary_logloss: 0.0119292\tvalid's binary_logloss: 0.0206729\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttrain's binary_logloss: 0.0346829\tvalid's binary_logloss: 0.0318692\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[283]\ttrain's binary_logloss: 0.0272458\tvalid's binary_logloss: 0.0295327\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[416]\ttrain's binary_logloss: 0.0180185\tvalid's binary_logloss: 0.067933\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0169774\tvalid's binary_logloss: 0.0304048\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[352]\ttrain's binary_logloss: 0.0108969\tvalid's binary_logloss: 0.015073\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[304]\ttrain's binary_logloss: 0.01526\tvalid's binary_logloss: 0.0551472\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1472]\ttrain's binary_logloss: 0.0220198\tvalid's binary_logloss: 0.00961314\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 1.81235e-05\tvalid's binary_logloss: 2.11265e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttrain's multi_logloss: 0.0269288\tvalid's multi_logloss: 0.0364909\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[491]\ttrain's binary_logloss: 0.00486927\tvalid's binary_logloss: 0.0416858\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[534]\ttrain's binary_logloss: 0.00291822\tvalid's binary_logloss: 0.0307252\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[703]\ttrain's binary_logloss: 0.00118706\tvalid's binary_logloss: 0.0057854\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2279]\ttrain's binary_logloss: 1.79174e-05\tvalid's binary_logloss: 0.0003038\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttrain's multi_logloss: 0.0160123\tvalid's multi_logloss: 0.0739319\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttrain's multi_logloss: 0.0179807\tvalid's multi_logloss: 0.0572154\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[784]\ttrain's multi_logloss: 0.00596687\tvalid's multi_logloss: 0.0475542\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\ttrain's multi_logloss: 0.0363036\tvalid's multi_logloss: 0.0565128\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttrain's binary_logloss: 0.00296939\tvalid's binary_logloss: 0.0386417\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1077]\ttrain's binary_logloss: 1.16017e-05\tvalid's binary_logloss: 1.09594e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1232]\ttrain's binary_logloss: 1.20725e-05\tvalid's binary_logloss: 6.98545e-06\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1494]\ttrain's binary_logloss: 1.12627e-05\tvalid's binary_logloss: 1.45744e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[511]\ttrain's binary_logloss: 0.00723088\tvalid's binary_logloss: 0.0426267\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's binary_logloss: 1.1574e-05\tvalid's binary_logloss: 1.25903e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[552]\ttrain's binary_logloss: 0.0108571\tvalid's binary_logloss: 0.0736752\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[433]\ttrain's binary_logloss: 0.0314112\tvalid's binary_logloss: 0.0377844\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[526]\ttrain's binary_logloss: 0.00773046\tvalid's binary_logloss: 0.0286393\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1081]\ttrain's binary_logloss: 1.19404e-05\tvalid's binary_logloss: 1.64167e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1888]\ttrain's multi_logloss: 1.54482e-05\tvalid's multi_logloss: 0.000359449\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2452]\ttrain's binary_logloss: 1.70225e-05\tvalid's binary_logloss: 5.36933e-06\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[592]\ttrain's multi_logloss: 0.0411204\tvalid's multi_logloss: 0.0714316\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's binary_logloss: 0.00106383\tvalid's binary_logloss: 0.00766664\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1600]\ttrain's binary_logloss: 0.0160764\tvalid's binary_logloss: 0.00349804\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[556]\ttrain's binary_logloss: 0.00771472\tvalid's binary_logloss: 0.0221322\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1001]\ttrain's binary_logloss: 1.17977e-05\tvalid's binary_logloss: 1.21855e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[888]\ttrain's binary_logloss: 0.000550652\tvalid's binary_logloss: 0.00621148\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2080]\ttrain's binary_logloss: 3.01916e-05\tvalid's binary_logloss: 8.01532e-06\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[990]\ttrain's binary_logloss: 1.16901e-05\tvalid's binary_logloss: 1.1688e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1018]\ttrain's binary_logloss: 1.31733e-05\tvalid's binary_logloss: 1.12178e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[574]\ttrain's binary_logloss: 0.0155782\tvalid's binary_logloss: 0.0354538\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1088]\ttrain's multi_logloss: 0.0142663\tvalid's multi_logloss: 0.0923621\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[381]\ttrain's binary_logloss: 0.0222722\tvalid's binary_logloss: 0.0909542\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[602]\ttrain's multi_logloss: 0.0106953\tvalid's multi_logloss: 0.0691801\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[560]\ttrain's multi_logloss: 0.00763195\tvalid's multi_logloss: 0.0359515\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttrain's multi_logloss: 0.0303679\tvalid's multi_logloss: 0.0856867\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[253]\ttrain's binary_logloss: 0.00960242\tvalid's binary_logloss: 0.0442477\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1536]\ttrain's multi_logloss: 2.62377e-05\tvalid's multi_logloss: 7.99401e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[465]\ttrain's binary_logloss: 0.0115487\tvalid's binary_logloss: 0.0393298\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1450]\ttrain's binary_logloss: 1.53366e-05\tvalid's binary_logloss: 7.00085e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[673]\ttrain's multi_logloss: 0.00296906\tvalid's multi_logloss: 0.0339962\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\ttrain's binary_logloss: 0.0143143\tvalid's binary_logloss: 0.0265831\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1776]\ttrain's binary_logloss: 2.56448e-05\tvalid's binary_logloss: 0.00206362\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1231]\ttrain's binary_logloss: 1.51604e-05\tvalid's binary_logloss: 1.29421e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[597]\ttrain's multi_logloss: 0.0287912\tvalid's multi_logloss: 0.0289324\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1040]\ttrain's binary_logloss: 0.00792275\tvalid's binary_logloss: 0.0062488\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1760]\ttrain's binary_logloss: 3.17348e-05\tvalid's binary_logloss: 0.000118416\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1028]\ttrain's binary_logloss: 1.15011e-05\tvalid's binary_logloss: 1.16223e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[462]\ttrain's binary_logloss: 0.00520579\tvalid's binary_logloss: 0.0393495\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[528]\ttrain's binary_logloss: 0.00907714\tvalid's binary_logloss: 0.0192989\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2124]\ttrain's binary_logloss: 3.50434e-05\tvalid's binary_logloss: 3.68003e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1056]\ttrain's multi_logloss: 0.0119722\tvalid's multi_logloss: 0.00280994\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttrain's binary_logloss: 0.0105681\tvalid's binary_logloss: 0.0424089\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[974]\ttrain's binary_logloss: 1.16787e-05\tvalid's binary_logloss: 1.51492e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1424]\ttrain's binary_logloss: 0.022709\tvalid's binary_logloss: 0.00785516\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[672]\ttrain's binary_logloss: 0.00147952\tvalid's binary_logloss: 0.0349807\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[479]\ttrain's binary_logloss: 0.00810914\tvalid's binary_logloss: 0.0282772\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1626]\ttrain's binary_logloss: 0.00139776\tvalid's binary_logloss: 0.00267675\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttrain's binary_logloss: 0.023108\tvalid's binary_logloss: 0.0291798\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1920]\ttrain's multi_logloss: 7.57176e-06\tvalid's multi_logloss: 1.1889e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttrain's binary_logloss: 0.0171308\tvalid's binary_logloss: 0.0520398\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttrain's binary_logloss: 0.0242418\tvalid's binary_logloss: 0.022911\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[896]\ttrain's multi_logloss: 0.0189913\tvalid's multi_logloss: 0.0330642\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2237]\ttrain's binary_logloss: 3.16256e-05\tvalid's binary_logloss: 0.000179658\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[416]\ttrain's binary_logloss: 0.00881546\tvalid's binary_logloss: 0.0410806\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[484]\ttrain's multi_logloss: 0.0299981\tvalid's multi_logloss: 0.0605508\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[336]\ttrain's binary_logloss: 0.0154119\tvalid's binary_logloss: 0.018657\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[752]\ttrain's multi_logloss: 0.00788572\tvalid's multi_logloss: 0.0347475\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1088]\ttrain's binary_logloss: 1.15783e-05\tvalid's binary_logloss: 1.14064e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttrain's binary_logloss: 0.0123383\tvalid's binary_logloss: 0.0230248\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[590]\ttrain's multi_logloss: 0.00961404\tvalid's multi_logloss: 0.0731811\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1184]\ttrain's binary_logloss: 0.0252728\tvalid's binary_logloss: 0.0271802\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttrain's binary_logloss: 0.0129216\tvalid's binary_logloss: 0.0295538\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1106]\ttrain's binary_logloss: 1.28063e-05\tvalid's binary_logloss: 7.07326e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[448]\ttrain's binary_logloss: 0.00897004\tvalid's binary_logloss: 0.0884396\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1862]\ttrain's binary_logloss: 1.76003e-05\tvalid's binary_logloss: 5.39817e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2336]\ttrain's binary_logloss: 1.68247e-05\tvalid's binary_logloss: 1.64282e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 1.1925e-05\tvalid's multi_logloss: 0.000143663\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[401]\ttrain's binary_logloss: 0.025816\tvalid's binary_logloss: 0.0241926\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[403]\ttrain's multi_logloss: 0.0449063\tvalid's multi_logloss: 0.12344\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1088]\ttrain's multi_logloss: 0.00676015\tvalid's multi_logloss: 0.034121\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttrain's binary_logloss: 0.00212049\tvalid's binary_logloss: 0.0355803\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttrain's binary_logloss: 0.0381415\tvalid's binary_logloss: 0.0310768\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[448]\ttrain's multi_logloss: 0.0210351\tvalid's multi_logloss: 0.0474018\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[752]\ttrain's binary_logloss: 0.0372479\tvalid's binary_logloss: 0.0116034\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1874]\ttrain's binary_logloss: 1.63117e-05\tvalid's binary_logloss: 2.27125e-06\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[880]\ttrain's multi_logloss: 0.00683532\tvalid's multi_logloss: 0.044352\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1728]\ttrain's binary_logloss: 1.6327e-05\tvalid's binary_logloss: 1.60641e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[562]\ttrain's multi_logloss: 0.00387012\tvalid's multi_logloss: 0.049798\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1152]\ttrain's multi_logloss: 0.0156124\tvalid's multi_logloss: 0.00566625\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[250]\ttrain's binary_logloss: 0.0206131\tvalid's binary_logloss: 0.0857512\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[788]\ttrain's multi_logloss: 0.00262318\tvalid's multi_logloss: 0.038788\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttrain's multi_logloss: 0.0092416\tvalid's multi_logloss: 0.0445353\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[608]\ttrain's multi_logloss: 0.0311525\tvalid's multi_logloss: 0.0589123\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[352]\ttrain's binary_logloss: 0.00333563\tvalid's binary_logloss: 0.0271248\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 1.03278e-05\tvalid's multi_logloss: 1.10821e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[208]\ttrain's binary_logloss: 0.041415\tvalid's binary_logloss: 0.0506624\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's multi_logloss: 0.0490525\tvalid's multi_logloss: 0.21473\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttrain's binary_logloss: 0.00753678\tvalid's binary_logloss: 0.0608162\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[528]\ttrain's multi_logloss: 0.0165462\tvalid's multi_logloss: 0.0352185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[688]\ttrain's multi_logloss: 0.00912379\tvalid's multi_logloss: 0.0590447\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\ttrain's binary_logloss: 0.00498292\tvalid's binary_logloss: 0.0311958\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\ttrain's binary_logloss: 0.00482755\tvalid's binary_logloss: 0.0335159\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1082]\ttrain's binary_logloss: 1.13507e-05\tvalid's binary_logloss: 1.03517e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttrain's binary_logloss: 0.0678803\tvalid's binary_logloss: 0.0830578\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1024]\ttrain's multi_logloss: 0.015221\tvalid's multi_logloss: 0.0528089\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1699]\ttrain's binary_logloss: 0.0085162\tvalid's binary_logloss: 0.00648823\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[640]\ttrain's multi_logloss: 0.0155192\tvalid's multi_logloss: 0.0273146\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[301]\ttrain's binary_logloss: 0.0129588\tvalid's binary_logloss: 0.0244458\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0183667\tvalid's binary_logloss: 0.0058465\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1030]\ttrain's binary_logloss: 1.19528e-05\tvalid's binary_logloss: 1.49171e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1066]\ttrain's binary_logloss: 1.23685e-05\tvalid's binary_logloss: 0.000128883\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2669]\ttrain's binary_logloss: 2.69316e-05\tvalid's binary_logloss: 5.727e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1049]\ttrain's binary_logloss: 1.1566e-05\tvalid's binary_logloss: 1.16243e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\ttrain's binary_logloss: 0.00895286\tvalid's binary_logloss: 0.0382493\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttrain's binary_logloss: 0.0138396\tvalid's binary_logloss: 0.00657553\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[912]\ttrain's multi_logloss: 0.0193016\tvalid's multi_logloss: 0.0131944\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[484]\ttrain's multi_logloss: 0.0215924\tvalid's multi_logloss: 0.0652498\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 7.8994e-05\tvalid's multi_logloss: 0.000145722\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[881]\ttrain's binary_logloss: 0.0578116\tvalid's binary_logloss: 0.120648\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1061]\ttrain's binary_logloss: 1.20896e-05\tvalid's binary_logloss: 1.28311e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2225]\ttrain's binary_logloss: 3.10893e-05\tvalid's binary_logloss: 0.000184434\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's multi_logloss: 0.0347599\tvalid's multi_logloss: 0.0477693\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttrain's binary_logloss: 0.00940328\tvalid's binary_logloss: 0.0204082\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1246]\ttrain's binary_logloss: 2.30331e-05\tvalid's binary_logloss: 0.000566791\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1888]\ttrain's binary_logloss: 0.0700815\tvalid's binary_logloss: 0.199043\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[544]\ttrain's binary_logloss: 0.00188628\tvalid's binary_logloss: 0.0071034\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1408]\ttrain's binary_logloss: 0.0179827\tvalid's binary_logloss: 0.0190431\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[720]\ttrain's binary_logloss: 0.00930785\tvalid's binary_logloss: 0.0588468\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 4\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1603]\ttrain's multi_logloss: 0.0105209\tvalid's multi_logloss: 0.0272961\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[392]\ttrain's binary_logloss: 0.00548444\tvalid's binary_logloss: 0.0310231\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1104]\ttrain's multi_logloss: 0.00384974\tvalid's multi_logloss: 0.0337019\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[926]\ttrain's binary_logloss: 0.00133422\tvalid's binary_logloss: 0.0088842\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1312]\ttrain's binary_logloss: 2.23018e-05\tvalid's binary_logloss: 0.000156937\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttrain's multi_logloss: 2.31271e-05\tvalid's multi_logloss: 2.02819e-05\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1696]\ttrain's binary_logloss: 0.0176968\tvalid's binary_logloss: 0.0184447\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1600]\ttrain's binary_logloss: 3.41332e-05\tvalid's binary_logloss: 0.00443354\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttrain's binary_logloss: 1.19514e-05\tvalid's binary_logloss: 1.19803e-05\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 3\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[717]\ttrain's multi_logloss: 0.0065747\tvalid's multi_logloss: 0.0324986\n",
      "\n",
      "\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[441]\ttrain's binary_logloss: 0.00832995\tvalid's binary_logloss: 0.015967\n",
      "\n",
      "\n",
      "Just 1 class. Skipped!\n",
      "Just 1 class. Skipped!\n",
      "df[\"cluster\"].nunique() 2\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[768]\ttrain's binary_logloss: 0.00177614\tvalid's binary_logloss: 0.0080652\n",
      "\n",
      "\n",
      "261.6 seconds\n"
     ]
    }
   ],
   "source": [
    "train_error_list = list()\n",
    "valid_error_list = list()\n",
    "skip_list = list()\n",
    "\n",
    "\n",
    "\n",
    "tick = time.time()\n",
    "\n",
    "for name, df in zip(list(data.keys()), data_list):\n",
    "    run_lgb_and_shap(name, df, feature_names = feature_names, train_error_list = train_error_list, valid_error_list=valid_error_list, skip_list=skip_list, colors_dict=colors_dict)\n",
    "\n",
    "tock = time.time()\n",
    "print(f\"{np.round(tock - tick, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['compute-1-10', 'compute-1-13', 'compute-1-14', 'compute-1-18', 'compute-1-19', 'compute-1-22', 'compute-1-23', 'compute-1-24', 'compute-1-26', 'compute-1-29', 'compute-1-3', 'compute-1-30', 'compute-1-31', 'compute-1-34', 'compute-1-36', 'compute-1-37', 'compute-1-4', 'compute-1-40', 'compute-1-42', 'compute-1-43', 'compute-1-45', 'compute-1-46', 'compute-1-47', 'compute-1-49', 'compute-1-53', 'compute-1-55', 'compute-1-58', 'compute-1-59', 'compute-1-6', 'compute-1-60', 'compute-1-7', 'compute-1-8', 'compute-10-26', 'compute-10-27', 'compute-10-28', 'compute-10-30', 'compute-10-31', 'compute-10-33', 'compute-10-34', 'compute-10-35', 'compute-10-36', 'compute-10-37', 'compute-10-38', 'compute-10-39', 'compute-10-40', 'compute-10-41', 'compute-10-44', 'compute-2-1', 'compute-2-10', 'compute-2-12', 'compute-2-19', 'compute-2-2', 'compute-2-20', 'compute-2-22', 'compute-2-26', 'compute-2-29', 'compute-2-30', 'compute-2-32', 'compute-2-33', 'compute-2-37', 'compute-2-4', 'compute-2-40', 'compute-2-42', 'compute-2-43', 'compute-2-46', 'compute-2-49', 'compute-2-5', 'compute-2-50', 'compute-2-53', 'compute-2-54', 'compute-2-56', 'compute-2-57', 'compute-2-60', 'compute-2-9', 'compute-3-10', 'compute-3-11', 'compute-3-13', 'compute-3-14', 'compute-3-15', 'compute-3-16', 'compute-3-17', 'compute-3-19', 'compute-3-2', 'compute-3-20', 'compute-3-22', 'compute-3-23', 'compute-3-25', 'compute-3-26', 'compute-3-29', 'compute-3-31', 'compute-3-33', 'compute-3-35', 'compute-3-36', 'compute-3-40', 'compute-3-44', 'compute-3-45', 'compute-3-46', 'compute-3-5', 'compute-3-51', 'compute-3-53', 'compute-3-54', 'compute-3-55', 'compute-3-7', 'compute-3-8', 'compute-3-9', 'compute-4-1', 'compute-4-10', 'compute-4-12', 'compute-4-14', 'compute-4-18', 'compute-4-19', 'compute-4-2', 'compute-4-22', 'compute-4-23', 'compute-4-24', 'compute-4-27', 'compute-4-3', 'compute-4-31', 'compute-4-34', 'compute-4-36', 'compute-4-37', 'compute-4-38', 'compute-4-39', 'compute-4-40', 'compute-4-42', 'compute-4-43', 'compute-4-44', 'compute-4-47', 'compute-4-48', 'compute-4-5', 'compute-4-6', 'compute-4-7', 'compute-4-8', 'compute-4-9', 'compute-5-11', 'compute-5-14', 'compute-5-15', 'compute-5-16', 'compute-5-17', 'compute-5-4', 'compute-5-6', 'compute-5-8', 'compute-6-10', 'compute-6-12', 'compute-6-15', 'compute-6-17', 'compute-6-18', 'compute-6-19', 'compute-6-5', 'compute-6-6', 'compute-6-8', 'compute-7-1', 'compute-7-11', 'compute-7-14', 'compute-7-15', 'compute-7-18', 'compute-7-19', 'compute-7-21', 'compute-7-23', 'compute-7-26', 'compute-7-27', 'compute-7-28', 'compute-7-32', 'compute-7-33', 'compute-7-34', 'compute-7-35', 'compute-7-36', 'compute-7-40', 'compute-7-42', 'compute-7-43', 'compute-7-45', 'compute-7-46', 'compute-7-47', 'compute-7-49', 'compute-7-51', 'compute-7-6', 'compute-7-8', 'compute-7-9', 'compute-8-10', 'compute-8-11', 'compute-8-12', 'compute-8-13', 'compute-8-15', 'compute-8-16', 'compute-8-17', 'compute-8-18', 'compute-8-19', 'compute-8-24', 'compute-8-27', 'compute-8-28', 'compute-8-29', 'compute-8-3', 'compute-8-31', 'compute-8-32', 'compute-8-34', 'compute-8-35', 'compute-8-38', 'compute-8-39', 'compute-8-42', 'compute-8-43', 'compute-8-46', 'compute-8-47', 'compute-8-48', 'compute-8-5', 'compute-8-50', 'compute-8-53', 'compute-8-54', 'compute-8-57', 'compute-8-59', 'compute-8-6', 'compute-8-8', 'compute-8-9', 'compute-9-1', 'compute-9-11', 'compute-9-13', 'compute-9-14', 'compute-9-16', 'compute-9-17', 'compute-9-19', 'compute-9-2', 'compute-9-20', 'compute-9-23', 'compute-9-24', 'compute-9-25', 'compute-9-26', 'compute-9-27', 'compute-9-28', 'compute-9-3', 'compute-9-32', 'compute-9-33', 'compute-9-41', 'compute-9-45', 'compute-9-47', 'compute-9-48', 'compute-9-49', 'compute-9-5', 'compute-9-50', 'compute-9-52', 'compute-9-55', 'compute-9-57', 'compute-9-59', 'compute-9-7', 'compute-9-8']"
      ],
      "text/plain": [
       "['compute-1-10',\n",
       " 'compute-1-13',\n",
       " 'compute-1-14',\n",
       " 'compute-1-18',\n",
       " 'compute-1-19',\n",
       " 'compute-1-22',\n",
       " 'compute-1-23',\n",
       " 'compute-1-24',\n",
       " 'compute-1-26',\n",
       " 'compute-1-29',\n",
       " 'compute-1-3',\n",
       " 'compute-1-30',\n",
       " 'compute-1-31',\n",
       " 'compute-1-34',\n",
       " 'compute-1-36',\n",
       " 'compute-1-37',\n",
       " 'compute-1-4',\n",
       " 'compute-1-40',\n",
       " 'compute-1-42',\n",
       " 'compute-1-43',\n",
       " 'compute-1-45',\n",
       " 'compute-1-46',\n",
       " 'compute-1-47',\n",
       " 'compute-1-49',\n",
       " 'compute-1-53',\n",
       " 'compute-1-55',\n",
       " 'compute-1-58',\n",
       " 'compute-1-59',\n",
       " 'compute-1-6',\n",
       " 'compute-1-60',\n",
       " 'compute-1-7',\n",
       " 'compute-1-8',\n",
       " 'compute-10-26',\n",
       " 'compute-10-27',\n",
       " 'compute-10-28',\n",
       " 'compute-10-30',\n",
       " 'compute-10-31',\n",
       " 'compute-10-33',\n",
       " 'compute-10-34',\n",
       " 'compute-10-35',\n",
       " 'compute-10-36',\n",
       " 'compute-10-37',\n",
       " 'compute-10-38',\n",
       " 'compute-10-39',\n",
       " 'compute-10-40',\n",
       " 'compute-10-41',\n",
       " 'compute-10-44',\n",
       " 'compute-2-1',\n",
       " 'compute-2-10',\n",
       " 'compute-2-12',\n",
       " 'compute-2-19',\n",
       " 'compute-2-2',\n",
       " 'compute-2-20',\n",
       " 'compute-2-22',\n",
       " 'compute-2-26',\n",
       " 'compute-2-29',\n",
       " 'compute-2-30',\n",
       " 'compute-2-32',\n",
       " 'compute-2-33',\n",
       " 'compute-2-37',\n",
       " 'compute-2-4',\n",
       " 'compute-2-40',\n",
       " 'compute-2-42',\n",
       " 'compute-2-43',\n",
       " 'compute-2-46',\n",
       " 'compute-2-49',\n",
       " 'compute-2-5',\n",
       " 'compute-2-50',\n",
       " 'compute-2-53',\n",
       " 'compute-2-54',\n",
       " 'compute-2-56',\n",
       " 'compute-2-57',\n",
       " 'compute-2-60',\n",
       " 'compute-2-9',\n",
       " 'compute-3-10',\n",
       " 'compute-3-11',\n",
       " 'compute-3-13',\n",
       " 'compute-3-14',\n",
       " 'compute-3-15',\n",
       " 'compute-3-16',\n",
       " 'compute-3-17',\n",
       " 'compute-3-19',\n",
       " 'compute-3-2',\n",
       " 'compute-3-20',\n",
       " 'compute-3-22',\n",
       " 'compute-3-23',\n",
       " 'compute-3-25',\n",
       " 'compute-3-26',\n",
       " 'compute-3-29',\n",
       " 'compute-3-31',\n",
       " 'compute-3-33',\n",
       " 'compute-3-35',\n",
       " 'compute-3-36',\n",
       " 'compute-3-40',\n",
       " 'compute-3-44',\n",
       " 'compute-3-45',\n",
       " 'compute-3-46',\n",
       " 'compute-3-5',\n",
       " 'compute-3-51',\n",
       " 'compute-3-53',\n",
       " 'compute-3-54',\n",
       " 'compute-3-55',\n",
       " 'compute-3-7',\n",
       " 'compute-3-8',\n",
       " 'compute-3-9',\n",
       " 'compute-4-1',\n",
       " 'compute-4-10',\n",
       " 'compute-4-12',\n",
       " 'compute-4-14',\n",
       " 'compute-4-18',\n",
       " 'compute-4-19',\n",
       " 'compute-4-2',\n",
       " 'compute-4-22',\n",
       " 'compute-4-23',\n",
       " 'compute-4-24',\n",
       " 'compute-4-27',\n",
       " 'compute-4-3',\n",
       " 'compute-4-31',\n",
       " 'compute-4-34',\n",
       " 'compute-4-36',\n",
       " 'compute-4-37',\n",
       " 'compute-4-38',\n",
       " 'compute-4-39',\n",
       " 'compute-4-40',\n",
       " 'compute-4-42',\n",
       " 'compute-4-43',\n",
       " 'compute-4-44',\n",
       " 'compute-4-47',\n",
       " 'compute-4-48',\n",
       " 'compute-4-5',\n",
       " 'compute-4-6',\n",
       " 'compute-4-7',\n",
       " 'compute-4-8',\n",
       " 'compute-4-9',\n",
       " 'compute-5-11',\n",
       " 'compute-5-14',\n",
       " 'compute-5-15',\n",
       " 'compute-5-16',\n",
       " 'compute-5-17',\n",
       " 'compute-5-4',\n",
       " 'compute-5-6',\n",
       " 'compute-5-8',\n",
       " 'compute-6-10',\n",
       " 'compute-6-12',\n",
       " 'compute-6-15',\n",
       " 'compute-6-17',\n",
       " 'compute-6-18',\n",
       " 'compute-6-19',\n",
       " 'compute-6-5',\n",
       " 'compute-6-6',\n",
       " 'compute-6-8',\n",
       " 'compute-7-1',\n",
       " 'compute-7-11',\n",
       " 'compute-7-14',\n",
       " 'compute-7-15',\n",
       " 'compute-7-18',\n",
       " 'compute-7-19',\n",
       " 'compute-7-21',\n",
       " 'compute-7-23',\n",
       " 'compute-7-26',\n",
       " 'compute-7-27',\n",
       " 'compute-7-28',\n",
       " 'compute-7-32',\n",
       " 'compute-7-33',\n",
       " 'compute-7-34',\n",
       " 'compute-7-35',\n",
       " 'compute-7-36',\n",
       " 'compute-7-40',\n",
       " 'compute-7-42',\n",
       " 'compute-7-43',\n",
       " 'compute-7-45',\n",
       " 'compute-7-46',\n",
       " 'compute-7-47',\n",
       " 'compute-7-49',\n",
       " 'compute-7-51',\n",
       " 'compute-7-6',\n",
       " 'compute-7-8',\n",
       " 'compute-7-9',\n",
       " 'compute-8-10',\n",
       " 'compute-8-11',\n",
       " 'compute-8-12',\n",
       " 'compute-8-13',\n",
       " 'compute-8-15',\n",
       " 'compute-8-16',\n",
       " 'compute-8-17',\n",
       " 'compute-8-18',\n",
       " 'compute-8-19',\n",
       " 'compute-8-24',\n",
       " 'compute-8-27',\n",
       " 'compute-8-28',\n",
       " 'compute-8-29',\n",
       " 'compute-8-3',\n",
       " 'compute-8-31',\n",
       " 'compute-8-32',\n",
       " 'compute-8-34',\n",
       " 'compute-8-35',\n",
       " 'compute-8-38',\n",
       " 'compute-8-39',\n",
       " 'compute-8-42',\n",
       " 'compute-8-43',\n",
       " 'compute-8-46',\n",
       " 'compute-8-47',\n",
       " 'compute-8-48',\n",
       " 'compute-8-5',\n",
       " 'compute-8-50',\n",
       " 'compute-8-53',\n",
       " 'compute-8-54',\n",
       " 'compute-8-57',\n",
       " 'compute-8-59',\n",
       " 'compute-8-6',\n",
       " 'compute-8-8',\n",
       " 'compute-8-9',\n",
       " 'compute-9-1',\n",
       " 'compute-9-11',\n",
       " 'compute-9-13',\n",
       " 'compute-9-14',\n",
       " 'compute-9-16',\n",
       " 'compute-9-17',\n",
       " 'compute-9-19',\n",
       " 'compute-9-2',\n",
       " 'compute-9-20',\n",
       " 'compute-9-23',\n",
       " 'compute-9-24',\n",
       " 'compute-9-25',\n",
       " 'compute-9-26',\n",
       " 'compute-9-27',\n",
       " 'compute-9-28',\n",
       " 'compute-9-3',\n",
       " 'compute-9-32',\n",
       " 'compute-9-33',\n",
       " 'compute-9-41',\n",
       " 'compute-9-45',\n",
       " 'compute-9-47',\n",
       " 'compute-9-48',\n",
       " 'compute-9-49',\n",
       " 'compute-9-5',\n",
       " 'compute-9-50',\n",
       " 'compute-9-52',\n",
       " 'compute-9-55',\n",
       " 'compute-9-57',\n",
       " 'compute-9-59',\n",
       " 'compute-9-7',\n",
       " 'compute-9-8']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df[\"cluster\"].nunique() 7\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4000]\ttrain's multi_logloss: 0.00470968\tvalid's multi_logloss: 0.00769448\n",
      "\n",
      "\n",
      "82.78 seconds\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "\n",
    "lgb_params = {\n",
    "        'boosting_type':'gbdt', 'colsample_bytree':0.8, #'class_weight': {0 : 1. , 1: weight},\n",
    "        'importance_type':'gain', 'learning_rate':0.01, 'max_depth':2,\n",
    "        'min_child_samples':15,# 'min_child_weight':0.001, 'min_split_gain':0.0,\n",
    "        'n_estimators':4000, 'n_jobs': 16, 'num_leaves':31, 'subsample_freq':16,\n",
    "        'seed': random_seed, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "        'subsample':.9, 'subsample_for_bin':200000,  \"metric\":metric ,'objective':objective\n",
    "    }\n",
    "\n",
    "run_lgb_and_shap(\"TotalData\", df=pd.concat(data_list,0), feature_names=feature_names, train_error_list = train_error_list, valid_error_list=valid_error_list, \n",
    "                 skip_list=skip_list, colors_dict=colors_dict,lgb_params=lgb_params)\n",
    "tock = time.time()\n",
    "print(f\"{np.round(tock - tick, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[1.0, 0.9895833333333334, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9895833333333334, 0.96875, 0.9583333333333334, 0.9895833333333334, 0.9791666666666666, 0.9894736842105263, 0.9583333333333334, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 0.9791666666666666, 1.0, 0.9791666666666666, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9789473684210527, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9583333333333334, 0.9479166666666666, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.96875, 1.0, 0.9791666666666666, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9791666666666666, 0.9789473684210527, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.96875, 0.9791666666666666, 0.9791666666666666, 1.0, 0.96875, 0.9791666666666666, 1.0, 1.0, 1.0, 0.9789473684210527, 0.9895833333333334, 0.9789473684210527, 0.9791666666666666, 0.96875, 0.9791666666666666, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 0.9894736842105263, 0.9895833333333334, 0.9894736842105263, 0.9791666666666666, 1.0, 1.0, 0.9894736842105263, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9791666666666666, 0.96875, 0.9791666666666666, 0.9791666666666666, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9894736842105263, 1.0, 0.9789473684210527, 0.9791666666666666, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9791666666666666, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9791666666666666, 0.96875, 0.9791666666666666, 0.9895833333333334, 0.96875, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9894736842105263, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9791666666666666, 0.9894736842105263, 1.0, 0.96875, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9895833333333334, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9583333333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 0.96875, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 0.9791666666666666, 1.0, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 0.9791666666666666, 0.9895833333333334, 1.0, 0.9791666666666666, 0.9368421052631579, 0.9791666666666666, 0.9791666666666666, 0.96875, 0.9895833333333334, 0.9895833333333334, 1.0, 0.96875, 0.9791666666666666, 1.0, 0.9791666666666666, 0.9895833333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 0.96875, 1.0, 0.9583333333333334, 1.0, 1.0, 0.9895833333333334, 0.9895833333333334, 1.0, 0.9479166666666666, 1.0, 0.9895833333333334, 0.96875, 0.9895833333333334, 0.9895833333333334, 0.9895833333333334, 1.0, 1.0, 1.0, 0.9895833333333334, 1.0, 1.0, 0.9791666666666666, 0.9895833333333334, 0.9895833333333334, 0.9975216294160058, 0.9965528118240807, 0.9975216294160058, 0.9975216294160058]"
      ],
      "text/plain": [
       "[1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9583333333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9894736842105263,\n",
       " 0.9583333333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9789473684210527,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9583333333333334,\n",
       " 0.9479166666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9789473684210527,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9789473684210527,\n",
       " 0.9895833333333334,\n",
       " 0.9789473684210527,\n",
       " 0.9791666666666666,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9894736842105263,\n",
       " 0.9895833333333334,\n",
       " 0.9894736842105263,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9894736842105263,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9894736842105263,\n",
       " 1.0,\n",
       " 0.9789473684210527,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9894736842105263,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9894736842105263,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9583333333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9368421052631579,\n",
       " 0.9791666666666666,\n",
       " 0.9791666666666666,\n",
       " 0.96875,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.9791666666666666,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.9583333333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 0.9479166666666666,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 0.96875,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9895833333333334,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9791666666666666,\n",
       " 0.9895833333333334,\n",
       " 0.9895833333333334,\n",
       " 0.9975216294160058,\n",
       " 0.9965528118240807,\n",
       " 0.9975216294160058,\n",
       " 0.9975216294160058]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of computes having SHAP: 225\n"
     ]
    }
   ],
   "source": [
    "print(\"num of computes having SHAP:\", (len(list(data.keys()))) - len(skip_list) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
